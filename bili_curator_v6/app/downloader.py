"""
V6下载核心 - 基于V5版本改进
"""
import asyncio
import json
import os
import re
import subprocess
from datetime import datetime
import tempfile
import os
from sqlalchemy.exc import IntegrityError
from pathlib import Path
from typing import Dict, List, Optional, Any
from sqlalchemy.orm import Session
from loguru import logger

from .models import Video, DownloadTask, Subscription, Settings, get_db
from .cookie_manager import cookie_manager, rate_limiter, simple_retry

class BilibiliDownloaderV6:
    def __init__(self, output_dir: str = None):
        # 从环境变量获取下载路径，默认为/app/downloads
        if output_dir is None:
            output_dir = os.getenv('DOWNLOAD_PATH', '/app/downloads')
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.concurrent_downloads = 3
        self.download_semaphore = asyncio.Semaphore(self.concurrent_downloads)
    
    async def download_collection(self, subscription_id: int, db: Session) -> Dict[str, Any]:
        """下载合集"""
        subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
        if not subscription:
            raise ValueError(f"订阅 {subscription_id} 不存在")
        
        logger.info(f"开始下载合集: {subscription.name}")
        
        # 获取合集视频列表
        video_list = await self._get_collection_videos(subscription.url, db)
        
        # 扫描已有文件
        existing_videos = self._scan_existing_files(db)
        
        # 过滤需要下载的视频
        new_videos = []
        for video_info in video_list:
            video_id = video_info.get('id')
            if video_id not in existing_videos:
                new_videos.append(video_info)
        
        logger.info(f"发现 {len(new_videos)} 个新视频需要下载")
        
        # 创建下载任务
        download_results = []
        for video_info in new_videos:
            try:
                result = await self._download_single_video(video_info, subscription_id, db)
                download_results.append(result)
            except Exception as e:
                logger.error(f"下载视频 {video_info.get('title', 'Unknown')} 失败: {e}")
                download_results.append({
                    'video_id': video_info.get('id'),
                    'success': False,
                    'error': str(e)
                })
        
        # 更新订阅检查时间
        subscription.last_check = datetime.now()
        db.commit()
        
        return {
            'subscription_id': subscription_id,
            'total_videos': len(video_list),
            'new_videos': len(new_videos),
            'download_results': download_results
        }
    

    async def _get_collection_videos(self, collection_url: str, db: Session) -> List[Dict]:
        """获取合集视频列表（使用cookies.txt，避免header方式不稳定）"""
        await rate_limiter.wait()
        
        cookie = cookie_manager.get_available_cookie(db)
        if not cookie:
            raise Exception("没有可用的Cookie")
        
        cookies_path = None
        try:
            # 写入临时 cookies.txt (Netscape 格式)
            fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
            os.close(fd)
            with open(cookies_path, 'w', encoding='utf-8') as cf:
                # Netscape cookie file header is required by yt-dlp
                cf.write("# Netscape HTTP Cookie File\n")
                cf.write("# This file was generated by bili_curator V6\n\n")
                cf.writelines([
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
                ])
            
            cmd = [
                'yt-dlp',
                '--dump-json',
                '--flat-playlist',
                '--cookies', cookies_path,
                collection_url,
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            if process.returncode != 0:
                error_msg = stderr.decode('utf-8', errors='ignore')
                logger.error(f"yt-dlp获取视频列表失败: {error_msg}")
                raise Exception(f"获取视频列表失败: {error_msg}")
            
            video_list = []
            for line in stdout.decode('utf-8', errors='ignore').strip().split('\n'):
                if line.strip():
                    try:
                        video_info = json.loads(line)
                        video_list.append(video_info)
                    except json.JSONDecodeError:
                        continue
            cookie_manager.update_cookie_usage(db, cookie.id)
            logger.info(f"获取到 {len(video_list)} 个视频")
            return video_list
        except Exception as e:
            logger.error(f"获取合集视频列表失败: {e}")
            if "403" in str(e) or "401" in str(e):
                cookie_manager.mark_cookie_banned(db, cookie.id, str(e))
            raise
        finally:
            if cookies_path and os.path.exists(cookies_path):
                try:
                    os.remove(cookies_path)
                except Exception:
                    pass
    
    def _scan_existing_files(self, db: Session) -> Dict[str, str]:
        """扫描已有文件，构建视频ID映射"""
        existing_videos = {}
        
        # 从数据库获取已下载的视频
        downloaded_videos = db.query(Video).filter(Video.downloaded == True).all()
        for video in downloaded_videos:
            existing_videos[video.bilibili_id] = video.video_path
        
        # 同时扫描文件系统中的JSON文件（兼容V5格式），递归子目录，支持 *.json 与 *.info.json
        scanned = set()
        for json_file in list(self.output_dir.rglob("*.json")) + list(self.output_dir.rglob("*.info.json")):
            # 避免重复处理相同路径
            if json_file in scanned:
                continue
            scanned.add(json_file)
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    video_id = data.get('id')
                    if video_id and video_id not in existing_videos:
                        # 计算可能的mp4文件名
                        name = json_file.name
                        if name.endswith('.info.json'):
                            base_name = name[:-10]
                        elif name.endswith('.json'):
                            base_name = name[:-5]
                        else:
                            base_name = json_file.stem
                        # 假设视频文件与json同目录
                        video_file = json_file.parent / f"{base_name}.mp4"
                        if video_file.exists():
                            existing_videos[video_id] = str(video_file)
            except Exception as e:
                logger.warning(f"读取JSON文件 {json_file} 失败: {e}")
        
        logger.info(f"发现 {len(existing_videos)} 个已存在的视频")
        return existing_videos
    
    async def _download_single_video(self, video_info: Dict[str, Any], subscription_id: int, db: Session) -> Dict[str, Any]:
        """下载单个视频"""
        async with self.download_semaphore:
            video_id = video_info.get('id')
            title = video_info.get('title', 'Unknown')
            
            logger.info(f"开始下载: {title} ({video_id})")
            
            # 创建下载任务记录
            task = DownloadTask(
                video_id=video_id,  # 兼容旧库的 NOT NULL 约束
                bilibili_id=video_id,
                subscription_id=subscription_id,
                status='downloading',
                started_at=datetime.now()
            )
            db.add(task)
            db.commit()
            
            try:
                # 获取Cookie
                cookie = cookie_manager.get_available_cookie(db)
                if not cookie:
                    raise Exception("没有可用的Cookie")
                
                await rate_limiter.wait()

                # 如果标题缺失或为 Unknown，则补抓元数据以保证命名正确
                if not title or str(title).strip().lower() in ('', 'unknown'):
                    try:
                        detail_url = video_info.get('webpage_url') or video_info.get('url') or f"https://www.bilibili.com/video/{video_id}"
                        meta = await self._fetch_video_metadata(detail_url, cookie)
                        if meta:
                            # 回填关键信息
                            video_info.update(meta)
                            title = meta.get('title') or title
                    except Exception as e:
                        logger.warning(f"获取视频元数据失败，使用回退命名: {video_id} - {e}")
                        if (not title) or (str(title).strip().lower() in ('', 'unknown')):
                            title = str(video_id)
                
                # 生成安全的文件名
                safe_title = self._sanitize_filename(title)
                base_filename = f"{safe_title}"
                
                # 标题重名冲突处理：若同名文件存在且其JSON的id与当前不同，则回退为“标题 - BV号”
                subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
                if not subscription:
                    raise Exception("订阅不存在")
                subscription_dir = Path(self._create_subscription_directory(subscription))
                existing_base = None
                for ext in ['.mp4', '.mkv', '.webm']:
                    p = subscription_dir / f"{base_filename}{ext}"
                    if p.exists():
                        existing_base = p.with_suffix('')
                        break
                if existing_base:
                    info_path = existing_base.with_suffix('.info.json')
                    try:
                        if info_path.exists():
                            with open(info_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if data.get('id') and data.get('id') != video_id:
                                    base_filename = f"{safe_title} - {video_id}"
                    except Exception:
                        # 无法读取则保守地回退加上BV号
                        base_filename = f"{safe_title} - {video_id}"
                
                # 下载视频
                video_path = await self._download_with_ytdlp(
                    video_info.get('url', f"https://www.bilibili.com/video/{video_id}"),
                    base_filename,
                    subscription_dir,
                    cookie,
                    task.id,
                    db
                )
                
                # 创建NFO文件
                await self._create_nfo_file(video_info, base_filename, subscription_dir)
                
                # 保存视频信息到数据库（查重避免唯一键冲突）
                existing = db.query(Video).filter_by(bilibili_id=video_id).first()
                if existing:
                    # 已存在：标记任务完成并返回现有路径
                    task.status = 'completed'
                    task.progress = 100.0
                    task.completed_at = datetime.now()
                    db.commit()
                    logger.info(f"已存在，跳过入库: {title} ({video_id})")
                    return {
                        'video_id': video_id,
                        'title': title,
                        'success': True,
                        'video_path': existing.video_path
                    }

                # 计算JSON与缩略图路径
                json_path = subscription_dir / f"{base_filename}.info.json"
                thumbnail_path = subscription_dir / f"{base_filename}.jpg"

                video = Video(
                    bilibili_id=video_id,
                    title=title,
                    uploader=video_info.get('uploader', ''),
                    uploader_id=video_info.get('uploader_id', ''),
                    duration=video_info.get('duration'),
                    upload_date=self._parse_upload_date(video_info.get('upload_date')),
                    description=video_info.get('description', ''),
                    video_path=str(video_path),
                    json_path=str(json_path) if json_path.exists() else None,
                    thumbnail_path=str(thumbnail_path) if thumbnail_path.exists() else None,
                    downloaded=True,
                    subscription_id=subscription_id
                )
                db.add(video)
                
                # 更新任务状态
                task.status = 'completed'
                task.progress = 100.0
                task.completed_at = datetime.now()
                db.commit()
                
                # 更新Cookie使用统计
                cookie_manager.update_cookie_usage(db, cookie.id)
                
                logger.info(f"下载完成: {title}")
                return {
                    'video_id': video_id,
                    'title': title,
                    'success': True,
                    'video_path': str(video_path)
                }
                
            except IntegrityError as e:
                # 唯一键或约束异常处理
                db.rollback()
                task.status = 'failed'
                task.error_message = str(e)
                task.completed_at = datetime.now()
                db.commit()
                logger.error(f"下载失败(数据库约束): {title} - {e}")
                return {
                    'video_id': video_id,
                    'title': title,
                    'success': False,
                    'error': str(e)
                }
            except Exception as e:
                logger.error(f"下载失败: {title} - {e}")
                
                # 更新任务状态
                task.status = 'failed'
                task.error_message = str(e)
                task.completed_at = datetime.now()
                db.commit()
                
                # 如果是认证错误，标记Cookie为不可用
                if "403" in str(e) or "401" in str(e):
                    if cookie:
                        cookie_manager.mark_cookie_banned(db, cookie.id, str(e))
                
                return {
                    'video_id': video_id,
                    'title': title,
                    'success': False,
                    'error': str(e)
                }

    async def _fetch_video_metadata(self, url: str, cookie) -> Optional[Dict[str, Any]]:
        """使用 yt-dlp 拉取单视频的详细元数据（含标题），用于命名与NFO补全"""
        cookies_path = None
        try:
            fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
            os.close(fd)
            with open(cookies_path, 'w', encoding='utf-8') as cf:
                cf.write("# Netscape HTTP Cookie File\n")
                cf.write("# This file was generated by bili_curator V6\n\n")
                cf.writelines([
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
                ])

            cmd = [
                'yt-dlp',
                '--dump-single-json',
                '--no-playlist',
                '--cookies', cookies_path,
                url,
            ]
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            if process.returncode != 0:
                err = stderr.decode('utf-8', errors='ignore')
                logger.warning(f"获取视频元数据失败: {err}")
                return None
            try:
                data = json.loads(stdout.decode('utf-8', errors='ignore') or '{}')
                # 兼容某些站点的外层entries结构
                if isinstance(data, dict) and 'title' in data:
                    return data
                if isinstance(data, dict) and 'entries' in data and data['entries']:
                    return data['entries'][0]
                return None
            except Exception as e:
                logger.warning(f"解析视频元数据失败: {e}")
                return None
        finally:
            if cookies_path and os.path.exists(cookies_path):
                try:
                    os.remove(cookies_path)
                except Exception:
                    pass
    
    async def _download_with_ytdlp(self, url: str, base_filename: str, subscription_dir: Path, cookie, task_id: int, db: Session) -> Path:
        """使用yt-dlp下载视频（输出到订阅目录）"""
        output_template = str(subscription_dir / f"{base_filename}.%(ext)s")

        # 写入临时 cookies.txt（Netscape 格式最简行）
        cookies_path = None
        try:
            fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
            os.close(fd)
            with open(cookies_path, 'w', encoding='utf-8') as cf:
                # Netscape cookie file header is required by yt-dlp
                cf.write("# Netscape HTTP Cookie File\n")
                cf.write("# This file was generated by bili_curator V6\n\n")
                # domain, include_subdomains, path, secure, expires, name, value
                lines = [
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                    f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
                ]
                cf.writelines(lines)

            async def run_yt_dlp(format_str: str):
                cmd = [
                    'yt-dlp',
                    '--format', format_str,
                    '--output', output_template,
                    '--write-info-json',
                    '--write-thumbnail',
                    '--convert-thumbnails', 'jpg',
                    '--merge-output-format', 'mp4',
                    '--cookies', cookies_path,
                    url
                ]
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                out, err = await proc.communicate()
                return proc.returncode, out, err

            # 首选 1080 限制，失败则回退到通用最佳
            ret, out, err = await run_yt_dlp('bestvideo*[height<=1080]+bestaudio/best')
            if ret != 0 and b'Requested format is not available' in err:
                ret, out, err = await run_yt_dlp('bv+ba/b')
            if ret != 0:
                error_msg = (err or b'').decode('utf-8', errors='ignore')
                raise Exception(f"yt-dlp下载失败: {error_msg}")
        finally:
            if cookies_path and os.path.exists(cookies_path):
                try:
                    os.remove(cookies_path)
                except Exception:
                    pass
        
        # 查找下载的视频文件（在订阅目录下）
        video_file = subscription_dir / f"{base_filename}.mp4"
        if not video_file.exists():
            # 先尝试常见扩展名
            for ext in ['.webm', '.mkv', '.flv', '.m4v', '.ts', '.mov', '.avi']:
                alt_file = subscription_dir / f"{base_filename}{ext}"
                if alt_file.exists():
                    video_file = alt_file
                    break
        
        if not video_file.exists():
            # 通配回退：处理 base_filename.fXXXX.mp4 等情况
            candidates = []
            try:
                for p in subscription_dir.glob(f"{base_filename}.*"):
                    name = p.name
                    # 过滤临时/非媒体文件
                    if name.endswith(('.part', '.ytdl', '.temp', '.aria2', '.info.json', '.jpg', '.png', '.nfo')):
                        continue
                    if p.is_file():
                        candidates.append(p)
            except Exception:
                pass
            if candidates:
                # 选择最大文件作为视频文件
                candidates.sort(key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)
                picked = candidates[0]
                logger.info(f"产物查找回退匹配: 选择 {picked}")
                video_file = picked
        
        if not video_file.exists():
            # 打印该前缀下的文件，便于排查
            try:
                listing = [p.name for p in subscription_dir.glob(f"{base_filename}.*")]
                logger.error(f"未找到下载产物，前缀匹配文件: {listing}")
            except Exception:
                pass
            raise Exception("下载的视频文件未找到")

        # 若文件名包含格式后缀（例如 .f100113.mp4），规范化重命名为 base_filename.mp4（如无冲突）
        try:
            expected = subscription_dir / f"{base_filename}.mp4"
            if video_file.suffix.lower() == '.mp4' and video_file.name != expected.name:
                # 仅当目标不存在时重命名
                if not expected.exists():
                    logger.info(f"规范化重命名: {video_file.name} -> {expected.name}")
                    video_file.rename(expected)
                    video_file = expected
        except Exception as e:
            logger.warning(f"重命名标准化失败，保持原名: {video_file} - {e}")

        return video_file
    
    async def _create_nfo_file(self, video_info: Dict[str, Any], base_filename: str, subscription_dir: Path):
        """创建增强版NFO文件（与V5一致）"""
        nfo_path = subscription_dir / f"{base_filename}.nfo"
        title = video_info.get('title', 'Unknown')
        video_id = video_info.get('id', '')
        uploader = video_info.get('uploader', '')
        upload_date = video_info.get('upload_date', '')
        duration = video_info.get('duration', 0) or 0
        description = video_info.get('description', '') or ''
        tags = video_info.get('tags', []) or []
        view_count = video_info.get('view_count', 0) or 0
        like_count = video_info.get('like_count', 0) or 0
        webpage_url = video_info.get('webpage_url', video_info.get('url', ''))

        # 格式化
        formatted_date = self._format_date(upload_date)
        runtime_minutes = duration // 60 if duration else 0
        clean_desc = self._escape_xml(description)
        if len(clean_desc) > 500:
            clean_desc = clean_desc[:500] + '...'

        lines = [
            '<?xml version="1.0" encoding="utf-8" standalone="yes"?>',
            '<movie>',
            f'  <title>{self._escape_xml(title)}</title>',
            f'  <sorttitle>{self._escape_xml(title)}</sorttitle>',
            f'  <plot>{clean_desc}</plot>',
            f'  <outline>{clean_desc}</outline>',
            f'  <runtime>{runtime_minutes}</runtime>',
            f'  <year>{upload_date[:4] if upload_date else ""}</year>',
            f'  <studio>{self._escape_xml(uploader)}</studio>',
            f'  <director>{self._escape_xml(uploader)}</director>',
            f'  <credits>{self._escape_xml(uploader)}</credits>',
            f'  <uniqueid type="bilibili">{self._escape_xml(video_id)}</uniqueid>',
            f'  <dateadded>{formatted_date} 00:00:00</dateadded>',
            f'  <premiered>{formatted_date}</premiered>',
            f'  <playcount>{view_count}</playcount>',
            f'  <userrating>{min(10, like_count // 1000) if like_count else 0}</userrating>',
            f'  <trailer>{self._escape_xml(webpage_url)}</trailer>',
        ]
        for tag in tags[:10]:
            lines.append(f'  <tag>{self._escape_xml(tag)}</tag>')
        lines.extend([
            '  <fileinfo>',
            '    <streamdetails>',
            '      <video>',
            '        <codec>h264</codec>',
            '        <aspect>16:9</aspect>',
            '        <width>1920</width>',
            '        <height>1080</height>',
            '      </video>',
            '      <audio>',
            '        <codec>aac</codec>',
            '        <language>zh</language>',
            '        <channels>2</channels>',
            '      </audio>',
            '    </streamdetails>',
            '  </fileinfo>',
            '</movie>'
        ])

        with open(nfo_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        logger.info(f"已生成NFO: {nfo_path}")
    
    def _create_subscription_directory(self, subscription: Subscription) -> str:
        """创建订阅目录，返回目录路径"""
        from .models import Subscription
        
        # 根据订阅类型确定目录名
        if subscription.type == 'collection':
            # 合集订阅：优先使用订阅名称
            dir_name = self._sanitize_filename(subscription.name or "")
            if not dir_name:
                # 兜底，避免落在根目录
                dir_name = self._sanitize_filename(f"合集订阅_{subscription.id}")
        elif subscription.type == 'keyword':
            # 关键词订阅：使用关键词
            base = subscription.keyword or subscription.name or "关键词订阅"
            dir_name = self._sanitize_filename(f"关键词_{base}")
        elif subscription.type == 'uploader':
            # UP主订阅：使用UP主名称/ID
            base = subscription.name or getattr(subscription, 'uploader_id', None) or "UP主订阅"
            dir_name = self._sanitize_filename(f"UP主_{base}")
        else:
            # 其他类型：使用订阅名称，必要时兜底
            dir_name = self._sanitize_filename(subscription.name or "")
            if not dir_name:
                dir_name = self._sanitize_filename(f"订阅_{subscription.id}")
        
        # 创建目录路径
        subscription_dir = os.path.join(self.output_dir, dir_name)
        os.makedirs(subscription_dir, exist_ok=True)
        
        logger.info(f"创建订阅目录: {subscription_dir}")
        return subscription_dir
    
    def _sanitize_filename(self, filename: str) -> str:
        """清理文件名，移除非法字符"""
        # 移除或替换非法字符
        illegal_chars = r'[<>:"/\\|?*]'
        filename = re.sub(illegal_chars, '_', filename)
        
        # 移除前后空格和点
        filename = filename.strip(' .')
        
        # 限制长度
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename
    
    def _parse_upload_date(self, date_str: str) -> Optional[datetime]:
        """解析上传日期"""
        if not date_str:
            return None
        
        try:
            # yt-dlp通常返回YYYYMMDD格式
            if len(date_str) == 8 and date_str.isdigit():
                return datetime.strptime(date_str, '%Y%m%d')
            return datetime.fromisoformat(date_str)
        except:
            return None
    
    def _extract_year(self, date_str: str) -> str:
        """提取年份"""
        date_obj = self._parse_upload_date(date_str)
        return str(date_obj.year) if date_obj else ""
    
    def _format_date(self, date_str: str) -> str:
        """格式化日期为YYYY-MM-DD"""
        date_obj = self._parse_upload_date(date_str)
        return date_obj.strftime('%Y-%m-%d') if date_obj else ""
    
    def _escape_xml(self, text: str) -> str:
        """转义XML特殊字符"""
        if not text:
            return ""
        
        text = str(text)
        text = text.replace('&', '&amp;')
        text = text.replace('<', '&lt;')
        text = text.replace('>', '&gt;')
        text = text.replace('"', '&quot;')
        text = text.replace("'", '&apos;')
        return text

# 全局下载器实例
downloader = BilibiliDownloaderV6()
