# UA ç­–ç•¥ï¼šæ—  Cookie ä½¿ç”¨éšæœº UAï¼ŒCookie æ¨¡å¼å¯ç”¨ç¨³å®š UA
_UA_POOL = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',
]

def get_user_agent(requires_cookie: bool) -> str:
    try:
        if requires_cookie:
            return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
        return random.choice(_UA_POOL)
    except Exception:
        return _UA_POOL[0]

"""
FastAPI APIè·¯ç”±å®šä¹‰
"""
from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from sqlalchemy.orm import Session
from sqlalchemy import func, case
from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from datetime import datetime
import logging
import asyncio
import random
import json
import os
from pathlib import Path

from .models import (
    Subscription, Video, DownloadTask, Cookie, Settings, SubscriptionUpdate, CookieCreate, CookieUpdate, SettingUpdate,
    get_db
)
from .scheduler import scheduler, task_manager
from .cookie_manager import cookie_manager
from .downloader import downloader
from .video_detection_service import video_detection_service
from .queue_manager import yt_dlp_semaphore, get_subscription_lock, request_queue

# Logger
logger = logging.getLogger(__name__)

# UA ç­–ç•¥ï¼šæ—  Cookie ä½¿ç”¨éšæœº UAï¼ŒCookie æ¨¡å¼ä½¿ç”¨ç¨³å®š UA
_UA_POOL = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',
]

def get_user_agent(requires_cookie: bool) -> str:
    try:
        if requires_cookie:
            return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
        return random.choice(_UA_POOL)
    except Exception:
        return _UA_POOL[0]

# Pydanticæ¨¡å‹å®šä¹‰
class SubscriptionCreate(BaseModel):
    name: str
    type: str  # 'collection', 'uploader', 'keyword'
    url: Optional[str] = None
    uploader_id: Optional[str] = None
    keyword: Optional[str] = None

class SubscriptionUpdate(BaseModel):
    name: Optional[str] = None
    is_active: Optional[bool] = None
    active: Optional[bool] = None
    url: Optional[str] = None
    uploader_id: Optional[str] = None
    keyword: Optional[str] = None

class CookieCreate(BaseModel):
    name: str
    sessdata: str
    bili_jct: Optional[str] = ""
    dedeuserid: Optional[str] = ""

class CookieUpdate(BaseModel):
    name: Optional[str] = None
    is_active: Optional[bool] = None
    active: Optional[bool] = None
    sessdata: Optional[str] = None
    bili_jct: Optional[str] = None
    dedeuserid: Optional[str] = None

class SettingUpdate(BaseModel):
    value: str

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="bili_curator V6",
    description="Bç«™è§†é¢‘ä¸‹è½½ç®¡ç†ç³»ç»Ÿ",
    version="6.0.0"
)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory="static"), name="static")
app.mount("/web", StaticFiles(directory="web/dist"), name="web")

# æ ¹è·¯å¾„è¿”å›å‰ç«¯é¡µé¢
@app.get("/", response_class=HTMLResponse)
async def read_root():
    """è¿”å›å‰ç«¯é¡µé¢"""
    try:
        with open("web/dist/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse(content="""
        <html>
            <head><title>bili_curator V6</title></head>
            <body>
                <h1>ğŸ¬ bili_curator V6</h1>
                <p>å‰ç«¯é¡µé¢æ­£åœ¨æ„å»ºä¸­...</p>
                <p>APIæ–‡æ¡£: <a href="/docs">/docs</a></p>
            </body>
        </html>
        """)

# ç³»ç»ŸçŠ¶æ€API
@app.get("/api/status")
async def get_system_status(db: Session = Depends(get_db)):
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    # ç»Ÿè®¡æ•°æ®
    total_subscriptions = db.query(Subscription).count()
    active_subscriptions = db.query(Subscription).filter(Subscription.is_active == True).count()
    total_videos = db.query(Video).count()
    downloaded_videos = db.query(Video).filter(Video.video_path.isnot(None)).count()
    active_cookies = db.query(Cookie).filter(Cookie.is_active == True).count()
    
    return {
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "version": "6.0.0",
        "statistics": {
            "total_subscriptions": total_subscriptions,
            "active_subscriptions": active_subscriptions,
            "total_videos": total_videos,
            "downloaded_videos": downloaded_videos,
            "active_cookies": active_cookies
        },
        "recent_tasks": [],
        "scheduler_jobs": [],
        "running_tasks": []
    }

# é˜Ÿåˆ—è°ƒè¯•æ¥å£
@app.get("/api/queue/stats")
async def queue_stats():
    """è¿”å›é˜Ÿåˆ—å®¹é‡/è¿è¡Œ/æš‚åœä¸å„é€šé“æ’é˜Ÿæ•°ã€‚"""
    try:
        return request_queue.stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/queue/list")
async def queue_list():
    """è¿”å›æ‰€æœ‰ä»»åŠ¡çš„å¿«ç…§ï¼ˆåŒ…å« wait_msã€last_wait_reason ç­‰è¯Šæ–­å­—æ®µï¼‰ã€‚"""
    try:
        return request_queue.list()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# è®¢é˜…ç®¡ç†API
@app.get("/api/subscriptions")
async def get_subscriptions(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰è®¢é˜…"""
    subscriptions = db.query(Subscription).all()
    result = []
    
    for sub in subscriptions:
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_videos = db.query(Video).filter(Video.subscription_id == sub.id).count()
        downloaded_videos = db.query(Video).filter(
            Video.subscription_id == sub.id,
            Video.video_path.isnot(None)
        ).count()
        pending_videos = max(0, total_videos - downloaded_videos)
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
        sub.total_videos = total_videos
        sub.downloaded_videos = downloaded_videos
        
        result.append({
            "id": sub.id,
            "name": sub.name,
            "type": sub.type,
            "url": sub.url,
            "uploader_id": sub.uploader_id,
            "keyword": sub.keyword,
            "specific_urls": sub.specific_urls,
            "date_after": sub.date_after.isoformat() if sub.date_after else None,
            "date_before": sub.date_before.isoformat() if sub.date_before else None,
            "min_likes": sub.min_likes,
            "min_favorites": sub.min_favorites,
            "min_views": sub.min_views,
            "total_videos": total_videos,
            "downloaded_videos": downloaded_videos,
            "pending_videos": pending_videos,
            "is_active": sub.is_active,
            "last_check": sub.last_check.isoformat() if sub.last_check else None,
            "created_at": sub.created_at.isoformat() if sub.created_at else None,
            "updated_at": sub.updated_at.isoformat() if sub.updated_at else None
        })
    
    db.commit()
    return result

@app.post("/api/subscriptions")
async def create_subscription(subscription: dict, db: Session = Depends(get_db)):
    """åˆ›å»ºæ–°è®¢é˜…"""
    try:
        # è§£ææ—¥æœŸå­—æ®µ
        date_after = None
        date_before = None
        if subscription.get("date_after"):
            try:
                date_after = datetime.strptime(subscription["date_after"], "%Y-%m-%d").date()
            except ValueError:
                pass
        if subscription.get("date_before"):
            try:
                date_before = datetime.strptime(subscription["date_before"], "%Y-%m-%d").date()
            except ValueError:
                pass

        # è§„èŒƒåŒ–åç§°ï¼šå½“ä¸ºåˆé›†è®¢é˜…ä¸”æœªæä¾›åç§°æ—¶ï¼Œè‡ªåŠ¨è¯†åˆ«åç§°ï¼ˆä¼˜å…ˆä½¿ç”¨åˆé›†å±‚title + uploaderï¼‰
        name_to_use = (subscription.get("name") or "").strip()
        sub_type = (subscription.get("type") or "").strip()
        url_for_parse = (subscription.get("url") or "").strip()

        if (not name_to_use) and sub_type == "collection" and url_for_parse:
            try:
                # å€Ÿç”¨ cookies ä»¥æé«˜è§£ææˆåŠŸç‡
                cookie = cookie_manager.get_available_cookie(db)
                import tempfile, os
                cookies_path = None
                if cookie:
                    fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
                    os.close(fd)
                    with open(cookies_path, 'w', encoding='utf-8') as cf:
                        cf.write("# Netscape HTTP Cookie File\n")
                        cf.write("# This file was generated by bili_curator V6\n\n")
                        cf.writelines([
                            f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                            f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                            f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
                        ])

                # æ–¹æ¡ˆAï¼šä¼˜å…ˆè·å–åˆé›†å±‚ä¿¡æ¯ï¼ˆæ›´æ¥è¿‘ç½‘ç«™æ˜¾ç¤ºï¼‰ï¼štitle + uploader
                import json as json_lib
                pl_cmd = [
                    'yt-dlp',
                    '--flat-playlist',
                    '--dump-single-json',
                    '--no-download',
                ]
                if cookies_path:
                    pl_cmd += ['--cookies', cookies_path]
                pl_cmd.append(url_for_parse)

                pl_proc = await asyncio.create_subprocess_exec(
                    *pl_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                pl_stdout, pl_stderr = await pl_proc.communicate()
                if pl_proc.returncode == 0:
                    try:
                        data = json_lib.loads(pl_stdout.decode('utf-8', errors='ignore') or '{}')
                    except Exception:
                        data = {}
                    uploader = ''
                    playlist_title = ''
                    if isinstance(data, dict):
                        # Bç«™åˆé›†å±‚å¸¸è§å­—æ®µ
                        playlist_title = (data.get('title') or data.get('playlist_title') or '').strip()
                        uploader = (data.get('uploader') or data.get('channel') or '').strip()
                    # å»é‡ï¼šå¦‚æœæ ‡é¢˜å·²åŒ…å«uploaderï¼Œé¿å…é‡å¤
                    if uploader and playlist_title:
                        if playlist_title.startswith(uploader) or uploader in playlist_title:
                            name_to_use = playlist_title
                        else:
                            name_to_use = f"{uploader}ï¼š{playlist_title}"
                    elif playlist_title:
                        name_to_use = playlist_title
                    elif uploader:
                        name_to_use = f"{uploader}çš„åˆé›†"
                    # æ›´æ–°cookieä½¿ç”¨ç»Ÿè®¡
                    if cookie:
                        try:
                            cookie_manager.update_cookie_usage(db, cookie.id)
                        except Exception:
                            pass

                # æ–¹æ¡ˆBï¼šå¦‚æœªè·å¾—æœ‰æ•ˆåç§°ï¼Œå›é€€é¦–æ¡è§†é¢‘çš„ playlist_title
                if not name_to_use:
                    fe_cmd = [
                        'yt-dlp',
                        '--dump-json',
                        '--playlist-items', '1',
                        '--no-download',
                    ]
                    if cookies_path:
                        fe_cmd += ['--cookies', cookies_path]
                    fe_cmd.append(url_for_parse)
                    fe_proc = await asyncio.create_subprocess_exec(
                        *fe_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    fe_stdout, fe_stderr = await fe_proc.communicate()
                    if fe_proc.returncode == 0:
                        for line in fe_stdout.decode('utf-8', errors='ignore').strip().split('\n'):
                            if not line.strip():
                                continue
                            try:
                                info = json_lib.loads(line)
                                uploader = (info.get('uploader') or '').strip()
                                playlist_title = (info.get('playlist_title') or '').strip()
                                title = (info.get('title') or '').strip()
                                # å…¼å®¹ä¸ªåˆ«æƒ…å†µä¸‹ playlist_title ä¸ºç©ºæ—¶å›é€€ title
                                base_title = playlist_title or title
                                if uploader and base_title:
                                    if base_title.startswith(uploader) or uploader in base_title:
                                        name_to_use = base_title
                                    else:
                                        name_to_use = f"{uploader}ï¼š{base_title}"
                                elif base_title:
                                    name_to_use = base_title
                                elif uploader:
                                    name_to_use = f"{uploader}çš„åˆé›†"
                                if name_to_use:
                                    break
                            except Exception:
                                continue

                # æ¸…ç†ä¸´æ—¶ cookie æ–‡ä»¶
                if cookies_path and os.path.exists(cookies_path):
                    try:
                        os.remove(cookies_path)
                    except Exception:
                        pass

            except Exception:
                # è§£æå¤±è´¥åˆ™ç»§ç»­èµ°å…œåº•é€»è¾‘
                pass

            # å…œåº•ï¼šå– URL æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºåç§°
            if not name_to_use and url_for_parse:
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url_for_parse)
                    last_seg = (parsed.path.rstrip('/') or '/').split('/')[-1]
                    name_to_use = last_seg or parsed.netloc or "æœªçŸ¥åˆé›†"
                except Exception:
                    name_to_use = "æœªçŸ¥åˆé›†"

        # è‹¥ä»ä¸ºç©ºï¼Œä½¿ç”¨ä¼ å…¥åç§°ï¼ˆå…¼å®¹éåˆé›†ç±»å‹ï¼‰ï¼›ç¡®ä¿éç©ºä»¥æ»¡è¶³ NOT NULL çº¦æŸ
        if not name_to_use:
            name_to_use = (subscription.get("name") or "æœªçŸ¥è®¢é˜…").strip() or "æœªçŸ¥è®¢é˜…"
        
        db_subscription = Subscription(
            name=name_to_use,
            type=subscription["type"],
            url=subscription.get("url"),
            uploader_id=subscription.get("uploader_id"),
            keyword=subscription.get("keyword"),
            specific_urls=subscription.get("specific_urls"),
            date_after=date_after,
            date_before=date_before,
            min_likes=subscription.get("min_likes"),
            min_favorites=subscription.get("min_favorites"),
            min_views=subscription.get("min_views")
        )
        db.add(db_subscription)
        db.commit()
        db.refresh(db_subscription)
        
        # è‡ªåŠ¨å…³è”å·²ä¸‹è½½çš„è§†é¢‘
        try:
            from app.auto_import import auto_import_service
            matching_videos = auto_import_service._find_matching_videos(db_subscription, db)
            associated_count = 0
            
            for video in matching_videos:
                if not video.subscription_id:  # åªå…³è”æœªå…³è”çš„è§†é¢‘
                    video.subscription_id = db_subscription.id
                    associated_count += 1
            
            # æ›´æ–°è®¢é˜…ç»Ÿè®¡
            db_subscription.downloaded_videos = len([v for v in matching_videos if v.downloaded])
            db_subscription.total_videos = len(matching_videos)
            pending_videos = max(0, (db_subscription.total_videos or 0) - (db_subscription.downloaded_videos or 0))
            db_subscription.updated_at = datetime.now()
            
            db.commit()
            
            return {
                "message": "è®¢é˜…åˆ›å»ºæˆåŠŸ",
                "id": db_subscription.id,
                "associated_videos": associated_count,
                "total_videos": db_subscription.total_videos or 0,
                "downloaded_videos": db_subscription.downloaded_videos or 0,
                "pending_videos": pending_videos,
            }
        except Exception as e:
            # å¦‚æœå…³è”å¤±è´¥ï¼Œä¸å½±å“è®¢é˜…åˆ›å»º
            logger.warning(f"è®¢é˜…åˆ›å»ºæˆåŠŸï¼Œä½†è‡ªåŠ¨å…³è”å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€ç»Ÿè®¡ä¸º0ï¼Œå‰ç«¯å¯åœ¨åˆ—è¡¨æ‹‰å–æ—¶åˆ·æ–°
            return {
                "message": "è®¢é˜…åˆ›å»ºæˆåŠŸ",
                "id": db_subscription.id,
                "associated_videos": 0,
                "total_videos": 0,
                "downloaded_videos": 0,
                "pending_videos": 0,
            }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/subscriptions/{subscription_id}/expected-total")
async def get_subscription_expected_total(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–è¿œç«¯åˆé›†åº”æœ‰æ€»è®¡è§†é¢‘æ•°ï¼ˆä¸ä¾èµ–æœ¬åœ°DBï¼‰ï¼Œç”¨äºæ ¡å‡†æ˜¾ç¤ºã€‚
    ä»…å¯¹ type=collection ä¸”å­˜åœ¨ url çš„è®¢é˜…æœ‰æ•ˆã€‚
    """
    sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not sub:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    if sub.type != "collection" or not sub.url:
        raise HTTPException(status_code=400, detail="è¯¥è®¢é˜…ä¸æ˜¯åˆé›†æˆ–ç¼ºå°‘URL")

    try:
        import tempfile, os, json as json_lib
        sub_lock = get_subscription_lock(sub.id)

        async def run_expected_total(cookies_path: Optional[str], requires_cookie: bool) -> Optional[int]:
            # å…¬å…±å‚æ•°ï¼šUA/Referer/é‡è¯•/è½»ç¡çœ 
            common_args = [
                'yt-dlp',
                '--user-agent', get_user_agent(requires_cookie),
                '--referer', 'https://www.bilibili.com/',
                '--sleep-interval', '2',
                '--max-sleep-interval', '5',
                '--retries', '5',
                '--fragment-retries', '5',
                '--retry-sleep', '3',
                '--ignore-errors',
                '--no-warnings',
                '--no-download',
            ]
            if cookies_path:
                common_args += ['--cookies', cookies_path]

            async def run_and_parse(args):
                async with sub_lock:
                    async with yt_dlp_semaphore:
                        proc = await asyncio.create_subprocess_exec(
                            *args,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE
                        )
                        out, err = await proc.communicate()
                        return proc.returncode, out, err

            expected_total = None
            last_err = None
            # æ‰‹åŠ¨åˆ†é¡µ
            try:
                chunk_size = 100
                max_chunks = 50
                total_count = 0
                chunk_idx = 0
                while chunk_idx < max_chunks:
                    start = chunk_idx * chunk_size + 1
                    end = start + chunk_size - 1
                    cmd_chunk = common_args + ['--dump-json', '--flat-playlist', '--playlist-items', f'{start}-{end}', sub.url]
                    rc, out, err = await run_and_parse(cmd_chunk)
                    if rc != 0:
                        last_err = err.decode('utf-8', errors='ignore')
                        break
                    count_this = 0
                    for line in (out.decode('utf-8', errors='ignore') or '').strip().split('\n'):
                        if not line.strip():
                            continue
                        try:
                            _ = json_lib.loads(line)
                            count_this += 1
                        except json_lib.JSONDecodeError:
                            continue
                    total_count += count_this
                    if count_this < chunk_size:
                        break
                    chunk_idx += 1
                    try:
                        await asyncio.sleep(random.uniform(2, 4))
                    except Exception:
                        pass
                if total_count > 0:
                    expected_total = total_count
            except Exception as e:
                last_err = e

            # Aï¼šdump-single-json
            try:
                cmd_a = common_args + ['--flat-playlist', '--dump-single-json', sub.url]
                rc, out, err = await run_and_parse(cmd_a)
                if rc == 0:
                    try:
                        data = json_lib.loads(out.decode('utf-8', errors='ignore') or '{}')
                        if isinstance(data, dict):
                            expected_total = data.get('n_entries')
                            if expected_total is None:
                                entries = data.get('entries')
                                if isinstance(entries, list):
                                    expected_total = len(entries)
                    except Exception as e:
                        last_err = e
                else:
                    last_err = err.decode('utf-8', errors='ignore')
            except Exception as e:
                last_err = e

            # Bï¼š-J
            if expected_total is None:
                try:
                    cmd_b = common_args + ['-J', sub.url]
                    rc, out, err = await run_and_parse(cmd_b)
                    if rc == 0:
                        try:
                            data = json_lib.loads(out.decode('utf-8', errors='ignore') or '{}')
                            if isinstance(data, dict):
                                expected_total = data.get('n_entries')
                                if expected_total is None:
                                    entries = data.get('entries')
                                    if isinstance(entries, list):
                                        expected_total = len(entries)
                                if expected_total is None:
                                    pc = data.get('playlist_count')
                                    if isinstance(pc, int):
                                        expected_total = pc
                        except Exception as e:
                            last_err = e
                    else:
                        last_err = err.decode('utf-8', errors='ignore')
                except Exception as e:
                    last_err = e

            # Dï¼š--dump-json --flat-playlist
            if expected_total is None:
                try:
                    cmd_d = common_args + ['--dump-json', '--flat-playlist', sub.url]
                    rc, out, err = await run_and_parse(cmd_d)
                    if rc == 0:
                        count = 0
                        for line in (out.decode('utf-8', errors='ignore') or '').strip().split('\n'):
                            if not line.strip():
                                continue
                            try:
                                _ = json_lib.loads(line)
                                count += 1
                            except json_lib.JSONDecodeError:
                                continue
                        if count > 0:
                            expected_total = count
                    else:
                        last_err = err.decode('utf-8', errors='ignore')
                except Exception as e:
                    last_err = e

            # Cï¼šé¦–æ¡ --dump-json
            if expected_total is None:
                try:
                    cmd_c = common_args + ['--dump-json', '--playlist-items', '1', sub.url]
                    rc, out, err = await run_and_parse(cmd_c)
                    if rc == 0:
                        first_line = (out.decode('utf-8', errors='ignore') or '').strip().split('\n')[0]
                        if first_line:
                            try:
                                info = json_lib.loads(first_line)
                                for key in ('n_entries', 'playlist_count', 'playlist_entries', 'total_count'):
                                    val = info.get(key)
                                    if isinstance(val, int):
                                        expected_total = val
                                        break
                                if expected_total is None:
                                    ents = info.get('entries')
                                    if isinstance(ents, list):
                                        expected_total = len(ents)
                            except Exception as e:
                                last_err = e
                    else:
                        last_err = err.decode('utf-8', errors='ignore')
                except Exception as e:
                    last_err = e

            return expected_total

        # ç¬¬ä¸€é˜¶æ®µï¼šæ—  Cookie é€šé“
        job_nc = await request_queue.enqueue(job_type="expected_total", subscription_id=sub.id, requires_cookie=False)
        await request_queue.mark_running(job_nc)
        try:
            expected = await run_expected_total(None, requires_cookie=False)
            if expected is not None:
                await request_queue.mark_done(job_nc)
                return {"expected_total_videos": int(expected), "job_id": job_nc}
            else:
                await request_queue.mark_failed(job_nc, "need_cookie_fallback")
        except Exception as e:
            await request_queue.mark_failed(job_nc, str(e))

        # ç¬¬äºŒé˜¶æ®µï¼šCookie é€šé“ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        cookie = cookie_manager.get_available_cookie(db)
        if not cookie:
            raise HTTPException(status_code=502, detail="éœ€è¦Cookieä½†æ²¡æœ‰å¯ç”¨Cookie")

        # å†™ cookie æ–‡ä»¶
        fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
        os.close(fd)
        with open(cookies_path, 'w', encoding='utf-8') as cf:
            cf.write("# Netscape HTTP Cookie File\n")
            cf.write("# This file was generated by bili_curator V6\n\n")
            if getattr(cookie, 'sessdata', None):
                cf.write(f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n")
            if getattr(cookie, 'bili_jct', None) and str(cookie.bili_jct).strip():
                cf.write(f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n")
            if getattr(cookie, 'dedeuserid', None) and str(cookie.dedeuserid).strip():
                cf.write(f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n")

        job_c = await request_queue.enqueue(job_type="expected_total", subscription_id=sub.id, requires_cookie=True, priority=0)
        await request_queue.mark_running(job_c)
        try:
            expected2 = await run_expected_total(cookies_path, requires_cookie=True)
            if expected2 is None:
                await request_queue.mark_failed(job_c, "cookie_fallback_failed")
                raise HTTPException(status_code=502, detail="æ— æ³•è§£æåˆé›†æ€»æ•°ï¼ˆCookie å›é€€å¤±è´¥ï¼‰")
            # Cookie ä½¿ç”¨ç»Ÿè®¡
            try:
                cookie_manager.update_cookie_usage(db, cookie.id)
            except Exception:
                pass
            await request_queue.mark_done(job_c)
            return {"expected_total_videos": int(expected2), "job_id": job_c}
        finally:
            try:
                if cookies_path and os.path.exists(cookies_path):
                    os.remove(cookies_path)
            except Exception:
                pass

    except HTTPException:
        raise
    except Exception as e:
        logger.warning(f"è·å–åˆé›†æ€»æ•°å¤±è´¥: {e}")
        # å¤–å±‚å…œåº•å¼‚å¸¸ï¼Œæ­¤æ—¶å†…éƒ¨å·²å°½åŠ›æ ‡è®° job_nc/job_c çš„çŠ¶æ€
        raise HTTPException(status_code=500, detail="è·å–åˆé›†æ€»æ•°å¤±è´¥")

@app.get("/api/subscriptions/{subscription_id}")
async def get_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–å•ä¸ªè®¢é˜…è¯¦æƒ…"""
    subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")

    # Calculate statistics
    total_videos = db.query(Video).filter(Video.subscription_id == subscription.id).count()
    downloaded_videos = db.query(Video).filter(
        Video.subscription_id == subscription.id,
        Video.video_path.isnot(None)
    ).count()
    pending_videos = max(0, total_videos - downloaded_videos)

    return {
        "id": subscription.id,
        "name": subscription.name,
        "type": subscription.type,
        "url": subscription.url,
        "uploader_id": subscription.uploader_id,
        "keyword": subscription.keyword,
        "specific_urls": subscription.specific_urls,
        "date_after": subscription.date_after.isoformat() if subscription.date_after else None,
        "date_before": subscription.date_before.isoformat() if subscription.date_before else None,
        "min_likes": subscription.min_likes,
        "min_favorites": subscription.min_favorites,
        "min_views": subscription.min_views,
        "total_videos": total_videos,
        "downloaded_videos": downloaded_videos,
        "pending_videos": pending_videos,
        "is_active": subscription.is_active,
        "last_check": subscription.last_check.isoformat() if subscription.last_check else None,
        "created_at": subscription.created_at.isoformat() if subscription.created_at else None,
        "updated_at": subscription.updated_at.isoformat() if subscription.updated_at else None
    }

@app.put("/api/subscriptions/{subscription_id}")
async def update_subscription(
    subscription_id: int,
    subscription: SubscriptionUpdate,
    db: Session = Depends(get_db)
):
    """æ›´æ–°è®¢é˜…"""
    db_subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not db_subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    
    # æ›´æ–°å­—æ®µï¼ˆå…¼å®¹ active -> is_activeï¼‰
    data = subscription.dict(exclude_unset=True)
    # æå–å¹¶è§„èŒƒ is_active
    is_active_value = data.pop("is_active", None)
    if is_active_value is None and "active" in data:
        is_active_value = data.pop("active")

    # è®¾ç½®å…¶ä½™å­—æ®µ
    for field, value in data.items():
        setattr(db_subscription, field, value)

    # å•ç‹¬å¤„ç† is_active
    if is_active_value is not None:
        db_subscription.is_active = is_active_value
    
    db_subscription.updated_at = datetime.now()
    db.commit()
    
    return {"message": "è®¢é˜…æ›´æ–°æˆåŠŸ"}

@app.delete("/api/subscriptions/{subscription_id}")
async def delete_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤è®¢é˜…"""
    db_subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not db_subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    
    db.delete(db_subscription)
    db.commit()
    
    return {"message": "è®¢é˜…åˆ é™¤æˆåŠŸ"}

@app.post("/api/subscriptions/parse-collection")
async def parse_collection_info(request: dict, db: Session = Depends(get_db)):
    """è§£æåˆé›†URLï¼Œè‡ªåŠ¨è¯†åˆ«åˆé›†åç§°"""
    url = request.get('url')
    if not url:
        raise HTTPException(status_code=400, detail="URLä¸èƒ½ä¸ºç©º")
    
    try:
        import json as json_lib
        import tempfile, os

        async def run_parse(cookies_path: Optional[str], requires_cookie: bool) -> Optional[str]:
            # A. åˆé›†å±‚
            pl_cmd = [
                'yt-dlp',
                '--flat-playlist',
                '--dump-single-json',
                '--no-download',
                '--user-agent', get_user_agent(requires_cookie),
                url
            ]
            if cookies_path:
                pl_cmd += ['--cookies', cookies_path]
            async with yt_dlp_semaphore:
                pl_proc = await asyncio.create_subprocess_exec(
                    *pl_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                pl_stdout, pl_stderr = await pl_proc.communicate()
            if pl_proc.returncode == 0:
                try:
                    data = json_lib.loads(pl_stdout.decode('utf-8', errors='ignore') or '{}')
                except Exception:
                    data = {}
                uploader = ''
                playlist_title = ''
                if isinstance(data, dict):
                    playlist_title = (data.get('title') or data.get('playlist_title') or '').strip()
                    uploader = (data.get('uploader') or data.get('channel') or '').strip()
                name = None
                if uploader and playlist_title:
                    if playlist_title.startswith(uploader) or uploader in playlist_title:
                        name = playlist_title
                    else:
                        name = f"{uploader}ï¼š{playlist_title}"
                elif playlist_title:
                    name = playlist_title
                elif uploader:
                    name = f"{uploader}çš„åˆé›†"
                if name:
                    return name

            # B. é¦–æ¡è§†é¢‘
            fe_cmd = [
                'yt-dlp',
                '--dump-json',
                '--playlist-items', '1',
                '--no-download',
                '--user-agent', get_user_agent(requires_cookie),
                url
            ]
            if cookies_path:
                fe_cmd += ['--cookies', cookies_path]
            async with yt_dlp_semaphore:
                fe_proc = await asyncio.create_subprocess_exec(
                    *fe_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                fe_stdout, fe_stderr = await fe_proc.communicate()
            if fe_proc.returncode == 0:
                for line in fe_stdout.decode('utf-8', errors='ignore').strip().split('\n'):
                    if not line.strip():
                        continue
                    try:
                        info = json_lib.loads(line)
                        uploader = (info.get('uploader') or '').strip()
                        playlist_title = (info.get('playlist_title') or '').strip()
                        title = (info.get('title') or '').strip()
                        base_title = playlist_title or title
                        name = None
                        if uploader and base_title:
                            if base_title.startswith(uploader) or uploader in base_title:
                                name = base_title
                            else:
                                name = f"{uploader}ï¼š{base_title}"
                        elif base_title:
                            name = base_title
                        elif uploader:
                            name = f"{uploader}çš„åˆé›†"
                        if name:
                            return name
                    except Exception:
                        continue
            return None

        # ç¬¬ä¸€é˜¶æ®µï¼šæ—  Cookie è§£æ
        job_nc = await request_queue.enqueue(job_type="parse", subscription_id=None, requires_cookie=False)
        await request_queue.mark_running(job_nc)
        name_nc = await run_parse(None, requires_cookie=False)
        if name_nc:
            await request_queue.mark_done(job_nc)
            return {"name": name_nc}
        else:
            await request_queue.mark_failed(job_nc, "need_cookie_fallback")

        # ç¬¬äºŒé˜¶æ®µï¼šCookie é€šé“ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        cookie = cookie_manager.get_available_cookie(db)
        if not cookie:
            return {"error": "éœ€è¦Cookieä½†æ²¡æœ‰å¯ç”¨Cookie"}
        fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
        os.close(fd)
        with open(cookies_path, 'w', encoding='utf-8') as cf:
            cf.write("# Netscape HTTP Cookie File\n")
            cf.write("# This file was generated by bili_curator V6\n\n")
            cf.writelines([
                f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
            ])
        job_c = await request_queue.enqueue(job_type="parse", subscription_id=None, requires_cookie=True, priority=0)
        await request_queue.mark_running(job_c)
        try:
            name_c = await run_parse(cookies_path, requires_cookie=True)
            if name_c:
                try:
                    cookie_manager.update_cookie_usage(db, cookie.id)
                except Exception:
                    pass
                await request_queue.mark_done(job_c)
                return {"name": name_c}
            else:
                await request_queue.mark_failed(job_c, "parse_failed")
                return {"error": "è§£æå¤±è´¥"}
        finally:
            try:
                if cookies_path and os.path.exists(cookies_path):
                    os.remove(cookies_path)
            except Exception:
                pass
    except Exception as e:
        # å†…éƒ¨å·²å¯¹ job_nc/job_c åšçŠ¶æ€æ ‡è®°ï¼Œè¿™é‡Œä»…è¿”å›é”™è¯¯
        return {"error": f"è§£æåˆé›†ä¿¡æ¯å¤±è´¥: {str(e)}"}

@app.post("/api/subscriptions/{subscription_id}/download")
async def start_download(subscription_id: int, db: Session = Depends(get_db)):
    """æ‰‹åŠ¨å¯åŠ¨ä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    
    try:
        # ä½¿ç”¨å¢å¼ºä»»åŠ¡ç®¡ç†å™¨å¯åŠ¨ä¸‹è½½
        task_id = await enhanced_task_manager.start_subscription_download(subscription_id)
        return {"message": "ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/tasks/{task_id}/pause")
async def pause_task(task_id: str):
    """æš‚åœä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    success = await enhanced_task_manager.pause_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•æš‚åœ")
    
    return {"message": "ä»»åŠ¡å·²æš‚åœ"}

@app.post("/api/tasks/{task_id}/resume")
async def resume_task(task_id: str):
    """æ¢å¤ä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    success = await enhanced_task_manager.resume_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•æ¢å¤")
    
    return {"message": "ä»»åŠ¡å·²æ¢å¤"}

@app.post("/api/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    success = await enhanced_task_manager.cancel_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•å–æ¶ˆ")
    
    return {"message": "ä»»åŠ¡å·²å–æ¶ˆ"}

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    from .task_manager import enhanced_task_manager
    
    task_status = enhanced_task_manager.get_task_status(task_id)
    if not task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return task_status

@app.get("/api/tasks")
async def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    from .task_manager import enhanced_task_manager
    
    return enhanced_task_manager.get_all_tasks()

# â€”â€” å…¨å±€è¯·æ±‚é˜Ÿåˆ—åªè¯»æ¥å£ â€”â€”
@app.get("/api/requests")
async def list_requests():
    items = request_queue.list()
    return {"count": len(items), "items": items}

@app.get("/api/requests/{job_id}")
async def get_request(job_id: str):
    item = request_queue.get(job_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¯·æ±‚ä¸å­˜åœ¨")
    return item

# â€”â€” å…¨å±€è¯·æ±‚é˜Ÿåˆ—ç®¡ç†ä¸å¯è§‚æµ‹æ¥å£ â€”â€”
@app.get("/api/queue/stats")
def get_queue_stats():
    try:
        return request_queue.stats()
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/queue/pause")
async def pause_queue(scope: str = "all"):
    try:
        await request_queue.pause(scope)
        return {"ok": True, "scope": scope, "stats": request_queue.stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/queue/resume")
async def resume_queue(scope: str = "all"):
    try:
        await request_queue.resume(scope)
        return {"ok": True, "scope": scope, "stats": request_queue.stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/requests/{job_id}/cancel")
async def cancel_request(job_id: str, reason: str = ""):
    ok = await request_queue.cancel(job_id, reason)
    if not ok:
        raise HTTPException(status_code=404, detail="not_found")
    return {"ok": True, "job": request_queue.get(job_id)}

@app.post("/api/requests/{job_id}/prioritize")
async def prioritize_request(job_id: str, priority: int = 0):
    ok = await request_queue.prioritize(job_id, priority)
    if not ok:
        raise HTTPException(status_code=404, detail="not_found")
    return {"ok": True, "job": request_queue.get(job_id)}

@app.post("/api/queue/capacity")
async def set_queue_capacity(requires_cookie: Optional[int] = None, no_cookie: Optional[int] = None):
    try:
        await request_queue.set_capacity(requires_cookie=requires_cookie, no_cookie=no_cookie)
        return {"ok": True, "stats": request_queue.stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/subscriptions/{subscription_id}/tasks")
async def get_subscription_tasks(subscription_id: int):
    """è·å–æŒ‡å®šè®¢é˜…çš„æ‰€æœ‰ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    return enhanced_task_manager.get_subscription_tasks(subscription_id)

# è§†é¢‘ç®¡ç†API
@app.get("/api/videos")
async def get_videos(
    page: int = 1,
    size: int = 20,
    subscription_id: Optional[int] = None,
    db: Session = Depends(get_db)
):
    """è·å–è§†é¢‘åˆ—è¡¨"""
    query = db.query(Video)
    
    if subscription_id:
        query = query.filter(Video.subscription_id == subscription_id)
    
    total = query.count()
    videos = query.order_by(Video.created_at.desc()).offset((page - 1) * size).limit(size).all()
    
    return {
        "total": total,
        "page": page,
        "size": size,
        "videos": [
            {
                "id": video.id,
                "bilibili_id": video.bilibili_id,
                "title": video.title,
                "uploader": video.uploader,
                "duration": video.duration,
                "upload_date": video.upload_date.isoformat() if video.upload_date else None,
                "video_path": video.video_path,
                "file_path": video.video_path,
                "file_size": video.file_size,
                "downloaded": video.downloaded,
                "subscription_id": video.subscription_id,
                "created_at": video.created_at.isoformat() if video.created_at else None,
                "updated_at": video.updated_at.isoformat() if video.updated_at else None
            }
            for video in videos
        ]
    }

# åª’ä½“ç»Ÿè®¡ä¸è®¢é˜…ç»´åº¦ç»Ÿè®¡
@app.get("/api/media/overview")
async def get_media_overview(scan: bool = False, db: Session = Depends(get_db)):
    """åª’ä½“ç›®å½•æ€»è§ˆç»Ÿè®¡
    - é»˜è®¤åŸºäºæ•°æ®åº“å¿«é€Ÿæ±‡æ€»ï¼ˆæ›´è½»é‡ï¼‰
    - å½“ scan=true æ—¶ï¼Œé¢å¤–æ‰«æ DOWNLOAD_PATH è®¡ç®—å®é™…å ç”¨ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰
    """
    total_videos = db.query(func.count(Video.id)).scalar() or 0
    downloaded_videos = db.query(func.count(Video.id)).filter(Video.video_path.isnot(None)).scalar() or 0
    total_size_db = db.query(func.coalesce(func.sum(Video.file_size), 0)).scalar() or 0

    overview: Dict[str, Any] = {
        "total_videos": total_videos,
        "downloaded_videos": downloaded_videos,
        "total_size": int(total_size_db),
        "scan": False,
    }

    if scan:
        download_root = Path(os.getenv('DOWNLOAD_PATH', '/app/downloads'))
        size_on_disk = 0
        files_on_disk = 0
        if download_root.exists():
            for p in download_root.rglob("*"):
                try:
                    if p.is_file():
                        files_on_disk += 1
                        size_on_disk += p.stat().st_size
                except Exception:
                    # å¿½ç•¥ä¸ªåˆ«æ–‡ä»¶è¯»å–å¼‚å¸¸
                    continue
        overview.update({
            "files_on_disk": files_on_disk,
            "size_on_disk": int(size_on_disk),
            "scan": True,
            "download_path": str(download_root),
        })

    return overview


@app.get("/api/media/subscription-stats")
async def get_subscription_stats(db: Session = Depends(get_db)):
    """æŒ‰è®¢é˜…èšåˆç»Ÿè®¡ï¼ˆæ•°é‡ã€å·²ä¸‹è½½ã€å®¹é‡ã€æœ€è¿‘ä¸Šä¼ ï¼‰"""
    # æ•°é‡ä¸å®¹é‡
    counts = (
        db.query(
            Video.subscription_id.label('sid'),
            func.count(Video.id).label('total'),
            func.sum(case((Video.video_path.isnot(None), 1), else_=0)).label('downloaded'),
            func.coalesce(func.sum(Video.file_size), 0).label('size'),
            func.max(Video.upload_date).label('latest_upload')
        )
        .group_by(Video.subscription_id)
        .all()
    )

    # è®¢é˜…åç§°
    subs = {s.id: s for s in db.query(Subscription).all()}

    result = []
    for row in counts:
        sid = row.sid
        sub = subs.get(sid)
        result.append({
            "subscription_id": sid,
            "subscription_name": sub.name if sub else None,
            "type": sub.type if sub else None,
            "total_videos": int(row.total or 0),
            "downloaded_videos": int(row.downloaded or 0),
            "total_size": int(row.size or 0),
            "latest_upload": row.latest_upload.isoformat() if row.latest_upload else None,
        })

    # ç¡®ä¿åŒ…å«æ²¡æœ‰è§†é¢‘è®°å½•ä½†å­˜åœ¨çš„è®¢é˜…ï¼ˆtotal=0ï¼‰
    for sid, sub in subs.items():
        if not any(item["subscription_id"] == sid for item in result):
            result.append({
                "subscription_id": sid,
                "subscription_name": sub.name,
                "type": sub.type,
                "total_videos": 0,
                "downloaded_videos": 0,
                "total_size": 0,
                "latest_upload": None,
            })

    # æŒ‰ä¸‹è½½æ•°é‡æˆ–å¤§å°æ’åºï¼Œä¾¿äºå‰ç«¯é»˜è®¤å±•ç¤º
    result.sort(key=lambda x: (x["downloaded_videos"], x["total_size"]), reverse=True)
    return result


@app.get("/api/media/subscriptions/{subscription_id}/videos")
async def get_subscription_videos_detail(
    subscription_id: int,
    page: int = 1,
    size: int = 20,
    include_disk: bool = True,
    db: Session = Depends(get_db)
):
    """è®¢é˜…ä¸‹è§†é¢‘æ˜ç»†ï¼ˆåˆ†é¡µï¼‰
    - include_disk: æ˜¯å¦æ ‡è®° on_diskï¼ˆåŸºäº video_path å­˜åœ¨æ€§ï¼‰
    """
    query = db.query(Video).filter(Video.subscription_id == subscription_id)
    total = query.count()
    items = (
        query.order_by(Video.created_at.desc())
        .offset((page - 1) * size)
        .limit(size)
        .all()
    )

    def to_item(v: Video) -> Dict[str, Any]:
        on_disk = None
        if include_disk:
            try:
                on_disk = bool(v.video_path and os.path.exists(v.video_path))
            except Exception:
                on_disk = False
        return {
            "id": v.id,
            "bilibili_id": v.bilibili_id,
            "title": v.title,
            "uploader": v.uploader,
            "duration": v.duration,
            "upload_date": v.upload_date.isoformat() if v.upload_date else None,
            "video_path": v.video_path,
            "file_size": v.file_size,
            "downloaded": v.downloaded,
            "subscription_id": v.subscription_id,
            "created_at": v.created_at.isoformat() if v.created_at else None,
            "updated_at": v.updated_at.isoformat() if v.updated_at else None,
            "on_disk": on_disk,
        }

    return {
        "total": total,
        "page": page,
        "size": size,
        "videos": [to_item(v) for v in items],
    }


# â€”â€” æŒ‰ä¸‹è½½ç›®å½•èšåˆç»Ÿè®¡ä¸ç›®å½•è§†é¢‘åˆ†é¡µ â€”â€”
@app.get("/api/media/directories")
async def get_directory_stats(db: Session = Depends(get_db)):
    """æŒ‰ä¸‹è½½æ ¹ç›®å½•ä¸‹çš„ä¸€çº§ç›®å½•èšåˆç»Ÿè®¡
    - åŸºäº DOWNLOAD_PATH çš„ç›´æ¥å­ç›®å½•è¿›è¡Œåˆ†ç»„
    - ç»Ÿè®¡: total_videosã€downloaded_videosã€total_size
    """
    download_root = Path(os.getenv('DOWNLOAD_PATH', '/app/downloads')).resolve()
    # ä»…ç»Ÿè®¡æœ‰æ–‡ä»¶è·¯å¾„çš„è§†é¢‘
    videos = db.query(Video).filter(Video.video_path.isnot(None)).all()
    stats: Dict[str, Dict[str, Any]] = {}

    for v in videos:
        try:
            vp = Path(v.video_path).resolve()
            rel = vp.relative_to(download_root)
            first = rel.parts[0] if len(rel.parts) > 0 else ""
        except Exception:
            # è·¯å¾„ä¸åœ¨ä¸‹è½½æ ¹å†…ï¼Œå½’ä¸º _others
            first = "_others"
        if not first:
            first = "_root"
        s = stats.setdefault(first, {
            "dir": first,
            "total_videos": 0,
            "downloaded_videos": 0,
            "total_size": 0,
        })
        s["total_videos"] += 1
        if v.video_path:
            s["downloaded_videos"] += 1
        # ä¼˜å…ˆä½¿ç”¨ total_sizeï¼›å¦åˆ™å›é€€åˆ° file_size + audio_size
        size_sum = (v.total_size if getattr(v, 'total_size', None) is not None else None)
        if size_sum is None:
            size_sum = int(v.file_size or 0) + int((getattr(v, 'audio_size', 0) or 0))
        s["total_size"] += int(size_sum or 0)

    # è½¬åˆ—è¡¨ï¼ŒæŒ‰å¤§å°/æ•°é‡å€’åº
    result = list(stats.values())
    result.sort(key=lambda x: (x["total_size"], x["downloaded_videos"], x["total_videos"]), reverse=True)
    return {
        "download_path": str(download_root),
        "items": result,
        "total_dirs": len(result),
    }


# â€”â€” å›å¡«ä¸æ ¡å‡†ï¼šåˆ·æ–°è§†é¢‘/éŸ³é¢‘å¤§å°ï¼Œå†™å› DB â€”â€”
@app.post("/api/media/refresh-sizes")
async def refresh_media_sizes(background: BackgroundTasks):
    """åå°å›å¡«ç£ç›˜å¤§å°ï¼ˆè§†é¢‘+éŸ³é¢‘ï¼‰ï¼Œå†™å›åˆ° videos.file_size / audio_size / total_sizeã€‚
    - éé˜»å¡ï¼šå¯åŠ¨åå°ä»»åŠ¡åç«‹å³è¿”å›ã€‚
    - æ‰«æç­–ç•¥ï¼šä»…å¤„ç†æœ‰ video_path çš„è®°å½•ï¼›éŸ³é¢‘å°è¯•åŒ¹é…åŒå .m4aã€‚
    """

    def _find_audio_path(video_path: str) -> Optional[str]:
        try:
            p = Path(video_path)
            stem = p.with_suffix("").name
            candidate = p.with_name(stem + ".m4a")
            if candidate.exists():
                return str(candidate)
        except Exception:
            return None
        return None

    def _worker():
        from .models import db as _db, Video as _Video
        session = _db.get_session()
        updated = 0
        try:
            q = session.query(_Video).filter(_Video.video_path.isnot(None))
            for v in q.yield_per(200):
                v_path = (v.video_path or '').strip()
                if not v_path:
                    continue
                video_size = None
                audio_size = None
                try:
                    if os.path.exists(v_path):
                        video_size = os.path.getsize(v_path)
                except Exception:
                    video_size = None
                # ä»…å½“å®¹å™¨ç¼ºå°‘éŸ³è½¨æ—¶ï¼Œæ‰ç»Ÿè®¡æ—è·¯ .m4a çš„ä½“ç§¯ï¼Œé¿å…åŒé‡ç»Ÿè®¡
                def _has_audio_track(pth: str) -> bool:
                    try:
                        import subprocess
                        proc = subprocess.run(['ffprobe', '-v', 'error', '-select_streams', 'a', '-show_entries', 'stream=index', '-of', 'csv=p=0', pth],
                                              stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                        return any(line.strip() for line in (proc.stdout or '').splitlines())
                    except Exception:
                        # æ¢æµ‹å¤±è´¥æ—¶ï¼Œä¿å®ˆè®¤ä¸ºâ€œæœ‰éŸ³è½¨â€ï¼Œä»¥é¿å…æŠŠæ®‹ç•™ m4a è®¡å…¥å¯¼è‡´å®¹é‡è™šé«˜
                        return True

                try:
                    # åªæœ‰å½“è§†é¢‘å­˜åœ¨è€Œä¸”â€œæ— éŸ³è½¨â€æ—¶ï¼Œå°è¯•ç»Ÿè®¡åŒå m4a
                    if v_path and os.path.exists(v_path) and (not _has_audio_track(v_path)):
                        a_path = _find_audio_path(v_path)
                        if a_path and os.path.exists(a_path):
                            audio_size = os.path.getsize(a_path)
                        else:
                            audio_size = 0
                    else:
                        # æœ‰éŸ³è½¨æˆ–è§†é¢‘ä¸å­˜åœ¨ï¼šä¸è®¡æ—è·¯éŸ³é¢‘
                        audio_size = 0 if video_size is not None else None
                except Exception:
                    audio_size = 0 if video_size is not None else None

                # è‹¥ä¸¤è€…çš†ä¸ºç©ºï¼Œè·³è¿‡
                if video_size is None and audio_size is None:
                    continue

                new_file_size = int(video_size) if video_size is not None else (v.file_size or 0)
                new_audio_size = int(audio_size) if audio_size is not None else (getattr(v, 'audio_size', 0) or 0)
                new_total = int(new_file_size) + int(new_audio_size)

                changed = False
                if v.file_size != new_file_size:
                    v.file_size = new_file_size
                    changed = True
                # å…¼å®¹æ—§åº“æ— å­—æ®µçš„æƒ…å†µï¼šgetattr/setattr
                if getattr(v, 'audio_size', None) != new_audio_size:
                    try:
                        setattr(v, 'audio_size', new_audio_size)
                        changed = True
                    except Exception:
                        pass
                if getattr(v, 'total_size', None) != new_total:
                    try:
                        setattr(v, 'total_size', new_total)
                        changed = True
                    except Exception:
                        pass
                if changed:
                    updated += 1
                    # åˆ†æ‰¹æäº¤ï¼Œé™ä½äº‹åŠ¡ä½“ç§¯
                    if updated % 200 == 0:
                        try:
                            session.commit()
                        except Exception:
                            session.rollback()
            try:
                session.commit()
            except Exception:
                session.rollback()
        finally:
            session.close()

    # åå°æ‰§è¡Œ
    background.add_task(_worker)
    return {"message": "åˆ·æ–°ä»»åŠ¡å·²å¯åŠ¨", "status": "started"}


@app.get("/api/media/directory-videos")
async def get_directory_videos(dir: str, page: int = 1, size: int = 20, db: Session = Depends(get_db)):
    """è·å–æŸä¸€çº§ç›®å½•ä¸‹çš„å…¨éƒ¨è§†é¢‘ï¼ˆå«å­ç›®å½•ï¼‰ï¼Œåˆ†é¡µè¿”å›
    - å‚æ•° dir: DOWNLOAD_PATH ä¸‹çš„ä¸€çº§ç›®å½•åï¼ˆä¸ get_directory_stats è¿”å›çš„ dir å¯¹åº”ï¼‰
    - é€šè¿‡ SQL LIKE å‰ç¼€åŒ¹é…æé«˜æ•ˆç‡
    """
    if not dir:
        raise HTTPException(status_code=400, detail="dir ä¸èƒ½ä¸ºç©º")
    download_root = Path(os.getenv('DOWNLOAD_PATH', '/app/downloads')).resolve()
    base = (download_root / dir).resolve()
    # å®‰å…¨æ ¡éªŒï¼Œé˜²æ­¢ç›®å½•ç©¿è¶Š
    try:
        base.relative_to(download_root)
    except Exception:
        raise HTTPException(status_code=400, detail="éæ³•ç›®å½•")

    # SQLite å‰ç¼€åŒ¹é…
    prefix = str(base) + os.sep
    query = db.query(Video).filter(Video.video_path.isnot(None), Video.video_path.like(f"{prefix}%"))
    total = query.count()
    items = (
        query.order_by(Video.created_at.desc())
        .offset((page - 1) * size)
        .limit(size)
        .all()
    )

    def to_item(v: Video) -> Dict[str, Any]:
        return {
            "id": v.id,
            "bilibili_id": v.bilibili_id,
            "title": v.title,
            "uploader": v.uploader,
            "duration": v.duration,
            "upload_date": v.upload_date.isoformat() if v.upload_date else None,
            "video_path": v.video_path,
            "file_size": v.file_size,
            "downloaded": v.downloaded,
            "subscription_id": v.subscription_id,
            "created_at": v.created_at.isoformat() if v.created_at else None,
            "updated_at": v.updated_at.isoformat() if v.updated_at else None,
        }

    return {
        "dir": dir,
        "download_path": str(download_root),
        "total": total,
        "page": page,
        "size": size,
        "videos": [to_item(v) for v in items],
    }

@app.delete("/api/videos/{video_id}")
async def delete_video(video_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤è§†é¢‘è®°å½•"""
    video = db.query(Video).filter(Video.id == video_id).first()
    if not video:
        raise HTTPException(status_code=404, detail="è§†é¢‘ä¸å­˜åœ¨")
    
    db.delete(video)
    db.commit()
    
    return {"message": "è§†é¢‘è®°å½•åˆ é™¤æˆåŠŸ"}

# è‡ªåŠ¨å¯¼å…¥ä¸å…³è”API
@app.post("/api/auto-import/scan")
async def auto_import_scan():
    """æ‰«æä¸‹è½½ç›®å½•å¹¶å°†æœªå…¥åº“çš„è§†é¢‘å¯¼å…¥æ•°æ®åº“"""
    try:
        from .auto_import import auto_import_service
        result = auto_import_service.scan_and_import()
        return {"message": "æ‰«æå®Œæˆ", **result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/auto-import/associate")
async def auto_import_associate():
    """å°†å·²å…¥åº“è§†é¢‘æŒ‰è®¢é˜…è§„åˆ™è‡ªåŠ¨å…³è”ï¼Œåˆ·æ–°ç»Ÿè®¡"""
    try:
        from .auto_import import auto_import_service
        result = auto_import_service.auto_associate_subscriptions()
        return {"message": "å…³è”å®Œæˆ", **result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/subscriptions/{subscription_id}/associate")
async def associate_single_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """ä»…å¯¹æŒ‡å®šè®¢é˜…æ‰§è¡Œè‡ªåŠ¨å…³è”å¹¶è¿”å›è¯¥è®¢é˜…çš„æœ€æ–°ç»Ÿè®¡"""
    try:
        sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
        if not sub:
            raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
        from .auto_import import auto_import_service
        matches = auto_import_service._find_matching_videos(sub, db)
        for v in matches:
            if not v.subscription_id:
                v.subscription_id = sub.id
        # åˆ·æ–°ç»Ÿè®¡
        total_videos = db.query(Video).filter(Video.subscription_id == sub.id).count()
        downloaded_videos = db.query(Video).filter(
            Video.subscription_id == sub.id,
            Video.video_path.isnot(None)
        ).count()
        sub.total_videos = total_videos
        sub.downloaded_videos = downloaded_videos
        sub.updated_at = datetime.now()
        db.commit()
        pending_videos = max(0, total_videos - downloaded_videos)
        return {
            "message": "å…³è”å®Œæˆ",
            "id": sub.id,
            "total_videos": total_videos,
            "downloaded_videos": downloaded_videos,
            "pending_videos": pending_videos,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ä»»åŠ¡ç®¡ç†API
@app.get("/api/tasks")
async def get_tasks(db: Session = Depends(get_db)):
    """è·å–ä¸‹è½½ä»»åŠ¡åˆ—è¡¨"""
    tasks = db.query(DownloadTask).order_by(DownloadTask.created_at.desc()).limit(50).all()
    
    return [
        {
            "id": task.id,
            "bilibili_id": task.bilibili_id,
            "subscription_id": task.subscription_id,
            "status": task.status,
            "progress": task.progress,
            "error_message": task.error_message,
            "started_at": task.started_at.isoformat() if task.started_at else None,
            "completed_at": task.completed_at.isoformat() if task.completed_at else None,
            "created_at": task.created_at.isoformat() if task.created_at else None
        }
        for task in tasks
    ]

@app.get("/api/tasks/{task_id}/status")
async def get_task_status(task_id: str):
    """è·å–æ‰‹åŠ¨ä»»åŠ¡çŠ¶æ€"""
    status = task_manager.get_task_status(task_id)
    if status['status'] == 'not_found':
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return status

@app.post("/api/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆæ‰‹åŠ¨ä»»åŠ¡"""
    success = task_manager.cancel_task(task_id)
    if not success:
        raise HTTPException(status_code=400, detail="æ— æ³•å–æ¶ˆä»»åŠ¡")
    
    return {"message": "ä»»åŠ¡å·²å–æ¶ˆ"}

@app.post("/api/tasks/clear-completed")
async def clear_completed_tasks(db: Session = Depends(get_db)):
    """æ¸…ç†å·²å®Œæˆ/å¤±è´¥/å–æ¶ˆçš„ä»»åŠ¡è®°å½•ï¼Œå¹¶æ¸…ç†å†…å­˜ä¸­è¿‡æœŸä»»åŠ¡"""
    try:
        # ç»Ÿè®¡æ•°é‡
        from sqlalchemy import or_
        q = db.query(DownloadTask).filter(
            or_(
                DownloadTask.status == 'completed',
                DownloadTask.status == 'failed',
                DownloadTask.status == 'cancelled'
            )
        )
        count = q.count()
        q.delete(synchronize_session=False)
        db.commit()

        # æ¸…ç†å†…å­˜ä»»åŠ¡ï¼ˆç«‹å³æ¸…ç†ï¼‰
        try:
            from .task_manager import enhanced_task_manager
            enhanced_task_manager.cleanup_completed_tasks(hours=0)
        except Exception:
            pass

        return {"message": "æ¸…ç†å®Œæˆ", "cleared": count}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Cookieç®¡ç†API
@app.get("/api/cookies")
async def get_cookies(db: Session = Depends(get_db)):
    """è·å–Cookieåˆ—è¡¨"""
    cookies = db.query(Cookie).order_by(Cookie.created_at.desc()).all()
    
    return [
        {
            "id": cookie.id,
            "name": cookie.name,
            "sessdata": cookie.sessdata,
            "bili_jct": cookie.bili_jct,
            "dedeuserid": cookie.dedeuserid,
            "is_active": cookie.is_active,
            "usage_count": cookie.usage_count,
            "last_used": cookie.last_used.isoformat() if cookie.last_used else None,
            "created_at": cookie.created_at.isoformat() if cookie.created_at else None,
            "updated_at": cookie.updated_at.isoformat() if cookie.updated_at else None
        }
        for cookie in cookies
    ]

@app.get("/api/cookies/{cookie_id}")
async def get_cookie(cookie_id: int, db: Session = Depends(get_db)):
    """è·å–å•ä¸ªCookieè¯¦æƒ…"""
    cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    return {
        "id": cookie.id,
        "name": cookie.name,
        "sessdata": cookie.sessdata,
        "bili_jct": cookie.bili_jct,
        "dedeuserid": cookie.dedeuserid,
        "is_active": cookie.is_active,
        "usage_count": cookie.usage_count,
        "last_used": cookie.last_used.isoformat() if cookie.last_used else None,
        "created_at": cookie.created_at.isoformat() if cookie.created_at else None,
        "updated_at": cookie.updated_at.isoformat() if cookie.updated_at else None
    }

@app.post("/api/cookies")
async def create_cookie(cookie: CookieCreate, db: Session = Depends(get_db)):
    """æ·»åŠ æ–°Cookie"""
    db_cookie = Cookie(
        name=cookie.name,
        sessdata=cookie.sessdata,
        bili_jct=cookie.bili_jct,
        dedeuserid=cookie.dedeuserid,
        created_at=datetime.now()
    )
    
    db.add(db_cookie)
    db.commit()
    db.refresh(db_cookie)
    
    return {"id": db_cookie.id, "message": "Cookieæ·»åŠ æˆåŠŸ"}

@app.put("/api/cookies/{cookie_id}")
async def update_cookie(
    cookie_id: int,
    cookie: CookieUpdate,
    db: Session = Depends(get_db)
):
    """æ›´æ–°Cookie"""
    db_cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not db_cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    # æ›´æ–°å­—æ®µï¼ˆå…¼å®¹ active -> is_activeï¼‰
    data = cookie.dict(exclude_unset=True)
    # æå–å¹¶è§„èŒƒ is_active
    is_active_value = data.pop("is_active", None)
    if is_active_value is None and "active" in data:
        is_active_value = data.pop("active")

    # è®¾ç½®å…¶ä½™å­—æ®µ
    for field, value in data.items():
        setattr(db_cookie, field, value)

    # å•ç‹¬å¤„ç† is_active
    if is_active_value is not None:
        db_cookie.is_active = is_active_value
    
    db_cookie.updated_at = datetime.now()
    db.commit()
    
    return {"message": "Cookieæ›´æ–°æˆåŠŸ"}

@app.delete("/api/cookies/{cookie_id}")
async def delete_cookie(cookie_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤Cookie"""
    db_cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not db_cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    db.delete(db_cookie)
    db.commit()
    
    return {"message": "Cookieåˆ é™¤æˆåŠŸ"}

@app.post("/api/cookies/{cookie_id}/validate")
async def validate_cookie(cookie_id: int, db: Session = Depends(get_db)):
    """éªŒè¯Cookie"""
    db_cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not db_cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    is_valid = await cookie_manager.validate_cookie(db_cookie)
    # æ ¹æ®ç»“æœæ›´æ–°å¤±è´¥è®¡æ•°æˆ–é‡ç½®
    if is_valid:
        try:
            cookie_manager.reset_failures(db, db_cookie.id)
        except Exception:
            pass
    else:
        try:
            cookie_manager.record_failure(db, db_cookie.id, "éªŒè¯å¤±è´¥")
        except Exception:
            # è€åº“ä¸æ”¯æŒå¤±è´¥å­—æ®µåˆ™å¯èƒ½å·²è¢«ç›´æ¥ç¦ç”¨
            pass

    # è¯»å–å½“å‰å¤±è´¥è®¡æ•°ï¼ˆè‹¥å­˜åœ¨ï¼‰
    failure_info = {}
    if hasattr(db_cookie, 'failure_count'):
        failure_info = {
            "failure_count": db_cookie.failure_count,
            "last_failure_at": db_cookie.last_failure_at.isoformat() if db_cookie.last_failure_at else None,
            "is_active": db_cookie.is_active,
        }
    
    return {"valid": is_valid, "message": "éªŒè¯å®Œæˆ", **failure_info}

# ç³»ç»Ÿè®¾ç½®API
@app.get("/api/settings")
async def get_settings(db: Session = Depends(get_db)):
    """è·å–ç³»ç»Ÿè®¾ç½®"""
    settings = db.query(Settings).all()
    
    return {
        setting.key: {
            "value": setting.value,
            "description": setting.description
        }
        for setting in settings
    }

@app.put("/api/settings/{key}")
async def update_setting(key: str, setting: SettingUpdate, db: Session = Depends(get_db)):
    """æ›´æ–°ç³»ç»Ÿè®¾ç½®"""
    db_setting = db.query(Settings).filter(Settings.key == key).first()
    if not db_setting:
        raise HTTPException(status_code=404, detail="è®¾ç½®é¡¹ä¸å­˜åœ¨")
    
    db_setting.value = setting.value
    db_setting.updated_at = datetime.now()
    db.commit()
    
    # å¦‚æœæ˜¯è®¢é˜…æ£€æŸ¥é—´éš”ï¼Œæ›´æ–°è°ƒåº¦å™¨
    if key == "auto_check_interval":
        try:
            interval = int(setting.value)
            scheduler.update_subscription_check_interval(interval)
        except ValueError:
            pass
    
    return {"message": "è®¾ç½®æ›´æ–°æˆåŠŸ"}

# è°ƒåº¦å™¨ç®¡ç†API
@app.get("/api/scheduler/jobs")
async def get_scheduler_jobs():
    """è·å–è°ƒåº¦å™¨ä»»åŠ¡åˆ—è¡¨"""
    return scheduler.get_jobs()

@app.post("/api/scheduler/check-subscriptions")
async def trigger_subscription_check(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘è®¢é˜…æ£€æŸ¥"""
    background_tasks.add_task(scheduler.check_subscriptions)
    return {"message": "è®¢é˜…æ£€æŸ¥å·²å¯åŠ¨"}

@app.post("/api/scheduler/validate-cookies")
async def trigger_cookie_validation(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘CookieéªŒè¯"""
    background_tasks.add_task(scheduler.validate_cookies)
    return {"message": "CookieéªŒè¯å·²å¯åŠ¨"}

# è§†é¢‘æ£€æµ‹æœåŠ¡API
@app.get("/api/video-detection/status")
async def get_video_detection_status():
    """è·å–è§†é¢‘æ£€æµ‹æœåŠ¡çŠ¶æ€"""
    try:
        status = video_detection_service.get_status()
        return {
            "success": True,
            "data": status
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/start")
async def start_video_detection_service():
    """å¯åŠ¨è§†é¢‘æ£€æµ‹æœåŠ¡"""
    try:
        if video_detection_service.is_running:
            return {
                "success": True,
                "message": "è§†é¢‘æ£€æµ‹æœåŠ¡å·²åœ¨è¿è¡Œä¸­"
            }
        
        await video_detection_service.start_service()
        return {
            "success": True,
            "message": "è§†é¢‘æ£€æµ‹æœåŠ¡å¯åŠ¨æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/stop")
async def stop_video_detection_service():
    """åœæ­¢è§†é¢‘æ£€æµ‹æœåŠ¡"""
    try:
        await video_detection_service.stop_service()
        return {
            "success": True,
            "message": "è§†é¢‘æ£€æµ‹æœåŠ¡åœæ­¢æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/scan/full")
async def trigger_full_video_scan(background_tasks: BackgroundTasks):
    """è§¦å‘å®Œæ•´è§†é¢‘æ‰«æ"""
    try:
        # åœ¨åå°æ‰§è¡Œæ‰«æä»»åŠ¡
        background_tasks.add_task(video_detection_service.full_scan)
        
        return {
            "success": True,
            "message": "å®Œæ•´æ‰«æä»»åŠ¡å·²å¯åŠ¨ï¼Œå°†åœ¨åå°æ‰§è¡Œ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/scan/incremental")
async def trigger_incremental_video_scan(background_tasks: BackgroundTasks):
    """è§¦å‘å¢é‡è§†é¢‘æ‰«æ"""
    try:
        # åœ¨åå°æ‰§è¡Œå¢é‡æ‰«æ
        background_tasks.add_task(video_detection_service.incremental_scan)
        
        return {
            "success": True,
            "message": "å¢é‡æ‰«æä»»åŠ¡å·²å¯åŠ¨ï¼Œå°†åœ¨åå°æ‰§è¡Œ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/video-detection/config")
async def update_video_detection_config(scan_interval: int = 300):
    """æ›´æ–°è§†é¢‘æ£€æµ‹æœåŠ¡é…ç½®"""
    try:
        if scan_interval < 60:
            raise HTTPException(status_code=400, detail="æ‰«æé—´éš”ä¸èƒ½å°‘äº60ç§’")
        
        video_detection_service.scan_interval = scan_interval
        
        return {
            "success": True,
            "message": f"æ‰«æé—´éš”å·²æ›´æ–°ä¸º{scan_interval}ç§’"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "6.0.0"
    }
