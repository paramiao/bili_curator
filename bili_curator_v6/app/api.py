# UA ç­–ç•¥ï¼šæ—  Cookie ä½¿ç”¨éšæœº UAï¼ŒCookie æ¨¡å¼å¯ç”¨ç¨³å®š UA
# UA ç­–ç•¥å·²æŠ½å–è‡³ services.http_utils.get_user_agent

"""
FastAPI APIè·¯ç”±å®šä¹‰
"""
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from sqlalchemy.orm import Session
from sqlalchemy import text
from sqlalchemy import func, case
from typing import Any, Dict, List, Optional, Tuple
import re
from pydantic import BaseModel
from datetime import datetime, timedelta
import logging
import asyncio
import random
import json
import os
from pathlib import Path

from .models import (
    Subscription, Video, DownloadTask, Cookie, Settings, SubscriptionUpdate, CookieCreate, CookieUpdate, SettingUpdate,
    get_db
)
from .scheduler import scheduler, task_manager
from .services.subscription_stats import recompute_all_subscriptions
from .cookie_manager import cookie_manager
from .downloader import downloader
from .video_detection_service import video_detection_service
from .queue_manager import yt_dlp_semaphore, get_subscription_lock, request_queue
from .services.http_utils import get_user_agent
from .consistency_checker import consistency_checker, periodic_consistency_check, startup_consistency_check
from .services.remote_sync_service import remote_sync_service
from .auto_import import auto_import_service

# Logger
logger = logging.getLogger(__name__)

# æœ¬åœ°å·¥å…·ï¼šBVID æ ¡éªŒä¸å®‰å…¨ URL æ„é€ ï¼ˆé¿å…éæ³•IDæ‹¼æ¥URLï¼‰
def _is_bvid(vid: str) -> bool:
    try:
        return bool(vid) and bool(re.match(r'^BV[0-9A-Za-z]{10}$', str(vid)))
    except Exception:
        return False

def _safe_bilibili_url(vid: Optional[str]) -> Optional[str]:
    if not vid:
        return None
    return f"https://www.bilibili.com/video/{vid}" if _is_bvid(vid) else None



# Pydanticæ¨¡å‹å®šä¹‰
class SubscriptionCreate(BaseModel):
    name: str
    type: str  # 'collection', 'uploader', 'keyword'
    url: Optional[str] = None
    uploader_id: Optional[str] = None
    keyword: Optional[str] = None

class SubscriptionUpdate(BaseModel):
    name: Optional[str] = None
    is_active: Optional[bool] = None
    active: Optional[bool] = None
    url: Optional[str] = None
    uploader_id: Optional[str] = None
    keyword: Optional[str] = None

class CookieCreate(BaseModel):
    name: str
    sessdata: str
    bili_jct: Optional[str] = ""
    dedeuserid: Optional[str] = ""

class CookieUpdate(BaseModel):
    name: Optional[str] = None
    is_active: Optional[bool] = None
    active: Optional[bool] = None
    sessdata: Optional[str] = None
    bili_jct: Optional[str] = None
    dedeuserid: Optional[str] = None

class SettingUpdate(BaseModel):
    value: str

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="bili_curator V6",
    description="Bç«™è§†é¢‘ä¸‹è½½ç®¡ç†ç³»ç»Ÿ",
    version="6.0.0"
)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory="static"), name="static")

# ------------------------------
# åº”ç”¨å¯åŠ¨ä¸å…³é—­äº‹ä»¶
# ------------------------------
@app.on_event("startup")
async def _on_startup():
    """æœåŠ¡å¯åŠ¨è‡ªåŠ¨æ‰§è¡Œï¼š
    - å¯åŠ¨è°ƒåº¦å™¨
    - æ‰§è¡Œä¸€æ¬¡æœ¬åœ°ä¼˜å…ˆçš„ä¸€è‡´æ€§ä¿®å¤ï¼ˆåœ¨åå°çº¿ç¨‹é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
    - å…¨é‡é‡ç®—è®¢é˜…ç»Ÿè®¡ï¼ˆæœ¬åœ°ä¼˜å…ˆå£å¾„ï¼‰
    """
    try:
        scheduler.start()
    except Exception as e:
        logger.warning(f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥ï¼š{e}")

    # ä¸€è‡´æ€§ä¿®å¤ï¼šæ”¾åˆ°çº¿ç¨‹ï¼Œé¿å…é˜»å¡
    try:
        await asyncio.to_thread(startup_consistency_check)
    except Exception as e:
        logger.warning(f"å¯åŠ¨ä¸€è‡´æ€§ä¿®å¤å¼‚å¸¸ï¼š{e}")

    # å¯åŠ¨åç»Ÿä¸€é‡ç®—è®¢é˜…ç»Ÿè®¡
    db = next(get_db())
    try:
        recompute_all_subscriptions(db, touch_last_check=False)
        db.commit()
    except Exception as e:
        db.rollback()
        logger.warning(f"å¯åŠ¨åé‡ç®—è®¢é˜…ç»Ÿè®¡å¤±è´¥ï¼š{e}")
    finally:
        db.close()

    # å¯åŠ¨ååå°æ‰¹é‡æ ¡éªŒ Cookieï¼ˆä¸é˜»å¡å¯åŠ¨ï¼‰
    async def _validate_cookies_bg():
        try:
            await asyncio.to_thread(cookie_manager.batch_validate_cookies)
        except Exception as e:
            logger.warning(f"å¯åŠ¨å Cookie æ‰¹é‡æ ¡éªŒå¼‚å¸¸ï¼š{e}")
    try:
        asyncio.create_task(_validate_cookies_bg())
    except Exception as e:
        logger.debug(f"schedule cookie batch validate failed: {e}")


# ------------------------------
# è‡ªåŠ¨å¯¼å…¥/è‡ªåŠ¨å…³è” API
# ------------------------------
class AutoImportBody(BaseModel):
    recompute: Optional[bool] = False  # æ˜¯å¦åœ¨å®Œæˆåè§¦å‘ä¸€æ¬¡ç»Ÿè®¡é‡ç®—

@app.post("/api/auto-import/scan-associate")
async def api_auto_import_scan_associate(body: AutoImportBody = None):
    """è§¦å‘ä¸€æ¬¡åå°çš„æœ¬åœ°æ‰«æå¯¼å…¥ + è‡ªåŠ¨å…³è”è®¢é˜…ã€‚
    - ç«‹å³è¿”å› {triggered: true}
    - å·¥ä½œåœ¨åå°çº¿ç¨‹ä¸­ä¸²è¡Œæ‰§è¡Œï¼šscan_and_import -> auto_associate_subscriptions
    - å¯é€‰ï¼šå®Œæˆåè§¦å‘ä¸€æ¬¡ recompute_all_subscriptionsï¼ˆå½“ body.recompute ä¸º Trueï¼‰
    """
    async def _run_job():
        # æ”¾åœ¨çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        try:
            await asyncio.to_thread(auto_import_service.scan_and_import)
        except Exception as e:
            logger.warning(f"scan_and_import å¼‚å¸¸ï¼š{e}")
        try:
            await asyncio.to_thread(auto_import_service.auto_associate_subscriptions)
        except Exception as e:
            logger.warning(f"auto_associate_subscriptions å¼‚å¸¸ï¼š{e}")
        if body and body.recompute:
            ldb = next(get_db())
            try:
                recompute_all_subscriptions(ldb, touch_last_check=False)
                ldb.commit()
            except Exception as e:
                ldb.rollback()
                logger.warning(f"recompute_all_subscriptions å¤±è´¥ï¼š{e}")
            finally:
                ldb.close()

    asyncio.create_task(_run_job())
    return {"triggered": True}

# ------------------------------
# è½»é‡åŒæ­¥ APIï¼ˆè§¦å‘ + çŠ¶æ€ï¼‰
# ------------------------------

class SyncTriggerBody(BaseModel):
    sid: Optional[int] = None
    mode: Optional[str] = "lite_head"  # lite_head | backfill_failures | full_head_small (ä¿ç•™å‘å‰å…¼å®¹)
    force: Optional[bool] = False  # å¼ºåˆ¶åˆ·æ–°ï¼šç»•è¿‡TTLä¸ç¼“å­˜ï¼Œç›´æ¥æ‰§è¡Œè¿œç«¯å¯¹æ¯”

@app.post("/api/sync/trigger")
async def api_sync_trigger(body: SyncTriggerBody, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """è§¦å‘è½»é‡åŒæ­¥ï¼š
    - æŒ‡å®š sidï¼šåå°åˆ·æ–°è¯¥è®¢é˜…çš„ pending ä¼°ç®—ï¼ˆcompute_pending_listï¼‰å¹¶å†™å…¥ç¼“å­˜ï¼›å¯ä½œä¸ºâ€œåˆ·æ–°æŒ‰é’®â€çš„è½»é‡æ“ä½œã€‚
    - æœªæŒ‡å®š sidï¼šè°ƒç”¨ä¸€æ¬¡ enqueue_coordinatorï¼ˆæŒ‰è½®è½¬/èŠ‚æµç­–ç•¥ï¼‰ï¼Œè§¦å‘å…¨å±€è½»é‡åŒæ­¥ä¸å…¥é˜Ÿåè°ƒã€‚
    è¿”å›ç«‹åˆ»ï¼Œä¸é˜»å¡å‰ç«¯ã€‚
    """

    async def _run_for_sid(sid: int):
        ldb = next(get_db())
        try:
            # è¯»å–æœ€è¿‘ä¸€æ¬¡ sync çŠ¶æ€ï¼Œå†³å®šæ˜¯å¦å¤ç”¨ç¼“å­˜æˆ–è·³è¿‡é‡å¤æŠ“å–
            try:
                status_key = f"sync:{sid}:status"
                srow = ldb.query(Settings).filter(Settings.key == status_key).first()
                status_data = json.loads(srow.value) if (srow and srow.value) else {}
            except Exception:
                status_data = {}

            status_str = (status_data.get('status') or '').strip()
            remote_total_cached = status_data.get('remote_total')
            # è§£ææ—¶é—´æˆ³ï¼ˆupdated_at ä¼˜å…ˆï¼Œå…¶æ¬¡ tsï¼‰
            ts_raw = status_data.get('updated_at') or status_data.get('ts')
            ts_dt = None
            try:
                if ts_raw:
                    ts_dt = datetime.fromisoformat(ts_raw)
            except Exception:
                ts_dt = None

            # TTLï¼šREMOTE_TOTAL_TTL_MINï¼Œé»˜è®¤60åˆ†é’Ÿ
            try:
                ttl_min = int(os.getenv('REMOTE_TOTAL_TTL_MIN', '60'))
            except Exception:
                ttl_min = 60
            fresh = False
            if ts_dt is not None:
                try:
                    fresh = (datetime.now() - ts_dt) <= timedelta(minutes=max(1, ttl_min))
                except Exception:
                    fresh = False

            # è‹¥å·²æœ‰è¿è¡Œä¸­ï¼š
            # - è‹¥ç¼“å­˜æ–°é²œä¸”å·²æœ‰ remote_totalï¼Œåˆ™ç›´æ¥æŒ‰æœ¬åœ°ä¸‹è½½æ•°åˆ·æ–° pendingï¼Œå¹¶å°†çŠ¶æ€ç½®ä¸º idle åè¿”å›
            # - å¦åˆ™ä¿æŒæ—©é€€ï¼Œé¿å…å¹¶å‘å¤–ç½‘æŠ“å–
            if status_str == 'running':
                if fresh and isinstance(remote_total_cached, int):
                    try:
                        downloaded = ldb.query(Video).filter(Video.subscription_id == sid, Video.video_path.isnot(None)).count()
                        if downloaded > int(remote_total_cached):
                            raise RuntimeError("cached remote_total suspicious on running; skip idle override")
                        pend = max(0, int(remote_total_cached) - int(downloaded))
                        pend_key = f"agg:{sid}:pending_estimated"
                        pend_val = str(pend)
                        s = ldb.query(Settings).filter(Settings.key == pend_key).first()
                        if not s:
                            s = Settings(key=pend_key, value=pend_val)
                            ldb.add(s)
                        else:
                            s.value = pend_val
                        ldb.commit()
                        # å°†çŠ¶æ€ç½®ä¸º idleï¼ˆUPSERTï¼‰
                        status_key = f"sync:{sid}:status"
                        payload = {
                            'status': 'idle',
                            'updated_at': datetime.now().isoformat(),
                            'remote_total': int(remote_total_cached),
                            'existing': int(downloaded),
                            'pending': int(pend),
                        }
                        val = json.dumps(payload, ensure_ascii=False)
                        ldb.execute(text("""
                            INSERT INTO settings (key, value, description)
                            VALUES (:key, :val, 'è®¢é˜…åŒæ­¥çŠ¶æ€')
                            ON CONFLICT(key) DO UPDATE SET
                              value = :val,
                              updated_at = CURRENT_TIMESTAMP
                        """), {"key": status_key, "val": val})
                        ldb.commit()
                    except Exception as e:
                        logger.debug(f"running->idle override failed: {e}")
                        ldb.rollback()
                    return
                else:
                    return

            # è‹¥æœªå¼ºåˆ¶ï¼Œä¸”ç¼“å­˜æ–°é²œä¸”æœ‰ remote_totalï¼Œåˆ™ä»…ç”¨æœ¬åœ°ä¸‹è½½æ•°é‡æ–°è®¡ç®— pendingï¼Œå¹¶å†™å…¥ç¼“å­˜ï¼Œé¿å…å¤–ç½‘æŠ“å–
            if (not (body and body.force)) and fresh and isinstance(remote_total_cached, int):
                try:
                    downloaded = ldb.query(Video).filter(Video.subscription_id == sid, Video.video_path.isnot(None)).count()
                    # çº åï¼šå¦‚æœ¬åœ°ä¸‹è½½æ•° > ç¼“å­˜è¿œç«¯æ•°ï¼Œè®¤ä¸ºç¼“å­˜å¯ç–‘ï¼Œè½¬ä¸ºå¼ºåˆ¶åˆ·æ–°
                    if downloaded > int(remote_total_cached):
                        raise RuntimeError("cached remote_total suspicious; force refresh")
                    pend = max(0, int(remote_total_cached) - int(downloaded))
                    pend_key = f"agg:{sid}:pending_estimated"
                    pend_val = str(pend)
                    s = ldb.query(Settings).filter(Settings.key == pend_key).first()
                    if not s:
                        s = Settings(key=pend_key, value=pend_val)
                        ldb.add(s)
                    else:
                        s.value = pend_val
                    ldb.commit()
                    # æå‰è¿”å›å‰ï¼Œç¡®ä¿ sync çŠ¶æ€è¢«è®¾ç½®ä¸º idleï¼ˆé¿å…å†å² running æ®‹ç•™ï¼‰
                    try:
                        status_key = f"sync:{sid}:status"
                        payload = {
                            'status': 'idle',
                            'updated_at': datetime.now().isoformat(),
                            'remote_total': int(remote_total_cached),
                        }
                        val = json.dumps(payload, ensure_ascii=False)
                        ldb.execute(text("""
                            INSERT INTO settings (key, value, description)
                            VALUES (:key, :val, 'è®¢é˜…åŒæ­¥çŠ¶æ€')
                            ON CONFLICT(key) DO UPDATE SET
                              value = :val,
                              updated_at = CURRENT_TIMESTAMP
                        """), {"key": status_key, "val": val})
                        ldb.commit()
                    except Exception as e:
                        logger.debug(f"early-return set idle failed: {e}")
                        ldb.rollback()
                    return
                except Exception:
                    # å¤±è´¥åˆ™é€€å›åˆ°å®Œæ•´è®¡ç®—
                    pass

            # èµ°è½»é‡è·¯å¾„ï¼šå…ˆæŠŠçŠ¶æ€æ ‡è®°ä¸º runningï¼Œè®©å‰ç«¯ç«‹åˆ»æ˜¾ç¤ºâ€œè·å–ä¸­â€ï¼Œå†åå°è®¡ç®—
            try:
                sdata = {
                    'status': 'running',
                    'updated_at': datetime.now().isoformat(),
                }
                srow = ldb.query(Settings).filter(Settings.key == status_key).first()
                if not srow:
                    srow = Settings(key=status_key, value=json.dumps(sdata, ensure_ascii=False))
                    ldb.add(srow)
                else:
                    srow.value = json.dumps(sdata, ensure_ascii=False)
                ldb.commit()
            except Exception:
                ldb.rollback()

            try:
                info = await downloader.compute_pending_list(sid, ldb)
                # å°† pending å†™å…¥ç¼“å­˜ï¼ˆä¸è°ƒåº¦å™¨è¡Œä¸ºä¸€è‡´ï¼Œç¡®ä¿å‰ç«¯ç«‹åˆ»å¯è§ï¼‰
                pend_key = f"agg:{sid}:pending_estimated"
                pend_val = str(info.get('pending') or 0)
                s = ldb.query(Settings).filter(Settings.key == pend_key).first()
                if not s:
                    s = Settings(key=pend_key, value=pend_val)
                    ldb.add(s)
                else:
                    s.value = pend_val
                ldb.commit()
                # æˆåŠŸï¼šå°† sync çŠ¶æ€ä» running æ›´æ–°ä¸º idleï¼ˆä½¿ç”¨ UPSERTï¼Œé¿å…è¢«å¹¶å‘è¦†ç›–ï¼‰
                try:
                    status_key = f"sync:{sid}:status"
                    payload = {
                        'status': 'idle',
                        'updated_at': datetime.now().isoformat(),
                        'remote_total': info.get('remote_total'),
                        'existing': info.get('existing'),
                        'pending': info.get('pending'),
                    }
                    val = json.dumps(payload, ensure_ascii=False)
                    ldb.execute(text("""
                        INSERT INTO settings (key, value, description)
                        VALUES (:key, :val, 'è®¢é˜…åŒæ­¥çŠ¶æ€')
                        ON CONFLICT(key) DO UPDATE SET
                          value = :val,
                          updated_at = CURRENT_TIMESTAMP
                    """), {"key": status_key, "val": val})
                    ldb.commit()
                except Exception as e:
                    logger.debug(f"post-compute set idle failed: {e}")
                    ldb.rollback()
            except Exception as e:
                logger.warning(f"sync trigger (sid={sid}) failed: {e}")
                # å¤±è´¥ï¼šå°† sync çŠ¶æ€æ›´æ–°ä¸º failedï¼Œé¿å…é•¿æ—¶é—´åœç•™åœ¨ running
                try:
                    status_key = f"sync:{sid}:status"
                    payload = {
                        'status': 'failed',
                        'error': str(e),
                        'updated_at': datetime.now().isoformat(),
                    }
                    val = json.dumps(payload, ensure_ascii=False)
                    ldb.execute(text("""
                        INSERT INTO settings (key, value, description)
                        VALUES (:key, :val, 'è®¢é˜…åŒæ­¥çŠ¶æ€')
                        ON CONFLICT(key) DO UPDATE SET
                          value = :val,
                          updated_at = CURRENT_TIMESTAMP
                    """), {"key": status_key, "val": val})
                    ldb.commit()
                except Exception as ee:
                    logger.debug(f"post-compute set failed failed: {ee}")
                    ldb.rollback()
        finally:
            ldb.close()

    async def _run_global():
        try:
            await scheduler.enqueue_coordinator()
        except Exception as e:
            logger.warning(f"sync trigger (global) failed: {e}")

    # æ³¨æ„ï¼šä¸è¦æŠŠ asyncio.create_task äº¤ç»™ BackgroundTasksï¼ˆå…¶åœ¨çº¿ç¨‹æ± é‡Œæ— äº‹ä»¶å¾ªç¯ï¼‰
    if body and body.sid:
        asyncio.create_task(_run_for_sid(body.sid))
        return {"triggered": True, "scope": "subscription", "sid": body.sid}
    else:
        asyncio.create_task(_run_global())
        return {"triggered": True, "scope": "global"}

# ------------------------------
# å¢é‡ç®¡çº¿ç®¡ç† APIï¼ˆç°åº¦ä¸æœ¬åœ°éªŒè¯ï¼‰
# ------------------------------
class IncrementalToggleBody(BaseModel):
    sid: Optional[int] = None  # None=å…¨å±€
    enabled: bool

@app.post("/api/incremental/toggle")
async def incremental_toggle(body: IncrementalToggleBody, db: Session = Depends(get_db)):
    """å¼€å¯/å…³é—­å¢é‡ç®¡çº¿ï¼š
    - å…¨å±€ï¼šsync:global:enable_incremental_pipeline = "1"|"0"
    - è®¢é˜…ï¼šsync:{sid}:enable_incremental = "1"|"0"ï¼ˆè¦†ç›–å…¨å±€ï¼‰
    """
    try:
        key = (
            'sync:global:enable_incremental_pipeline'
            if (body.sid is None) else f'sync:{int(body.sid)}:enable_incremental'
        )
        val = '1' if body.enabled else '0'
        row = db.query(Settings).filter(Settings.key == key).first()
        if not row:
            row = Settings(key=key, value=val)
            db.add(row)
        else:
            row.value = val
        db.commit()
        return {"ok": True, "key": key, "value": val}
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

class CookieToggleBody(BaseModel):
    id: int
    is_active: bool

@app.post("/api/cookie/toggle")
async def cookie_toggle(body: CookieToggleBody, db: Session = Depends(get_db)):
    """å¯ç”¨/ç¦ç”¨æŒ‡å®š Cookieã€‚"""
    try:
        row = db.query(Cookie).filter(Cookie.id == int(body.id)).first()
        if not row:
            raise HTTPException(status_code=404, detail="cookie not found")
        row.is_active = bool(body.is_active)
        db.commit()
        return {"id": row.id, "name": row.name, "is_active": bool(row.is_active)}
    except HTTPException:
        db.rollback()
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

@app.post("/api/cookie/validate-all")
async def cookie_validate_all():
    """åå°è§¦å‘æ‰¹é‡ Cookie æ ¡éªŒï¼Œç«‹å³è¿”å›ã€‚"""
    async def _run():
        try:
            await asyncio.to_thread(cookie_manager.batch_validate_cookies)
        except Exception as e:
            logger.warning(f"validate-all å¼‚å¸¸ï¼š{e}")
    try:
        asyncio.create_task(_run())
        return {"triggered": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class HeadSnapshotBody(BaseModel):
    sid: int
    head_ids: List[str]
    cap: Optional[int] = 200
    reset_cursor: Optional[bool] = True

@app.post("/api/incremental/head-snapshot")
async def set_head_snapshot(body: HeadSnapshotBody, db: Session = Depends(get_db)):
    """å†™å…¥è®¢é˜…çš„ head_snapshotï¼ˆç”¨äºä¸å‡ºç½‘éªŒè¯M1å¢é‡å…¥é˜Ÿï¼‰ã€‚æ”¯æŒå¯é€‰é‡ç½® last_cursorã€‚"""
    try:
        # è§„èŒƒåŒ–ä¸è£å‰ª
        arr = [str(x) for x in (body.head_ids or []) if isinstance(x, (str, int))]
        if not arr:
            raise HTTPException(status_code=400, detail="head_ids ä¸ºç©º")
        cap = max(1, int(body.cap or 200))
        remote_sync_service.update_head_snapshot(db, int(body.sid), arr[:cap])
        if body.reset_cursor:
            remote_sync_service.set_last_cursor(db, int(body.sid), None)
        return {"ok": True, "sid": int(body.sid), "size": len(arr[:cap])}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class RefreshHeadBody(BaseModel):
    sid: int
    cap: Optional[int] = 200
    reset_cursor: Optional[bool] = True

@app.post("/api/incremental/refresh-head")
async def refresh_head_snapshot(body: RefreshHeadBody, db: Session = Depends(get_db)):
    """è§¦å‘è®¢é˜…çš„è¿œç«¯å¤´éƒ¨å¿«ç…§åˆ·æ–°ï¼ˆåå°å¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰ã€‚
    - è°ƒç”¨ `remote_sync_service.refresh_head_snapshot()` æŠ“å–å‰ cap ä¸ª ID å¹¶å†™å…¥ã€‚
    - åˆ·æ–°è¿‡ç¨‹ä¼šæ›´æ–° `sync:{sid}:status` ä¸º running/idle/failedã€‚
    """
    if not body or not body.sid:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘ sid")

    async def _run():
        ldb = next(get_db())
        try:
            await remote_sync_service.refresh_head_snapshot(ldb, int(body.sid), cap=int(body.cap or 200), reset_cursor=bool(body.reset_cursor))
        except Exception as e:
            logger.warning(f"refresh head failed (sid={body.sid}): {e}")
        finally:
            ldb.close()

    # ä½¿ç”¨ asyncio.create_task å¯åŠ¨åå°ä»»åŠ¡
    asyncio.create_task(_run())
    return {"triggered": True, "sid": int(body.sid)}


@app.get("/api/incremental/status/{sid}")
async def get_incremental_status(sid: int, db: Session = Depends(get_db)):
    """æŸ¥è¯¢è®¢é˜…å¢é‡çŠ¶æ€ï¼š
    è¿”å› { sid, status, updated_at, remote_total_cached, head_size, last_cursor }ã€‚
    """
    try:
        status_key = f"sync:{sid}:status"
        head_key = f"sync:{sid}:head_snapshot"
        cursor_key = f"sync:{sid}:last_cursor"
        total_key = f"sync:{sid}:remote_total_cached"

        rows = db.query(Settings).filter(Settings.key.in_([status_key, head_key, cursor_key, total_key])).all()
        smap = {r.key: r.value for r in rows if r and r.key}

        # status
        status = None
        updated_at = None
        try:
            sval = smap.get(status_key)
            if sval:
                data = json.loads(sval)
                status = data.get('status')
                updated_at = data.get('updated_at') or data.get('ts')
        except Exception:
            pass

        # head size
        head_size = None
        try:
            hval = smap.get(head_key)
            if hval:
                arr = json.loads(hval)
                if isinstance(arr, list):
                    head_size = len(arr)
        except Exception:
            pass

        # last cursor
        last_cursor = None
        try:
            cval = smap.get(cursor_key)
            if cval:
                c = json.loads(cval)
                if isinstance(c, dict):
                    last_cursor = c.get('last_seen')
        except Exception:
            pass

        # remote total cached
        remote_total_cached = None
        try:
            tval = smap.get(total_key)
            if tval is not None:
                remote_total_cached = int(str(tval).strip())
        except Exception:
            pass

        return {
            'sid': int(sid),
            'status': status,
            'updated_at': updated_at,
            'remote_total_cached': remote_total_cached,
            'head_size': head_size,
            'last_cursor': last_cursor,
        }
    finally:
        db.close()


@app.get("/api/sync/status")
async def api_sync_status(sid: Optional[int] = None, db: Session = Depends(get_db)):
    """æŸ¥è¯¢åŒæ­¥çŠ¶æ€ï¼šè¿”å› last_sync çŠ¶æ€ã€remote_totalã€pending_estimatedã€retry_backfill é˜Ÿåˆ—é•¿åº¦ç­‰ã€‚
    - è‹¥æä¾› sidï¼Œåˆ™ä»…è¿”å›è¯¥è®¢é˜…ï¼›å¦åˆ™è¿”å›æ‰€æœ‰å¯ç”¨è®¢é˜…ã€‚
    """
    try:
        subs = []
        if sid is not None:
            s = db.query(Subscription).filter(Subscription.id == sid).first()
            if s:
                subs = [s]
        else:
            subs = db.query(Subscription).filter(Subscription.is_active == True).all()
        if not subs:
            return {"items": []}

        ids = [s.id for s in subs]
        keys = []
        for i in ids:
            keys.append(f"sync:{i}:status")
            keys.append(f"agg:{i}:pending_estimated")
            keys.append(f"retry:{i}:failed_backfill")
        rows = db.query(Settings).filter(Settings.key.in_(keys)).all()
        smap = {r.key: r.value for r in rows if r and r.key}

        items = []
        for s in subs:
            stat = {
                "subscription_id": s.id,
                "name": s.name,
                "remote_total": None,
                "pending_estimated": None,
                "retry_queue_len": 0,
                "fail_total": None,
                "fail_perm": None,
                "status": None,
                "updated_at": None,
                "is_fetching": False,
            }
            # sync status
            try:
                sval = smap.get(f"sync:{s.id}:status")
                if sval:
                    data = json.loads(sval)
                    stat["status"] = data.get("status")
                    stat["remote_total"] = data.get("remote_total")
                    stat["updated_at"] = data.get("updated_at") or data.get("ts")
                    # è¿œç«¯æ€»æ•°å°šæœªå†™å…¥ä¸”çŠ¶æ€ä¸ºè¿è¡Œä¸­ï¼Œè§†ä¸ºâ€œè·å–ä¸­â€
                    if (stat["status"] == 'running') and (stat["remote_total"] is None):
                        stat["is_fetching"] = True
            except Exception:
                pass
            # pending cached
            try:
                pc = smap.get(f"agg:{s.id}:pending_estimated")
                if pc is not None:
                    stat["pending_estimated"] = int(str(pc).strip())
            except Exception:
                pass
            # retry queue length
            try:
                rq = smap.get(f"retry:{s.id}:failed_backfill")
                if rq:
                    arr = json.loads(rq)
                    if isinstance(arr, list):
                        stat["retry_queue_len"] = len(arr)
            except Exception:
                pass
            items.append(stat)

        # å¦‚ä»…æŸ¥è¯¢å•ä¸ªè®¢é˜…ï¼Œè¡¥å……è¯¥è®¢é˜…çš„å¤±è´¥ç»Ÿè®¡ï¼ˆé¿å…å…¨é‡æŸ¥è¯¢è¿‡é‡ï¼‰
        if sid is not None and len(items) == 1:
            try:
                fails = db.query(Settings).filter(Settings.key.like('fail:%')).all()
                total = 0
                perm = 0
                for r in fails:
                    try:
                        data = json.loads(r.value) if (r and r.value) else {}
                        if not isinstance(data, dict):
                            continue
                        if int(data.get('sid') or -1) == int(sid):
                            total += 1
                            if (data.get('class') == 'permanent'):
                                perm += 1
                    except Exception:
                        continue
                items[0]['fail_total'] = total
                items[0]['fail_perm'] = perm
            except Exception:
                pass

        return {"items": items}
    finally:
        db.close()
app.mount("/web", StaticFiles(directory="web/dist"), name="web")

# æ ¹è·¯å¾„ä»…è¿”å› SPAï¼ˆweb/dist/index.htmlï¼‰ï¼Œç¼ºå¤±åˆ™ç›´æ¥æŠ¥é”™ï¼Œé¿å…å›é€€åˆ° legacy é¡µé¢é€ æˆæ··æ·†
@app.get("/", response_class=HTMLResponse)
async def read_root():
    """è¿”å› SPA é¦–é¡µï¼šä»… web/dist/index.htmlã€‚ç¼ºå¤±åˆ™ 500 æç¤ºæ„å»ºç¼ºå¤±ã€‚"""
    try:
        with open("web/dist/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        logger.warning("SPA index.html ç¼ºå¤±ï¼šweb/dist/index.html ä¸å­˜åœ¨")
        return HTMLResponse(content="""
        <html>
            <head><title>bili_curator V6</title></head>
            <body>
                <h1>ğŸ¬ bili_curator V6</h1>
                <p style='color:#b91c1c;'>å‰ç«¯æ„å»ºç¼ºå¤±ï¼šæœªæ‰¾åˆ° web/dist/index.html</p>
                <p>è¯·å…ˆæ„å»ºå‰ç«¯æˆ–ç¡®è®¤éƒ¨ç½²åŒ…åŒ…å« SPA äº§ç‰©ã€‚</p>
                <p>APIæ–‡æ¡£: <a href="/docs">/docs</a></p>
            </body>
        </html>
        """, status_code=500)

# Legacy ç®¡ç†é¡µé¢ï¼šä»…å½“ä»ä¿ç•™æ–‡ä»¶æ—¶å¯è®¿é—®
@app.get("/legacy/admin", response_class=HTMLResponse)
async def legacy_admin():
    try:
        with open("static/admin.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="legacy admin å·²ç§»é™¤")

@app.get("/admin")
async def read_admin():
    """å…¼å®¹æ—§å…¥å£ï¼Œ301 é‡å®šå‘åˆ° /legacy/adminï¼Œé¿å…ä¸é¦–é¡µæ··æ·†"""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="/legacy/admin", status_code=301)

# ç³»ç»ŸçŠ¶æ€API
@app.get("/api/status")
async def get_system_status(db: Session = Depends(get_db)):
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    # ç»Ÿè®¡æ•°æ®
    total_subscriptions = db.query(Subscription).count()
    active_subscriptions = db.query(Subscription).filter(Subscription.is_active == True).count()
    total_videos = db.query(Video).count()
    downloaded_videos = db.query(Video).filter(Video.video_path.isnot(None)).count()
    active_cookies = db.query(Cookie).filter(Cookie.is_active == True).count()
    total_cookies = db.query(Cookie).count()
    
    # è°ƒåº¦å™¨ä»»åŠ¡åˆ—è¡¨
    try:
        scheduler_jobs = scheduler.get_jobs()
    except Exception:
        scheduler_jobs = []

    # è¿è¡Œä¸­ä»»åŠ¡æ‘˜è¦ï¼ˆEnhancedTaskManagerï¼‰
    try:
        running_map = task_manager.get_all_tasks()
        running_tasks = list(running_map.values()) if isinstance(running_map, dict) else running_map
    except Exception:
        running_tasks = []

    # æœ€è¿‘ä¸‹è½½ä»»åŠ¡ï¼ˆæˆªå–æœ€è¿‘20æ¡ï¼‰
    try:
        recent_rows = (
            db.query(DownloadTask)
            .order_by(DownloadTask.updated_at.desc())
            .limit(20)
            .all()
        )
        recent_tasks = [
            {
                "id": r.id,
                "bilibili_id": r.bilibili_id,
                "subscription_id": r.subscription_id,
                "status": r.status,
                "progress": float(r.progress or 0.0),
                "started_at": r.started_at.isoformat() if r.started_at else None,
                "completed_at": r.completed_at.isoformat() if r.completed_at else None,
                "updated_at": r.updated_at.isoformat() if r.updated_at else None,
                "error_message": r.error_message,
            }
            for r in recent_rows
        ]
    except Exception:
        recent_tasks = []

    return {
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "version": "6.0.0",
        "statistics": {
            "total_subscriptions": total_subscriptions,
            "active_subscriptions": active_subscriptions,
            "total_videos": total_videos,
            "downloaded_videos": downloaded_videos,
            "active_cookies": active_cookies
        },
        "cookie_summary": {
            "active": active_cookies,
            "total": total_cookies,
            "current_cookie_id": getattr(cookie_manager, 'current_cookie_id', None),
        },
        "recent_tasks": recent_tasks,
        "scheduler_jobs": scheduler_jobs,
        "running_tasks": running_tasks,
    }

# é˜Ÿåˆ—è°ƒè¯•æ¥å£
@app.get("/api/queue/stats")
async def queue_stats():
    """è¿”å›é˜Ÿåˆ—å®¹é‡/è¿è¡Œ/æš‚åœä¸å„é€šé“æ’é˜Ÿæ•°ã€‚"""
    try:
        return request_queue.stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ------------------------------
# Cookie ç®¡ç† API
# ------------------------------

@app.get("/api/cookie/status")
async def cookie_status(db: Session = Depends(get_db)):
    """è¿”å› Cookie åˆ—è¡¨ä¸å½“å‰é€šé“ä¿¡æ¯ã€‚
    - items: æ¯ä¸ª cookie çš„åŸºæœ¬ä¿¡æ¯ï¼ˆä¸å«æ•æ„Ÿå­—æ®µï¼‰
    - current_cookie_id: å½“å‰è¢« SimpleCookieManager é€‰æ‹©çš„ cookie idï¼ˆå¯èƒ½ä¸º Noneï¼‰
    - counts: æ´»è·ƒ/ç¦ç”¨ æ•°é‡
    """
    try:
        rows = db.query(Cookie).all()
        items = []
        for r in rows:
            try:
                items.append({
                    'id': r.id,
                    'name': r.name,
                    'is_active': bool(r.is_active),
                    'usage_count': int(r.usage_count or 0),
                    'last_used': r.last_used.isoformat() if r.last_used else None,
                    'failure_count': int(r.failure_count or 0),
                    'last_failure_at': r.last_failure_at.isoformat() if r.last_failure_at else None,
                    'created_at': r.created_at.isoformat() if r.created_at else None,
                    'updated_at': r.updated_at.isoformat() if r.updated_at else None,
                })
            except Exception:
                continue
        active = sum(1 for x in items if x.get('is_active'))
        inactive = len(items) - active
        return {
            'items': items,
            'current_cookie_id': getattr(cookie_manager, 'current_cookie_id', None),
            'counts': {'active': active, 'inactive': inactive},
        }
    finally:
        db.close()


@app.post("/api/cookie/upload")
async def cookie_upload(body: CookieCreate, db: Session = Depends(get_db)):
    """ä¸Šä¼ /æ–°å¢ä¸€ä¸ª Cookieã€‚é»˜è®¤å†™åº“ä¸º active=trueï¼›å¯é€‰è¿›è¡Œä¸€æ¬¡åœ¨çº¿æ ¡éªŒã€‚
    è¿”å› { id, name, is_valid, is_active }
    """
    try:
        # å¹‚ç­‰ï¼šè‹¥åŒåå­˜åœ¨ï¼Œæ›´æ–°å†…å®¹ï¼›å¦åˆ™æ–°å¢
        row = db.query(Cookie).filter(Cookie.name == body.name).first()
        creating = False
        if not row:
            row = Cookie(
                name=body.name.strip(),
                sessdata=body.sessdata.strip(),
                bili_jct=(body.bili_jct or '').strip(),
                dedeuserid=(body.dedeuserid or '').strip(),
                is_active=True,
                usage_count=0,
            )
            db.add(row)
            creating = True
        else:
            row.sessdata = body.sessdata.strip()
            row.bili_jct = (body.bili_jct or '').strip()
            row.dedeuserid = (body.dedeuserid or '').strip()
            row.is_active = True
        db.commit()

        # åœ¨çº¿æ ¡éªŒï¼ˆä¸æŠ›å¼‚å¸¸ï¼Œå¤±è´¥å°†ä»…æ ‡è®°å¤±è´¥è®¡æ•°/å¿…è¦æ—¶ç¦ç”¨ï¼‰
        is_valid = False
        try:
            is_valid = await cookie_manager.validate_cookie(row)
            if is_valid:
                # æ¸…ç†å†å²å¤±è´¥çŠ¶æ€
                try:
                    cookie_manager.reset_failures(db, row.id)
                except Exception:
                    pass
            else:
                # è®°å½•å¤±è´¥å¹¶å¯èƒ½ç¦ç”¨
                try:
                    cookie_manager.record_failure(db, row.id, reason="upload_validate_failed")
                except Exception:
                    pass
        except Exception:
            # ç½‘ç»œæˆ–è§£æå¼‚å¸¸ä¹ŸæŒ‰å¤±è´¥è®¡ä¸€æ¬¡
            try:
                cookie_manager.record_failure(db, row.id, reason="upload_validate_exception")
            except Exception:
                pass

        return {
            'id': row.id,
            'name': row.name,
            'is_valid': bool(is_valid),
            'is_active': bool(row.is_active),
            'creating': creating,
        }
    except HTTPException:
        db.rollback()
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

# é˜Ÿåˆ—ç®¡ç†ä¸æ´å¯Ÿ APIï¼ˆç”¨äºå¯è§†åŒ–ä¸å®¹é‡è°ƒæ•´ï¼‰
class QueueCapacityBody(BaseModel):
    requires_cookie: Optional[int] = None  # ç›®æ ‡å¹¶å‘ä¸Šé™ï¼ˆcookie é€šé“ï¼‰
    no_cookie: Optional[int] = None        # ç›®æ ‡å¹¶å‘ä¸Šé™ï¼ˆno-cookie é€šé“ï¼‰
    persist: Optional[bool] = True         # æ˜¯å¦æŒä¹…åŒ–åˆ° Settingsï¼Œä¾¿äºé‡å¯åç”Ÿæ•ˆ

@app.get("/api/queue/list")
async def queue_list():
    """åˆ—å‡ºå½“å‰å†…å­˜é˜Ÿåˆ—çš„ä»»åŠ¡å¿«ç…§ï¼ˆä»…ç”¨äºè°ƒè¯•/åå°ï¼‰ã€‚"""
    try:
        return request_queue.list()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/requests/{job_id}/cancel")
async def queue_cancel(job_id: str):
    """å–æ¶ˆæŒ‡å®šé˜Ÿåˆ—ä»»åŠ¡ï¼ˆè¿è¡Œä¸­å°†é‡Šæ”¾å¯¹åº”é€šé“ä¿¡å·é‡ï¼‰ã€‚"""
    try:
        ok = await request_queue.cancel(job_id, reason="manual_cancel")
        if not ok:
            raise HTTPException(status_code=404, detail="job not found")
        # è¿”å›æœ€æ–°å¿«ç…§
        job = None
        try:
            job = next((j for j in request_queue.list() if j.get('id') == job_id), None)
        except Exception:
            job = None
        return {"ok": True, "job": job or {"id": job_id, "status": "canceled"}}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/queue/insights")
async def queue_insights():
    """è¿”å›é˜Ÿåˆ—æ´å¯Ÿï¼ˆä¸ /api/queue/stats ç›¸åŒè¯­ä¹‰ï¼Œå…¼å®¹å‰ç«¯å¯èƒ½çš„å‘½åï¼‰ã€‚"""
    try:
        return request_queue.stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/queue/capacity")
async def queue_capacity(db: Session = Depends(get_db)):
    """è¯»å–å½“å‰ç›®æ ‡å¹¶å‘å®¹é‡ï¼ˆæ¥è‡ªå†…å­˜çŠ¶æ€ï¼Œé™„å¸¦æŒä¹…åŒ–å»ºè®®å€¼ï¼‰ã€‚"""
    try:
        s = request_queue.stats()
        # è¯»å–å·²æŒä¹…åŒ–çš„å»ºè®®å€¼
        key_cookie = 'queue_cap_cookie'
        key_nocookie = 'queue_cap_nocookie'
        rowc = db.query(Settings).filter(Settings.key == key_cookie).first()
        r_own = db.query(Settings).filter(Settings.key == key_nocookie).first()
        persisted = {
            'requires_cookie': int(rowc.value) if rowc and rowc.value is not None else None,
            'no_cookie': int(r_own.value) if r_own and r_own.value is not None else None,
        }
        return {"capacity": s.get('capacity', {}), "persisted": persisted}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

@app.post("/api/queue/capacity")
async def queue_set_capacity(body: QueueCapacityBody, db: Session = Depends(get_db)):
    """è®¾ç½®å¹¶å‘å®¹é‡ä¸Šé™ï¼šç«‹å³å¯¹å†…å­˜ç”Ÿæ•ˆï¼Œå¯é€‰æŒä¹…åŒ–åˆ° Settingsã€‚
    - requires_cookie: åˆç†èŒƒå›´ 1-3ï¼ˆ0 è§†ä¸ºä¸å˜ï¼›å®‰å…¨èµ·è§ç¦æ­¢>3ï¼‰
    - no_cookie:      åˆç†èŒƒå›´ 1-5ï¼ˆ0 è§†ä¸ºä¸å˜ï¼›ç¦æ­¢>5ï¼‰
    """
    try:
        if body is None:
            raise HTTPException(status_code=400, detail="missing body")

        def _sanitize(v: Optional[int], lo: int, hi: int) -> Optional[int]:
            if v is None:
                return None
            try:
                iv = int(v)
            except Exception:
                raise HTTPException(status_code=400, detail="invalid value")
            if iv <= 0:
                return None
            if iv < lo:
                iv = lo
            if iv > hi:
                iv = hi
            return iv

        cap_cookie = _sanitize(body.requires_cookie, 1, 3)
        cap_nocookie = _sanitize(body.no_cookie, 1, 5)

        # ç«‹å³ç”Ÿæ•ˆ
        await request_queue.set_capacity(requires_cookie=cap_cookie, no_cookie=cap_nocookie)

        # å¯é€‰æŒä¹…åŒ–
        if body.persist:
            try:
                if cap_cookie is not None:
                    sc = db.query(Settings).filter(Settings.key == 'queue_cap_cookie').first()
                    if not sc:
                        sc = Settings(key='queue_cap_cookie', value=str(cap_cookie), description='é˜Ÿåˆ—å¹¶å‘ä¸Šé™(cookie)')
                        db.add(sc)
                    else:
                        sc.value = str(cap_cookie)
                if cap_nocookie is not None:
                    sn = db.query(Settings).filter(Settings.key == 'queue_cap_nocookie').first()
                    if not sn:
                        sn = Settings(key='queue_cap_nocookie', value=str(cap_nocookie), description='é˜Ÿåˆ—å¹¶å‘ä¸Šé™(no_cookie)')
                        db.add(sn)
                    else:
                        sn.value = str(cap_nocookie)
                db.commit()
            except Exception:
                db.rollback()

        return {"ok": True, "applied": {"requires_cookie": cap_cookie, "no_cookie": cap_nocookie}}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

# å¤±è´¥ç®¡ç† API
@app.get("/api/failures")
async def list_failures(sid: Optional[int] = None, clazz: Optional[str] = None, limit: int = 100, offset: int = 0, db: Session = Depends(get_db)):
    """åˆ—å‡ºå¤±è´¥è®°å½•ï¼ˆæ¥æºäº Settings: fail:{bvid}ï¼‰ã€‚æ”¯æŒæŒ‰è®¢é˜…ä¸åˆ†ç±»è¿‡æ»¤ã€‚
    - å‚æ•°ï¼šsidï¼ˆå¯é€‰ï¼‰ã€clazz=temporary|permanentï¼ˆå¯é€‰ï¼‰ã€limit/offsetï¼ˆç®€å•åˆ†é¡µï¼‰
    - è¿”å›ï¼šitems: [{ bvid, class, message, last_at, sid, retry_count }]
    """
    try:
        rows = db.query(Settings).filter(Settings.key.like('fail:%')).all()
        items = []
        for r in rows:
            try:
                data = json.loads(r.value) if (r and r.value) else {}
                if not isinstance(data, dict):
                    continue
                bvid = (r.key or '')[5:]
                if not bvid:
                    continue
                if sid is not None and int(data.get('sid') or -1) != int(sid):
                    continue
                if clazz and (data.get('class') != clazz):
                    continue
                items.append({
                    'bvid': bvid,
                    'class': data.get('class'),
                    'message': data.get('message'),
                    'last_at': data.get('last_at'),
                    'sid': data.get('sid'),
                    'retry_count': data.get('retry_count') or 0,
                })
            except Exception:
                continue
        # ç®€å•åˆ†é¡µ
        items_sorted = sorted(items, key=lambda x: x.get('last_at') or '', reverse=True)
        return {
            'total': len(items_sorted),
            'items': items_sorted[offset: offset + max(1, min(1000, limit))]
        }
    finally:
        db.close()

@app.get("/api/failures/{bvid}")
async def get_failure_detail(bvid: str, db: Session = Depends(get_db)):
    """è·å–å•æ¡å¤±è´¥è¯¦æƒ…ã€‚"""
    try:
        key = f"fail:{bvid}"
        r = db.query(Settings).filter(Settings.key == key).first()
        if not r:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°å¤±è´¥è®°å½•")
        try:
            data = json.loads(r.value) if r.value else {}
        except Exception:
            data = {}
        return {'bvid': bvid, **({} if not isinstance(data, dict) else data)}
    finally:
        db.close()

class FailureRetryBody(BaseModel):
    sid: Optional[int] = None
    mode: Optional[str] = 'enqueue'  # enqueue | queue_only

@app.post("/api/failures/{bvid}/unblock")
async def unblock_failure(bvid: str, db: Session = Depends(get_db)):
    """è§£å°ï¼šåˆ é™¤å¤±è´¥è®°å½•ï¼ˆå…è®¸åç»­å…¥é˜Ÿï¼‰ã€‚"""
    try:
        key = f"fail:{bvid}"
        r = db.query(Settings).filter(Settings.key == key).first()
        if r:
            db.delete(r)
            db.commit()
        return {'ok': True}
    finally:
        db.close()

@app.post("/api/failures/{bvid}/retry")
async def retry_failure(bvid: str, body: FailureRetryBody, db: Session = Depends(get_db)):
    """é‡è¯•ï¼šæ¸…ç†å¤±è´¥è®°å½•ï¼Œå¹¶å°†è§†é¢‘åŠ å…¥è¯¥è®¢é˜…çš„å…¥é˜Ÿæµç¨‹ã€‚
    - sid å–é¡ºåºï¼šbody.sid > failè®°å½•å†…sid
    - è‹¥æ— æ³•ç¡®å®š sidï¼Œåˆ™è¿”å› 400
    - mode=enqueue ç›´æ¥è°ƒç”¨ä¸‹è½½å…¥é˜Ÿï¼›queue_only ä»…å°†å…¶æ”¾å…¥å¤±è´¥å›è¡¥é˜Ÿåˆ—å°¾éƒ¨
    """
    # è¯»å–è®°å½•
    key = f"fail:{bvid}"
    rec = db.query(Settings).filter(Settings.key == key).first()
    rec_sid = None
    if rec and rec.value:
        try:
            data = json.loads(rec.value)
            if isinstance(data, dict):
                rec_sid = data.get('sid')
        except Exception:
            pass
    target_sid = body.sid or rec_sid
    if not target_sid:
        raise HTTPException(status_code=400, detail="æ— æ³•ç¡®å®šè®¢é˜…ID")
    # åˆ é™¤å¤±è´¥è®°å½•
    try:
        if rec:
            db.delete(rec)
            db.commit()
    except Exception:
        db.rollback()
    # å…¥é˜Ÿ
    try:
        url = _safe_bilibili_url(bvid)
        if not url:
            raise HTTPException(status_code=400, detail="éæ³•BVID")
        if (body.mode or 'enqueue') == 'queue_only':
            # æ”¾å…¥å›è¡¥é˜Ÿåˆ—å°¾éƒ¨
            k = f"retry:{int(target_sid)}:failed_backfill"
            s = db.query(Settings).filter(Settings.key == k).first()
            arr = []
            if s and s.value:
                try:
                    arr = json.loads(s.value)
                    if not isinstance(arr, list):
                        arr = []
                except Exception:
                    arr = []
            arr.append(bvid)
            val = json.dumps(arr, ensure_ascii=False)
            if s:
                s.value = val
                s.description = s.description or 'å¤±è´¥å›è¡¥é˜Ÿåˆ—'
            else:
                db.add(Settings(key=k, value=val, description='å¤±è´¥å›è¡¥é˜Ÿåˆ—'))
            db.commit()
            return {'queued': True, 'mode': 'queue_only', 'sid': int(target_sid)}
        else:
            # ç›´æ¥è°ƒç”¨ä¸‹è½½å…¥é˜Ÿ
            await downloader._download_single_video({
                'id': bvid,
                'title': bvid,
                'webpage_url': url,
                'url': url,
            }, int(target_sid), db)
            return {'enqueued': True, 'mode': 'enqueue', 'sid': int(target_sid)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ï¼šä¸€é”®æœ¬åœ°åŒæ­¥ï¼ˆå…¨é‡æ‰«æ + è‡ªåŠ¨å…³è” + ç»Ÿè®¡é‡ç®—ï¼‰
_local_sync_lock = asyncio.Lock()

@app.post("/api/auto-import/scan-associate")
async def auto_import_scan_and_associate():
    """ä¸€é”®æœ¬åœ°åŒæ­¥ï¼šé¡ºåºæ‰§è¡Œæ‰«æå¯¼å…¥ä¸è‡ªåŠ¨å…³è”ï¼Œå¹¶åšä¸€æ¬¡å…¨é‡ç»Ÿè®¡é‡ç®—ã€‚
    - äº’æ–¥ï¼šä¸è‡ªèº«å¹¶å‘äº’æ–¥ï¼Œé¿å…é‡å¤é‡å…¥ã€‚
    - è¿”å›ï¼šæ•´åˆä¸¤ä¸ªé˜¶æ®µè¿”å›å€¼ä¸æœ¬æ¬¡åŒæ­¥æ—¶é—´æˆ³ã€‚
    """
    if _local_sync_lock.locked():
        # è¿”å› 202 è¡¨ç¤ºå·²åœ¨è¿›è¡Œä¸­
        return {"message": "æœ¬åœ°åŒæ­¥å·²åœ¨è¿è¡Œä¸­", "running": True}
    async with _local_sync_lock:
        try:
            from .auto_import import auto_import_service
            # 1) æ‰«æå¯¼å…¥ï¼ˆçº¿ç¨‹æ± æ‰§è¡Œï¼‰
            scan_res = await asyncio.to_thread(auto_import_service.scan_and_import)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"æ‰«æå¯¼å…¥å¤±è´¥: {e}")
        try:
            # 2) è‡ªåŠ¨å…³è”ï¼ˆçº¿ç¨‹æ± æ‰§è¡Œï¼‰
            assoc_res = await asyncio.to_thread(auto_import_service.auto_associate_subscriptions)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨å…³è”å¤±è´¥: {e}")
        # 3) ç»Ÿä¸€é‡ç®—è®¢é˜…ç»Ÿè®¡
        db = next(get_db())
        try:
            try:
                recompute_all_subscriptions(db, touch_last_check=False)
                db.commit()
            except Exception as re:
                db.rollback()
                # ä¸è‡´å‘½ï¼Œçº³å…¥è¿”å›ä¿¡æ¯
                assoc_res = {**(assoc_res or {}), "recompute_error": str(re)}
        finally:
            db.close()

        return {
            "message": "æœ¬åœ°åŒæ­¥å®Œæˆ",
            "running": False,
            "scan": scan_res or {},
            "associate": assoc_res or {},
            "completed_at": datetime.now().isoformat(),
        }

@app.post("/api/auto-import/scan-associate/{subscription_id}")
async def auto_import_scan_and_associate_for_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """æŒ‰è®¢é˜…æ‰§è¡Œæœ¬åœ°åŒæ­¥ï¼ˆä»…è¯¥è®¢é˜…ç›®å½•æ‰«æå¯¼å…¥ + ä»…è¯¥è®¢é˜…è‡ªåŠ¨å…³è” + ä»…è¯¥è®¢é˜…ç»Ÿè®¡é‡ç®—ï¼‰ã€‚
    - ä¸ç›¸åŒè®¢é˜…å¹¶å‘äº’æ–¥ï¼ˆä¸ä¸‹è½½ç­‰å…¶ä»–æ“ä½œæ— å¼ºè€¦åˆï¼Œä»…ä½¿ç”¨è®¢é˜…çº§é”ï¼‰ã€‚
    - ä¸ä¸å…¨å±€æœ¬åœ°åŒæ­¥å…±ç”¨é”ï¼Œä»¥å‡å°‘é˜»å¡ï¼Œä½†éœ€æ³¨æ„ç”¨æˆ·ä¾§é¿å…åŒæ—¶è§¦å‘å…¨å±€ä¸å±€éƒ¨ã€‚
    """
    # è®¢é˜…æ˜¯å¦å­˜åœ¨
    sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not sub:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")

    # è®¢é˜…çº§äº’æ–¥
    lock = get_subscription_lock(subscription_id)
    if lock.locked():
        return {"message": "è¯¥è®¢é˜…çš„æœ¬åœ°åŒæ­¥å·²åœ¨è¿è¡Œä¸­", "running": True}

    async with lock:
        try:
            from .auto_import import auto_import_service
            # 1) ä»…æ‰«æè¯¥è®¢é˜…ç›®å½•å¹¶å¯¼å…¥
            scan_res = await asyncio.to_thread(auto_import_service.scan_and_import_for_subscription, subscription_id)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"æ‰«æå¯¼å…¥å¤±è´¥: {e}")

        # 2) ä»…è¯¥è®¢é˜…è‡ªåŠ¨å…³è”ï¼ˆä»¥æœ¬åœ°ç›®å½•ä¸ºå‡†ï¼šè‹¥è®°å½•å½“å‰å…³è”åˆ°å…¶ä»–è®¢é˜…ä½†è·¯å¾„è½åœ¨æœ¬è®¢é˜…ç›®å½•ä¸‹ï¼Œä¹Ÿè¿›è¡Œé‡å…³è”ä¿®å¤ï¼‰
        try:
            matches = auto_import_service._find_matching_videos(sub, db)
            associated_count = 0
            for v in matches:
                # ä»¥æœ¬åœ°ç›®å½•ä¸ºå‡†è¿›è¡Œå¼ºåˆ¶ä¿®å¤ï¼š
                # - åŸé€»è¾‘ä»…å…³è” subscription_id ä¸ºç©ºçš„è®°å½•ï¼Œæ— æ³•ä¿®å¤â€œé”™å…³è”åˆ°å…¶ä»–è®¢é˜…â€çš„æƒ…å†µ
                # - è¿™é‡Œå¯¹æ‰€æœ‰å‘½ä¸­æœ¬è®¢é˜…ç›®å½•çš„è®°å½•æ‰§è¡Œç»Ÿä¸€å½’å±ï¼Œç¡®ä¿DBä¸æœ¬åœ°ç›®å½•ä¸€è‡´
                if v.subscription_id != subscription_id:
                    v.subscription_id = subscription_id
                    associated_count += 1
            db.commit()
        except Exception as e:
            db.rollback()
            raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨å…³è”å¤±è´¥: {e}")

        # 3) ä»¥æœ¬åœ°ä¸ºå‡†çš„æ¸…ç†ä¸çº åï¼š
        #    - è‹¥è§†é¢‘ä¸JSONå‡ä¸å­˜åœ¨ï¼šåˆ¤å®šä¸ºâ€œå·²åˆ é™¤â€ï¼Œç›´æ¥åˆ é™¤DBè®°å½•ï¼ˆé¿å… total_videos åå¤§ï¼‰
        #    - è‹¥ä»…è§†é¢‘æ–‡ä»¶ç¼ºå¤±ä½†JSONå­˜åœ¨ï¼šæ ‡è®° downloaded=Falseï¼Œæ¸…ç©º video_path ä¸æ–‡ä»¶å¤§å°ç›¸å…³å­—æ®µ
        try:
            from pathlib import Path
            from .models import Video
            videos = db.query(Video).filter(Video.subscription_id == subscription_id).all()
            removed_count = 0
            downgraded_count = 0
            for v in videos:
                vp = Path(v.video_path) if getattr(v, 'video_path', None) else None
                jp = Path(v.json_path) if getattr(v, 'json_path', None) else None
                v_exists = (vp and vp.exists())
                j_exists = (jp and jp.exists())
                if (not v_exists) and (not j_exists):
                    # ä¸¤è€…éƒ½ä¸å­˜åœ¨ï¼šåˆ é™¤è®°å½•
                    db.delete(v)
                    removed_count += 1
                    continue
                if (not v_exists) and j_exists:
                    # ä»…è§†é¢‘ç¼ºï¼šé™çº§ downloaded çŠ¶æ€å¹¶æ¸…ç†è·¯å¾„/å¤§å°
                    v.downloaded = False
                    v.video_path = None
                    try:
                        v.file_size = 0
                    except Exception:
                        pass
                    downgraded_count += 1
            db.commit()
        except Exception as e:
            db.rollback()
            logger.warning(f"[sub={subscription_id}] æœ¬åœ°ä¸€è‡´æ€§æ¸…ç†å¤±è´¥ï¼š{e}")

        # 4) ä»…é‡ç®—è¯¥è®¢é˜…ç»Ÿè®¡
        try:
            from .services.subscription_stats import recompute_subscription_stats
            recompute_subscription_stats(db, subscription_id, touch_last_check=False)
            db.commit()
        except Exception as re:
            db.rollback()
            # ä¸è‡´å‘½ï¼Œçº³å…¥è¿”å›ä¿¡æ¯
            recompute_error = str(re)
        else:
            recompute_error = None

        return {
            "message": "æœ¬åœ°åŒæ­¥å®Œæˆï¼ˆæŒ‰è®¢é˜…ï¼‰",
            "running": False,
            "scan": scan_res or {},
            "associate": {"associated": associated_count},
            "subscription_id": subscription_id,
            "recompute_error": recompute_error,
            "completed_at": datetime.now().isoformat(),
        }

# èšåˆä¸‹è½½ç®¡ç† APIï¼šæŒ‰è®¢é˜…è¿”å›æœ¬åœ°å·²ä¸‹è½½ã€ä¼°ç®—å¾…ä¸‹è½½ã€é˜Ÿåˆ—æ’é˜Ÿ/è¿è¡Œä¸­è®¡æ•°
@app.get("/api/download/aggregate")
async def download_aggregate(db: Session = Depends(get_db)):
    """è¿”å›æ¯ä¸ªå¯ç”¨è®¢é˜…çš„èšåˆä¸‹è½½çŠ¶æ€ã€‚
    å­—æ®µï¼š
    - subscription: { id, name, type }
    - downloaded: æœ¬åœ°æœ‰æ–‡ä»¶æ•°ï¼ˆä»¥ç›®å½•ä¸ºå‡†ï¼‰
    - pending_estimated: ä¼°ç®—å¾…ä¸‹è½½æ•°ï¼ˆremote_total - æœ¬åœ°æœ‰æ–‡ä»¶æ•°ï¼›remote_total ç¼ºå¤±æ—¶æŒ‰0å¤„ç†ï¼‰
    - queue: { queued, running }
    - remote_total: æœ€è¿‘ä¸€æ¬¡åŒæ­¥è®°å½•ä¸­çš„è¿œç«¯æ€»æ•°ï¼ˆå¦‚æœ‰ï¼‰
    """
    try:
        # 1) å–å¯ç”¨è®¢é˜…
        active_subs = db.query(Subscription).filter(Subscription.is_active == True).all()
        if not active_subs:
            return { 'items': [], 'totals': { 'downloaded': 0, 'pending_estimated': 0, 'queued': 0, 'running': 0 } }

        sub_ids = [s.id for s in active_subs]

        # 2) é˜Ÿåˆ—å¿«ç…§ -> (sid -> queued/running)
        q_items = request_queue.list()
        q_index: Dict[int, Dict[str, int]] = {}
        for j in q_items:
            try:
                if j.get('type') != 'download':
                    continue
                sid = j.get('subscription_id')
                if sid is None:
                    continue
                status = j.get('status')
                if status not in ('queued', 'running'):
                    continue
                bucket = q_index.setdefault(int(sid), {'queued': 0, 'running': 0})
                bucket[status] += 1
            except Exception:
                continue

        # 3) å·²ä¸‹è½½è®¡æ•°ï¼ˆæœ¬åœ°æœ‰æ–‡ä»¶ï¼‰åˆ†ç»„ç»Ÿè®¡
        downloaded_rows = (
            db.query(Video.subscription_id, func.count(Video.id))
              .filter(Video.video_path.isnot(None), Video.subscription_id.in_(sub_ids))
              .group_by(Video.subscription_id)
              .all()
        )
        downloaded_map: Dict[int, int] = {sid: int(cnt) for sid, cnt in downloaded_rows}

        # 4) æ‰¹é‡è¯»å– Settingsï¼š
        #    - sync:{id}:status -> remote_total
        #    - agg:{id}:pending_estimated -> ç¼“å­˜çš„å¾…ä¸‹è½½ä¼°ç®—
        keys = []
        for sid in sub_ids:
            keys.append(f"sync:{sid}:status")
            keys.append(f"agg:{sid}:pending_estimated")
        settings_rows = db.query(Settings).filter(Settings.key.in_(keys)).all()
        settings_map = {s.key: s.value for s in settings_rows if s and s.key}

        result = []
        total_downloaded = 0
        total_pending = 0
        total_queued = 0
        total_running = 0

        for sub in active_subs:
            downloaded = int(downloaded_map.get(sub.id, 0))
            total_downloaded += downloaded

            # è¯»å– remote_total
            remote_total = None
            try:
                key = f"sync:{sub.id}:status"
                sval = settings_map.get(key)
                if sval:
                    data = json.loads(sval)
                    rt = data.get("remote_total")
                    if isinstance(rt, int) and rt >= 0:
                        remote_total = rt
            except Exception:
                remote_total = None

            # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„ pending ä¼°ç®—å€¼
            pending_cached = None
            try:
                pc_val = settings_map.get(f"agg:{sub.id}:pending_estimated")
                if pc_val is not None:
                    pending_cached = int(str(pc_val).strip())
            except Exception:
                pending_cached = None

            pending_estimated = None
            if isinstance(pending_cached, int):
                pending_estimated = max(0, pending_cached)
            else:
                pending_estimated = max(0, int(remote_total) - downloaded) if remote_total is not None else 0

            qbucket = q_index.get(sub.id, {'queued': 0, 'running': 0})
            total_pending += pending_estimated
            total_queued += qbucket['queued']
            total_running += qbucket['running']

            result.append({
                'subscription': {
                    'id': sub.id,
                    'name': sub.name,
                    'type': sub.type,
                },
                'downloaded': downloaded,
                'pending_estimated': pending_estimated,
                'queue': {
                    'queued': qbucket['queued'],
                    'running': qbucket['running'],
                },
                'remote_total': remote_total,
            })

        return {
            'items': result,
            'totals': {
                'downloaded': total_downloaded,
                'pending_estimated': total_pending,
                'queued': total_queued,
                'running': total_running,
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/subscriptions/{subscription_id}/enqueue_video")
async def enqueue_single_video(subscription_id: int, request: dict, db: Session = Depends(get_db)):
    """å°†æŒ‡å®šè§†é¢‘åŠ å…¥è¯¥è®¢é˜…çš„ä¸‹è½½é˜Ÿåˆ—ï¼ˆç«‹å³è¿›å…¥å…¨å±€è¯·æ±‚é˜Ÿåˆ—ï¼ŒæŒ‰å¹¶å‘ç­–ç•¥æ‰§è¡Œï¼‰ã€‚
    è¯·æ±‚ä½“ï¼š{ video_id: str, title?: str, webpage_url?: str }
    è¿”å›ï¼šä¸‹è½½æ‰§è¡Œç»“æœï¼ˆå¼€å§‹æ‰§è¡Œåå³æŒ‰ç°æœ‰æµç¨‹å…¥é˜Ÿå¹¶ä¸‹è½½ï¼‰ã€‚
    """
    try:
        sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
        if not sub:
            raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
        if sub.type != 'collection':
            raise HTTPException(status_code=400, detail="ä»…æ”¯æŒåˆé›†è®¢é˜…")

        video_id = (request or {}).get('video_id')
        if not video_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘ video_id")
        title = (request or {}).get('title')
        url = (request or {}).get('webpage_url') or _safe_bilibili_url(video_id)
        if not url:
            raise HTTPException(status_code=400, detail="éæ³•BVIDæˆ–ç¼ºå°‘URL")

        # å¤ç”¨å•è§†é¢‘ä¸‹è½½æµç¨‹ï¼šå†…éƒ¨ä¼šå°† download ä»»åŠ¡å†™å…¥å…¨å±€é˜Ÿåˆ—å¹¶å—å¹¶å‘æ§åˆ¶
        video_info = { 'id': video_id, 'title': title, 'webpage_url': url, 'url': url }
        result = await downloader._download_single_video(video_info, subscription_id, db)
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/queue/list")
async def queue_list():
    """è¿”å›æ‰€æœ‰ä»»åŠ¡çš„å¿«ç…§ï¼ˆåŒ…å« wait_msã€last_wait_reason ç­‰è¯Šæ–­å­—æ®µï¼‰ã€‚"""
    try:
        return request_queue.list()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# è®¢é˜…ç®¡ç†API
@app.get("/api/subscriptions")
async def get_subscriptions(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰è®¢é˜…ï¼ŒåŒ…å«è¿œç«¯æ€»è®¡ä¸å¾…ä¸‹è½½ä¼°ç®—ï¼ˆå¾…ä¸‹è½½=è¿œç«¯-æœ¬åœ°ç›®å½•è§†é¢‘æ•°ï¼‰ã€‚"""
    subscriptions = db.query(Subscription).all()
    result = []

    from .models import Settings
    import json

    for sub in subscriptions:
        # æœ¬åœ°ç»Ÿè®¡ï¼ˆå£å¾„Aï¼šä»¥æœ‰æ–‡ä»¶ä¸ºå‡†ï¼‰
        total_videos_local_all = db.query(Video).filter(Video.subscription_id == sub.id).count()
        downloaded_videos = db.query(Video).filter(
            Video.subscription_id == sub.id,
            Video.video_path.isnot(None)
        ).count()
        on_disk_total = downloaded_videos

        # è¿œç«¯æ€»è®¡è¯»å–ï¼šä¼˜å…ˆä½¿ç”¨ downloader å†™å…¥çš„ sync:{id}:statusï¼ˆremote_totalï¼‰
        remote_total = None
        remote_status = None
        try:
            key = f"sync:{sub.id}:status"
            s = db.query(Settings).filter(Settings.key == key).first()
            if s and s.value:
                data = json.loads(s.value)
                rt = data.get("remote_total")
                if isinstance(rt, int) and rt >= 0:
                    remote_total = rt
                st = data.get('status')
                # è‹¥æ­£åœ¨è·å–ä¸” remote_total å°šæœªå†™å›ï¼ŒçŠ¶æ€æ˜¾ç¤ºä¸º fetching
                if (st == 'running') and (remote_total is None):
                    remote_status = 'fetching'
        except Exception:
            remote_total = None

        # å¾…ä¸‹è½½ä»…ä»¥è¿œç«¯ä¸ºå‡†ï¼šremote_total ç¼ºå¤±æ—¶æŒ‰ 0 å¤„ç†ï¼ˆä¸å†ç”¨æœ¬åœ°å›é€€ï¼‰
        pending_videos = max(0, (remote_total or 0) - on_disk_total)

        # æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»¥æœ¬åœ°æœ‰æ–‡ä»¶æ•°ä¸ºæ€»æ•°ï¼Œä¿æŒä¸å…¶ä»–é¡µé¢ä¸€è‡´ï¼‰
        sub.total_videos = on_disk_total
        sub.downloaded_videos = downloaded_videos

        result.append({
            "id": sub.id,
            "name": sub.name,
            "type": sub.type,
            "url": sub.url,
            "is_active": sub.is_active,
            "total_videos": on_disk_total,
            "db_total_videos": total_videos_local_all,
            "remote_total": remote_total,
            "remote_status": remote_status,
            "downloaded_videos": downloaded_videos,
            "pending_videos": pending_videos,
            "last_check": sub.last_check.isoformat() if sub.last_check else None,
            "created_at": sub.created_at.isoformat() if sub.created_at else None,
            "updated_at": sub.updated_at.isoformat() if sub.updated_at else None
        })
    
    db.commit()
    return result

@app.get("/api/overview")
async def get_overview(db: Session = Depends(get_db)):
    """å…¨å±€æ€»è§ˆï¼šRemote/Local/Pending æ±‡æ€»ï¼ŒåŠé˜Ÿåˆ—åˆ†å¸ƒä¸æœ€è¿‘å¤±è´¥æ•°ã€‚"""
    try:
        subs = db.query(Subscription).all()
        from .models import Settings
        import json as json_lib

        remote_total_sum = 0
        local_total_sum = 0
        pending_total_sum = 0

        for sub in subs:
            local = db.query(Video).filter(Video.subscription_id == sub.id).count()
            local_total_sum += local

            remote = None
            try:
                key = f"sync:{sub.id}:status"
                s = db.query(Settings).filter(Settings.key == key).first()
                if s and s.value:
                    data = json_lib.loads(s.value)
                    rt = data.get("remote_total")
                    if isinstance(rt, int) and rt >= 0:
                        remote = rt
            except Exception:
                remote = None

            # è¿œç«¯æ±‡æ€»ä¸¥æ ¼ä»¥è¿œç«¯ä¸ºå‡†ï¼šä»…ç´¯åŠ å·²æœ‰è¿œç«¯æ•°æ®
            remote_total_sum += (remote or 0)
            # å¾…ä¸‹è½½æ€»æ•°ä»…åœ¨è¿œç«¯å­˜åœ¨æ—¶è®¡ç®—ï¼›ç¼ºå¤±è¿œç«¯æ—¶ä¸å†ç”¨æœ¬åœ°å›é€€
            if remote is not None:
                pending_total_sum += max(0, remote - local)

        # é˜Ÿåˆ—ç»Ÿè®¡
        qstats = request_queue.stats()
        qlist = request_queue.list()
        now = datetime.now()
        recent_failed_24h = sum(1 for j in qlist if j.get('status') == 'failed' and j.get('finished_at') and \
                                 isinstance(j.get('finished_at'), datetime) and (now - j['finished_at']) <= timedelta(hours=24))

        return {
            'remote_total': remote_total_sum,
            'local_total': local_total_sum,
            'pending_total': pending_total_sum,
            'queue': {
                'queued': qstats.get('counts', {}).get('queued', 0),
                'running': qstats.get('counts', {}).get('running', 0),
                'done': qstats.get('counts', {}).get('done', 0),
                'failed': qstats.get('counts', {}).get('failed', 0),
            },
            'recent_failed_24h': recent_failed_24h,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/queue/insights")
async def queue_insights():
    """é˜Ÿåˆ—è¯Šæ–­ï¼šç­‰å¾…åŸå› åˆ†å¸ƒã€é”™è¯¯TopNã€å¤±è´¥æ ·æœ¬ã€å®¹é‡ä¸æš‚åœçŠ¶æ€ã€‚"""
    try:
        items = request_queue.list()
        # ç­‰å¾…åŸå› åˆ†å¸ƒ
        wait_dist: Dict[str, int] = {}
        for j in items:
            reason = j.get('last_wait_reason') or ''
            if reason:
                wait_dist[reason] = wait_dist.get(reason, 0) + 1
        # é”™è¯¯ TopN
        error_count: Dict[str, int] = {}
        failed_samples = []
        for j in items:
            if j.get('status') == 'failed':
                err = (j.get('last_error') or '').strip()
                if err:
                    error_count[err] = error_count.get(err, 0) + 1
                # æ”¶é›†æ ·æœ¬
                failed_samples.append({
                    'id': j.get('id'),
                    'type': j.get('type'),
                    'subscription_id': j.get('subscription_id'),
                    'video_id': j.get('video_id'),
                    'finished_at': j.get('finished_at').isoformat() if j.get('finished_at') else None,
                    'wait_ms': j.get('wait_ms'),
                    'wait_cycles': j.get('wait_cycles'),
                    'acquired_scope': j.get('acquired_scope'),
                    'last_error': err,
                })
        errors_top = sorted([
            {'error': k, 'count': v} for k, v in error_count.items()
        ], key=lambda x: x['count'], reverse=True)[:10]

        qstats = request_queue.stats()
        # åªè¿”å›å¿…è¦å­—æ®µï¼Œé¿å…æ³„æ¼å†…éƒ¨ç»†èŠ‚
        return {
            'wait_reasons': wait_dist,
            'errors_top': errors_top,
            'failed_samples': failed_samples[-20:],
            'paused': qstats.get('paused', {}),
            'capacity': qstats.get('capacity', {}),
            'counts': qstats.get('counts', {}),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/subscriptions")
async def create_subscription(subscription: dict, db: Session = Depends(get_db)):
    """åˆ›å»ºæ–°è®¢é˜…"""
    try:
        # è§£ææ—¥æœŸå­—æ®µ
        date_after = None
        date_before = None
        if subscription.get("date_after"):
            try:
                date_after = datetime.strptime(subscription["date_after"], "%Y-%m-%d").date()
            except ValueError:
                pass
        if subscription.get("date_before"):
            try:
                date_before = datetime.strptime(subscription["date_before"], "%Y-%m-%d").date()
            except ValueError:
                pass

        # è§„èŒƒåŒ–åç§°ï¼šå½“ä¸ºåˆé›†è®¢é˜…ä¸”æœªæä¾›åç§°æ—¶ï¼Œè‡ªåŠ¨è¯†åˆ«åç§°ï¼ˆä¼˜å…ˆä½¿ç”¨åˆé›†å±‚title + uploaderï¼‰
        name_to_use = (subscription.get("name") or "").strip()
        sub_type = (subscription.get("type") or "").strip()
        url_for_parse = (subscription.get("url") or "").strip()

        if (not name_to_use) and sub_type == "collection" and url_for_parse:
            try:
                # å€Ÿç”¨ cookies ä»¥æé«˜è§£ææˆåŠŸç‡
                cookie = cookie_manager.get_available_cookie(db)
                import tempfile, os
                cookies_path = None
                if cookie:
                    fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
                    os.close(fd)
                    with open(cookies_path, 'w', encoding='utf-8') as cf:
                        cf.write("# Netscape HTTP Cookie File\n")
                        cf.write("# This file was generated by bili_curator V6\n\n")
                        cf.writelines([
                            f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                            f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                            f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
                        ])

                # æ–¹æ¡ˆAï¼šä¼˜å…ˆè·å–åˆé›†å±‚ä¿¡æ¯ï¼ˆæ›´æ¥è¿‘ç½‘ç«™æ˜¾ç¤ºï¼‰ï¼štitle + uploader
                import json as json_lib
                pl_cmd = [
                    'yt-dlp',
                    '--flat-playlist',
                    '--dump-single-json',
                    '--no-download',
                ]
                if cookies_path:
                    pl_cmd += ['--cookies', cookies_path]
                pl_cmd.append(url_for_parse)

                pl_proc = await asyncio.create_subprocess_exec(
                    *pl_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                pl_stdout, pl_stderr = await pl_proc.communicate()
                if pl_proc.returncode == 0:
                    try:
                        data = json_lib.loads(pl_stdout.decode('utf-8', errors='ignore') or '{}')
                    except Exception:
                        data = {}
                    uploader = ''
                    playlist_title = ''
                    if isinstance(data, dict):
                        # Bç«™åˆé›†å±‚å¸¸è§å­—æ®µ
                        playlist_title = (data.get('title') or data.get('playlist_title') or '').strip()
                        uploader = (data.get('uploader') or data.get('channel') or '').strip()
                    # å»é‡ï¼šå¦‚æœæ ‡é¢˜å·²åŒ…å«uploaderï¼Œé¿å…é‡å¤
                    if uploader and playlist_title:
                        if playlist_title.startswith(uploader) or uploader in playlist_title:
                            name_to_use = playlist_title
                        else:
                            name_to_use = f"{uploader}ï¼š{playlist_title}"
                    elif playlist_title:
                        name_to_use = playlist_title
                    elif uploader:
                        name_to_use = f"{uploader}çš„åˆé›†"
                    # æ›´æ–°cookieä½¿ç”¨ç»Ÿè®¡
                    if cookie:
                        try:
                            cookie_manager.update_cookie_usage(db, cookie.id)
                        except Exception:
                            pass

                # æ–¹æ¡ˆBï¼šå¦‚æœªè·å¾—æœ‰æ•ˆåç§°ï¼Œå›é€€é¦–æ¡è§†é¢‘çš„ playlist_title
                if not name_to_use:
                    fe_cmd = [
                        'yt-dlp',
                        '--dump-json',
                        '--playlist-items', '1',
                        '--no-download',
                    ]
                    if cookies_path:
                        fe_cmd += ['--cookies', cookies_path]
                    fe_cmd.append(url_for_parse)
                    fe_proc = await asyncio.create_subprocess_exec(
                        *fe_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    fe_stdout, fe_stderr = await fe_proc.communicate()
                    if fe_proc.returncode == 0:
                        for line in fe_stdout.decode('utf-8', errors='ignore').strip().split('\n'):
                            if not line.strip():
                                continue
                            try:
                                info = json_lib.loads(line)
                                uploader = (info.get('uploader') or '').strip()
                                playlist_title = (info.get('playlist_title') or '').strip()
                                title = (info.get('title') or '').strip()
                                # å…¼å®¹ä¸ªåˆ«æƒ…å†µä¸‹ playlist_title ä¸ºç©ºæ—¶å›é€€ title
                                base_title = playlist_title or title
                                if uploader and base_title:
                                    if base_title.startswith(uploader) or uploader in base_title:
                                        name_to_use = base_title
                                    else:
                                        name_to_use = f"{uploader}ï¼š{base_title}"
                                elif base_title:
                                    name_to_use = base_title
                                elif uploader:
                                    name_to_use = f"{uploader}çš„åˆé›†"
                                if name_to_use:
                                    break
                            except Exception:
                                continue

                # æ¸…ç†ä¸´æ—¶ cookie æ–‡ä»¶
                if cookies_path and os.path.exists(cookies_path):
                    try:
                        os.remove(cookies_path)
                    except Exception:
                        pass

            except Exception:
                # è§£æå¤±è´¥åˆ™ç»§ç»­èµ°å…œåº•é€»è¾‘
                pass

            # å…œåº•ï¼šå– URL æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºåç§°
            if not name_to_use and url_for_parse:
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url_for_parse)
                    last_seg = (parsed.path.rstrip('/') or '/').split('/')[-1]
                    name_to_use = last_seg or parsed.netloc or "æœªçŸ¥åˆé›†"
                except Exception:
                    name_to_use = "æœªçŸ¥åˆé›†"

        # è‹¥ä»ä¸ºç©ºï¼Œä½¿ç”¨ä¼ å…¥åç§°ï¼ˆå…¼å®¹éåˆé›†ç±»å‹ï¼‰ï¼›ç¡®ä¿éç©ºä»¥æ»¡è¶³ NOT NULL çº¦æŸ
        if not name_to_use:
            name_to_use = (subscription.get("name") or "æœªçŸ¥è®¢é˜…").strip() or "æœªçŸ¥è®¢é˜…"
        
        db_subscription = Subscription(
            name=name_to_use,
            type=subscription["type"],
            url=subscription.get("url"),
            uploader_id=subscription.get("uploader_id"),
            keyword=subscription.get("keyword"),
            specific_urls=subscription.get("specific_urls"),
            date_after=date_after,
            date_before=date_before,
            min_likes=subscription.get("min_likes"),
            min_favorites=subscription.get("min_favorites"),
            min_views=subscription.get("min_views")
        )
        db.add(db_subscription)
        db.commit()
        db.refresh(db_subscription)
        
        # è‡ªåŠ¨å…³è”å·²ä¸‹è½½çš„è§†é¢‘
        try:
            from app.auto_import import auto_import_service
            matching_videos = auto_import_service._find_matching_videos(db_subscription, db)
            associated_count = 0
            
            for video in matching_videos:
                if not video.subscription_id:  # åªå…³è”æœªå…³è”çš„è§†é¢‘
                    video.subscription_id = db_subscription.id
                    associated_count += 1
            
            # æ›´æ–°è®¢é˜…ç»Ÿè®¡
            db_subscription.downloaded_videos = len([v for v in matching_videos if v.downloaded])
            db_subscription.total_videos = len(matching_videos)
            pending_videos = max(0, (db_subscription.total_videos or 0) - (db_subscription.downloaded_videos or 0))
            db_subscription.updated_at = datetime.now()
            
            db.commit()
            
            return {
                "message": "è®¢é˜…åˆ›å»ºæˆåŠŸ",
                "id": db_subscription.id,
                "associated_videos": associated_count,
                "total_videos": db_subscription.total_videos or 0,
                "downloaded_videos": db_subscription.downloaded_videos or 0,
                "pending_videos": pending_videos,
            }
        except Exception as e:
            # å¦‚æœå…³è”å¤±è´¥ï¼Œä¸å½±å“è®¢é˜…åˆ›å»º
            logger.warning(f"è®¢é˜…åˆ›å»ºæˆåŠŸï¼Œä½†è‡ªåŠ¨å…³è”å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€ç»Ÿè®¡ä¸º0ï¼Œå‰ç«¯å¯åœ¨åˆ—è¡¨æ‹‰å–æ—¶åˆ·æ–°
            return {
                "message": "è®¢é˜…åˆ›å»ºæˆåŠŸ",
                "id": db_subscription.id,
                "associated_videos": 0,
                "total_videos": 0,
                "downloaded_videos": 0,
                "pending_videos": 0,
            }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/subscriptions/{subscription_id}/expected-total")
async def get_subscription_expected_total(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–è¿œç«¯åˆé›†åº”æœ‰æ€»è®¡è§†é¢‘æ•°ï¼ˆä¸ä¾èµ–æœ¬åœ°DBï¼‰ï¼Œç”¨äºæ ¡å‡†æ˜¾ç¤ºã€‚
    ä»…å¯¹ type=collection ä¸”å­˜åœ¨ url çš„è®¢é˜…æœ‰æ•ˆã€‚
    """
    sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not sub:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    if sub.type != "collection" or not sub.url:
        raise HTTPException(status_code=400, detail="è¯¥è®¢é˜…ä¸æ˜¯åˆé›†æˆ–ç¼ºå°‘URL")

    try:
        import tempfile, os, json as json_lib
        sub_lock = get_subscription_lock(sub.id)

        async def run_expected_total(cookies_path: Optional[str], requires_cookie: bool) -> Optional[int]:
            # å…¬å…±å‚æ•°ï¼šUA/Referer/é‡è¯•/è½»ç¡çœ 
            common_args = [
                'yt-dlp',
                '--user-agent', get_user_agent(requires_cookie),
                '--referer', 'https://www.bilibili.com/',
                '--sleep-interval', '2',
                '--max-sleep-interval', '5',
                '--retries', '5',
                '--fragment-retries', '5',
                '--retry-sleep', '3',
                '--ignore-errors',
                '--no-warnings',
                '--no-download',
            ]
            if cookies_path:
                common_args += ['--cookies', cookies_path]

            async def run_and_parse(args):
                timeout_sec = int(os.getenv('EXPECTED_TOTAL_TIMEOUT', '30'))
                async with sub_lock:
                    async with yt_dlp_semaphore:
                        proc = await asyncio.create_subprocess_exec(
                            *args,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE
                        )
                        try:
                            out, err = await asyncio.wait_for(proc.communicate(), timeout=timeout_sec)
                            return proc.returncode, out, err
                        except asyncio.TimeoutError:
                            logger.warning(f"expected_total å‘½ä»¤è¶…æ—¶ (>{timeout_sec}s)ï¼Œæ­£åœ¨ç»ˆæ­¢: {' '.join(args)}")
                            try:
                                proc.terminate()
                                await asyncio.wait_for(proc.wait(), timeout=5.0)
                            except asyncio.TimeoutError:
                                proc.kill()
                            raise

            expected_total = None
            last_err = None
            # åªä½¿ç”¨å¿«é€Ÿå…ƒæ•°æ®è·¯å¾„ï¼ˆä¸åšåˆ†é¡µæšä¸¾ï¼‰

            # Aï¼šdump-single-json
            try:
                cmd_a = common_args + ['--flat-playlist', '--dump-single-json', sub.url]
                rc, out, err = await run_and_parse(cmd_a)
                if rc == 0:
                    try:
                        data = json_lib.loads(out.decode('utf-8', errors='ignore') or '{}')
                        if isinstance(data, dict):
                            expected_total = data.get('n_entries')
                            if expected_total is None:
                                entries = data.get('entries')
                                if isinstance(entries, list):
                                    expected_total = len(entries)
                    except Exception as e:
                        last_err = e
                else:
                    last_err = err.decode('utf-8', errors='ignore')
            except Exception as e:
                last_err = e

            # Bï¼š-J
            if expected_total is None:
                try:
                    cmd_b = common_args + ['-J', sub.url]
                    rc, out, err = await run_and_parse(cmd_b)
                    if rc == 0:
                        try:
                            data = json_lib.loads(out.decode('utf-8', errors='ignore') or '{}')
                            if isinstance(data, dict):
                                expected_total = data.get('n_entries')
                                if expected_total is None:
                                    entries = data.get('entries')
                                    if isinstance(entries, list):
                                        expected_total = len(entries)
                                if expected_total is None:
                                    pc = data.get('playlist_count')
                                    if isinstance(pc, int):
                                        expected_total = pc
                        except Exception as e:
                            last_err = e
                    else:
                        last_err = err.decode('utf-8', errors='ignore')
                except Exception as e:
                    last_err = e

            # Dï¼š--dump-json --flat-playlist
            if expected_total is None:
                try:
                    cmd_d = common_args + ['--dump-json', '--flat-playlist', sub.url]
                    rc, out, err = await run_and_parse(cmd_d)
                    if rc == 0:
                        count = 0
                        for line in (out.decode('utf-8', errors='ignore') or '').strip().split('\n'):
                            if not line.strip():
                                continue
                            try:
                                _ = json_lib.loads(line)
                                count += 1
                            except json_lib.JSONDecodeError:
                                continue
                        if count > 0:
                            expected_total = count
                    else:
                        last_err = err.decode('utf-8', errors='ignore')
                except Exception as e:
                    last_err = e

            # Cï¼šé¦–æ¡ --dump-json
            if expected_total is None:
                try:
                    cmd_c = common_args + ['--dump-json', '--playlist-items', '1', sub.url]
                    rc, out, err = await run_and_parse(cmd_c)
                    if rc == 0:
                        first_line = (out.decode('utf-8', errors='ignore') or '').strip().split('\n')[0]
                        if first_line:
                            try:
                                info = json_lib.loads(first_line)
                                for key in ('n_entries', 'playlist_count', 'playlist_entries', 'total_count'):
                                    val = info.get(key)
                                    if isinstance(val, int):
                                        expected_total = val
                                        break
                                if expected_total is None:
                                    ents = info.get('entries')
                                    if isinstance(ents, list):
                                        expected_total = len(ents)
                            except Exception as e:
                                last_err = e
                    else:
                        last_err = err.decode('utf-8', errors='ignore')
                except Exception as e:
                    last_err = e

            return expected_total

        # ç¬¬ä¸€é˜¶æ®µï¼šæ—  Cookie é€šé“
        job_nc = await request_queue.enqueue(job_type="expected_total", subscription_id=sub.id, requires_cookie=False)
        await request_queue.mark_running(job_nc)
        try:
            expected = await run_expected_total(None, requires_cookie=False)
            if expected is not None:
                await request_queue.mark_done(job_nc)
                return {"expected_total_videos": int(expected), "job_id": job_nc}
            else:
                await request_queue.mark_failed(job_nc, "need_cookie_fallback")
        except Exception as e:
            await request_queue.mark_failed(job_nc, str(e))

        # ç¬¬äºŒé˜¶æ®µï¼šCookie é€šé“ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        cookie = cookie_manager.get_available_cookie(db)
        if not cookie:
            raise HTTPException(status_code=502, detail="éœ€è¦Cookieä½†æ²¡æœ‰å¯ç”¨Cookie")

        # å†™ cookie æ–‡ä»¶
        fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
        os.close(fd)
        with open(cookies_path, 'w', encoding='utf-8') as cf:
            cf.write("# Netscape HTTP Cookie File\n")
            cf.write("# This file was generated by bili_curator V6\n\n")
            if getattr(cookie, 'sessdata', None):
                cf.write(f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n")
            if getattr(cookie, 'bili_jct', None) and str(cookie.bili_jct).strip():
                cf.write(f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n")
            if getattr(cookie, 'dedeuserid', None) and str(cookie.dedeuserid).strip():
                cf.write(f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n")

        job_c = await request_queue.enqueue(job_type="expected_total", subscription_id=sub.id, requires_cookie=True, priority=0)
        await request_queue.mark_running(job_c)
        try:
            expected2 = await run_expected_total(cookies_path, requires_cookie=True)
            if expected2 is None:
                await request_queue.mark_failed(job_c, "cookie_fallback_failed")
                raise HTTPException(status_code=502, detail="æ— æ³•è§£æåˆé›†æ€»æ•°ï¼ˆCookie å›é€€å¤±è´¥ï¼‰")
            # Cookie ä½¿ç”¨ç»Ÿè®¡
            try:
                cookie_manager.update_cookie_usage(db, cookie.id)
            except Exception:
                pass
            await request_queue.mark_done(job_c)
            return {"expected_total_videos": int(expected2), "job_id": job_c}
        finally:
            try:
                if cookies_path and os.path.exists(cookies_path):
                    os.remove(cookies_path)
            except Exception:
                pass

    except HTTPException:
        raise
    except Exception as e:
        logger.warning(f"è·å–åˆé›†æ€»æ•°å¤±è´¥: {e}")
        # å¤–å±‚å…œåº•å¼‚å¸¸ï¼Œæ­¤æ—¶å†…éƒ¨å·²å°½åŠ›æ ‡è®° job_nc/job_c çš„çŠ¶æ€
        raise HTTPException(status_code=500, detail="è·å–åˆé›†æ€»æ•°å¤±è´¥")

@app.get("/api/subscriptions/{subscription_id}")
async def get_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–å•ä¸ªè®¢é˜…è¯¦æƒ…"""
    subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")

    # Calculate statistics
    total_videos = db.query(Video).filter(Video.subscription_id == subscription.id).count()
    downloaded_videos = db.query(Video).filter(
        Video.subscription_id == subscription.id,
        Video.video_path.isnot(None)
    ).count()
    pending_videos = max(0, total_videos - downloaded_videos)

    return {
        "id": subscription.id,
        "name": subscription.name,
        "type": subscription.type,
        "url": subscription.url,
        "uploader_id": subscription.uploader_id,
        "keyword": subscription.keyword,
        "specific_urls": subscription.specific_urls,
        "date_after": subscription.date_after.isoformat() if subscription.date_after else None,
        "date_before": subscription.date_before.isoformat() if subscription.date_before else None,
        "min_likes": subscription.min_likes,
        "min_favorites": subscription.min_favorites,
        "min_views": subscription.min_views,
        "total_videos": total_videos,
        "downloaded_videos": downloaded_videos,
        "pending_videos": pending_videos,
        "is_active": subscription.is_active,
        "last_check": subscription.last_check.isoformat() if subscription.last_check else None,
        "created_at": subscription.created_at.isoformat() if subscription.created_at else None,
        "updated_at": subscription.updated_at.isoformat() if subscription.updated_at else None
    }

@app.put("/api/subscriptions/{subscription_id}")
async def update_subscription(
    subscription_id: int,
    subscription: SubscriptionUpdate,
    db: Session = Depends(get_db)
):
    """æ›´æ–°è®¢é˜…"""
    db_subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not db_subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    
    # æ›´æ–°å­—æ®µï¼ˆå…¼å®¹ active -> is_activeï¼‰
    data = subscription.dict(exclude_unset=True)
    # æå–å¹¶è§„èŒƒ is_active
    is_active_value = data.pop("is_active", None)
    if is_active_value is None and "active" in data:
        is_active_value = data.pop("active")

    # è®¾ç½®å…¶ä½™å­—æ®µ
    for field, value in data.items():
        setattr(db_subscription, field, value)

    # å•ç‹¬å¤„ç† is_active
    if is_active_value is not None:
        db_subscription.is_active = is_active_value
    
    db_subscription.updated_at = datetime.now()
    db.commit()
    
    return {"message": "è®¢é˜…æ›´æ–°æˆåŠŸ"}

@app.delete("/api/subscriptions/{subscription_id}")
async def delete_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤è®¢é˜…"""
    db_subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not db_subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    
    db.delete(db_subscription)
    db.commit()
    
    return {"message": "è®¢é˜…åˆ é™¤æˆåŠŸ"}

@app.get("/api/subscriptions/{subscription_id}/pending")
async def get_subscription_pending(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–æŒ‡å®šè®¢é˜…çš„å¾…ä¸‹è½½è§†é¢‘åˆ—è¡¨ï¼ˆè¿œç«¯-æœ¬åœ°å·®é›†ï¼‰ï¼Œä¸è§¦å‘ä¸‹è½½ã€‚
    ä»…å¯¹ type=collection ä¸”å­˜åœ¨ url çš„è®¢é˜…æœ‰æ•ˆã€‚
    """
    sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not sub:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    if sub.type != "collection" or not sub.url:
        raise HTTPException(status_code=400, detail="è¯¥è®¢é˜…ä¸æ˜¯åˆé›†æˆ–ç¼ºå°‘URL")

    try:
        data = await downloader.compute_pending_list(subscription_id, db)
        return data
    except HTTPException:
        raise
    except Exception as e:
        logger.warning(f"è·å–å¾…ä¸‹è½½åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–å¾…ä¸‹è½½åˆ—è¡¨å¤±è´¥")

@app.post("/api/subscriptions/parse-collection")
async def parse_collection_info(request: dict, db: Session = Depends(get_db)):
    """è§£æåˆé›†URLï¼Œè‡ªåŠ¨è¯†åˆ«åˆé›†åç§°"""
    url = request.get('url')
    if not url:
        raise HTTPException(status_code=400, detail="URLä¸èƒ½ä¸ºç©º")
    
    try:
        import json as json_lib
        import tempfile, os

        async def run_parse(cookies_path: Optional[str], requires_cookie: bool) -> Optional[str]:
            # A. åˆé›†å±‚
            pl_cmd = [
                'yt-dlp',
                '--flat-playlist',
                '--dump-single-json',
                '--no-download',
                '--user-agent', get_user_agent(requires_cookie),
                url
            ]
            if cookies_path:
                pl_cmd += ['--cookies', cookies_path]
            async with yt_dlp_semaphore:
                pl_proc = await asyncio.create_subprocess_exec(
                    *pl_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                pl_stdout, pl_stderr = await pl_proc.communicate()
            if pl_proc.returncode == 0:
                try:
                    data = json_lib.loads(pl_stdout.decode('utf-8', errors='ignore') or '{}')
                except Exception:
                    data = {}
                uploader = ''
                playlist_title = ''
                if isinstance(data, dict):
                    playlist_title = (data.get('title') or data.get('playlist_title') or '').strip()
                    uploader = (data.get('uploader') or data.get('channel') or '').strip()
                name = None
                if uploader and playlist_title:
                    if playlist_title.startswith(uploader) or uploader in playlist_title:
                        name = playlist_title
                    else:
                        name = f"{uploader}ï¼š{playlist_title}"
                elif playlist_title:
                    name = playlist_title
                elif uploader:
                    name = f"{uploader}çš„åˆé›†"
                if name:
                    return name

            # B. é¦–æ¡è§†é¢‘
            fe_cmd = [
                'yt-dlp',
                '--dump-json',
                '--playlist-items', '1',
                '--no-download',
                '--user-agent', get_user_agent(requires_cookie),
                url
            ]
            if cookies_path:
                fe_cmd += ['--cookies', cookies_path]
            async with yt_dlp_semaphore:
                fe_proc = await asyncio.create_subprocess_exec(
                    *fe_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                fe_stdout, fe_stderr = await fe_proc.communicate()
            if fe_proc.returncode == 0:
                for line in fe_stdout.decode('utf-8', errors='ignore').strip().split('\n'):
                    if not line.strip():
                        continue
                    try:
                        info = json_lib.loads(line)
                        uploader = (info.get('uploader') or '').strip()
                        playlist_title = (info.get('playlist_title') or '').strip()
                        title = (info.get('title') or '').strip()
                        base_title = playlist_title or title
                        name = None
                        if uploader and base_title:
                            if base_title.startswith(uploader) or uploader in base_title:
                                name = base_title
                            else:
                                name = f"{uploader}ï¼š{base_title}"
                        elif base_title:
                            name = base_title
                        elif uploader:
                            name = f"{uploader}çš„åˆé›†"
                        if name:
                            return name
                    except Exception:
                        continue
            return None

        # ç¬¬ä¸€é˜¶æ®µï¼šæ—  Cookie è§£æ
        job_nc = await request_queue.enqueue(job_type="parse", subscription_id=None, requires_cookie=False)
        await request_queue.mark_running(job_nc)
        name_nc = await run_parse(None, requires_cookie=False)
        if name_nc:
            await request_queue.mark_done(job_nc)
            return {"name": name_nc}
        else:
            await request_queue.mark_failed(job_nc, "need_cookie_fallback")

        # ç¬¬äºŒé˜¶æ®µï¼šCookie é€šé“ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        cookie = cookie_manager.get_available_cookie(db)
        if not cookie:
            return {"error": "éœ€è¦Cookieä½†æ²¡æœ‰å¯ç”¨Cookie"}
        fd, cookies_path = tempfile.mkstemp(prefix='cookies_', suffix='.txt')
        os.close(fd)
        with open(cookies_path, 'w', encoding='utf-8') as cf:
            cf.write("# Netscape HTTP Cookie File\n")
            cf.write("# This file was generated by bili_curator V6\n\n")
            cf.writelines([
                f".bilibili.com\tTRUE\t/\tFALSE\t0\tSESSDATA\t{cookie.sessdata}\n",
                f".bilibili.com\tTRUE\t/\tFALSE\t0\tbili_jct\t{cookie.bili_jct}\n",
                f".bilibili.com\tTRUE\t/\tFALSE\t0\tDedeUserID\t{cookie.dedeuserid}\n",
            ])
        job_c = await request_queue.enqueue(job_type="parse", subscription_id=None, requires_cookie=True, priority=0)
        await request_queue.mark_running(job_c)
        try:
            name_c = await run_parse(cookies_path, requires_cookie=True)
            if name_c:
                try:
                    cookie_manager.update_cookie_usage(db, cookie.id)
                except Exception:
                    pass
                await request_queue.mark_done(job_c)
                return {"name": name_c}
            else:
                await request_queue.mark_failed(job_c, "parse_failed")
                return {"error": "è§£æå¤±è´¥"}
        finally:
            try:
                if cookies_path and os.path.exists(cookies_path):
                    os.remove(cookies_path)
            except Exception:
                pass
    except Exception as e:
        # å†…éƒ¨å·²å¯¹ job_nc/job_c åšçŠ¶æ€æ ‡è®°ï¼Œè¿™é‡Œä»…è¿”å›é”™è¯¯
        return {"error": f"è§£æåˆé›†ä¿¡æ¯å¤±è´¥: {str(e)}"}

@app.post("/api/subscriptions/{subscription_id}/download")
async def start_download(subscription_id: int, db: Session = Depends(get_db)):
    """æ‰‹åŠ¨å¯åŠ¨ä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    subscription = db.query(Subscription).filter(Subscription.id == subscription_id).first()
    if not subscription:
        raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
    
    try:
        # ä½¿ç”¨å¢å¼ºä»»åŠ¡ç®¡ç†å™¨å¯åŠ¨ä¸‹è½½
        task_id = await enhanced_task_manager.start_subscription_download(subscription_id)
        return {"message": "ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/tasks/{task_id}/pause")
async def pause_task(task_id: str):
    """æš‚åœä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    success = await enhanced_task_manager.pause_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•æš‚åœ")
    
    return {"message": "ä»»åŠ¡å·²æš‚åœ"}

@app.post("/api/tasks/{task_id}/resume")
async def resume_task(task_id: str):
    """æ¢å¤ä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    success = await enhanced_task_manager.resume_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•æ¢å¤")
    
    return {"message": "ä»»åŠ¡å·²æ¢å¤"}

@app.post("/api/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆä¸‹è½½ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    success = await enhanced_task_manager.cancel_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•å–æ¶ˆ")
    
    return {"message": "ä»»åŠ¡å·²å–æ¶ˆ"}

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    from .task_manager import enhanced_task_manager
    
    task_status = enhanced_task_manager.get_task_status(task_id)
    if not task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return task_status

@app.get("/api/tasks")
async def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    from .task_manager import enhanced_task_manager
    
    return enhanced_task_manager.get_all_tasks()

# â€”â€” å…¨å±€è¯·æ±‚é˜Ÿåˆ—åªè¯»æ¥å£ â€”â€”
@app.get("/api/requests")
async def list_requests():
    items = request_queue.list()
    return {"count": len(items), "items": items}

@app.get("/api/requests/{job_id}")
async def get_request(job_id: str):
    item = request_queue.get(job_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¯·æ±‚ä¸å­˜åœ¨")
    return item

# â€”â€” å…¨å±€è¯·æ±‚é˜Ÿåˆ—ç®¡ç†ä¸å¯è§‚æµ‹æ¥å£ â€”â€”
@app.get("/api/queue/stats")
def get_queue_stats():
    try:
        return request_queue.stats()
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/queue/pause")
async def pause_queue(scope: str = "all"):
    try:
        await request_queue.pause(scope)
        return {"ok": True, "scope": scope, "stats": request_queue.stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/queue/resume")
async def resume_queue(scope: str = "all"):
    try:
        await request_queue.resume(scope)
        return {"ok": True, "scope": scope, "stats": request_queue.stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/requests/{job_id}/cancel")
async def cancel_request(job_id: str, reason: str = ""):
    ok = await request_queue.cancel(job_id, reason)
    if not ok:
        raise HTTPException(status_code=404, detail="not_found")
    return {"ok": True, "job": request_queue.get(job_id)}

@app.post("/api/requests/{job_id}/prioritize")
async def prioritize_request(job_id: str, priority: int = 0):
    ok = await request_queue.prioritize(job_id, priority)
    if not ok:
        raise HTTPException(status_code=404, detail="not_found")
    return {"ok": True, "job": request_queue.get(job_id)}

@app.post("/api/queue/capacity")
async def set_queue_capacity(requires_cookie: Optional[int] = None, no_cookie: Optional[int] = None):
    try:
        await request_queue.set_capacity(requires_cookie=requires_cookie, no_cookie=no_cookie)
        return {"ok": True, "stats": request_queue.stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/subscriptions/{subscription_id}/tasks")
async def get_subscription_tasks(subscription_id: int):
    """è·å–æŒ‡å®šè®¢é˜…çš„æ‰€æœ‰ä»»åŠ¡"""
    from .task_manager import enhanced_task_manager
    
    return enhanced_task_manager.get_subscription_tasks(subscription_id)

# ä¸€è‡´æ€§æ£€æŸ¥API
@app.post("/api/system/consistency-check")
async def trigger_consistency_check(db: Session = Depends(get_db)):
    """æ‰‹åŠ¨è§¦å‘ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆåŒæ­¥æ‰§è¡Œï¼Œç›´æ¥è¿”å›ç»Ÿè®¡ç»“æœï¼‰ã€‚
    å‰ç«¯ä¼šç­‰å¾…æœ¬æ¥å£è¿”å›ç»Ÿè®¡æ•°æ®ï¼Œå› æ­¤ä¸å†ä½¿ç”¨åå°ä»»åŠ¡æ–¹å¼ã€‚
    """
    try:
        stats = consistency_checker.check_and_sync(db)
        logger.info(f"æ‰‹åŠ¨è§¦å‘çš„ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ: {stats}")
        # é™„åŠ ä¸€ä¸ªæ—¶é—´æˆ³ï¼Œä¾¿äºå‰ç«¯æ˜¾ç¤º
        stats_with_time = dict(stats)
        stats_with_time["last_check_time"] = datetime.now().isoformat()
        return stats_with_time
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/system/consistency-stats")
async def get_consistency_stats(db: Session = Depends(get_db)):
    """è·å–ä¸€è‡´æ€§ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = consistency_checker.quick_stats(db)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# è§†é¢‘ç®¡ç†API
@app.get("/api/videos")
async def get_videos(
    page: int = 1,
    size: int = 20,
    subscription_id: Optional[int] = None,
    db: Session = Depends(get_db)
):
    """è·å–è§†é¢‘åˆ—è¡¨"""
    query = db.query(Video)
    
    if subscription_id:
        query = query.filter(Video.subscription_id == subscription_id)
    
    total = query.count()
    videos = query.order_by(Video.created_at.desc()).offset((page - 1) * size).limit(size).all()
    
    return {
        "total": total,
        "page": page,
        "size": size,
        "videos": [
            {
                "id": video.id,
                "bilibili_id": video.bilibili_id,
                "title": video.title,
                "uploader": video.uploader,
                "duration": video.duration,
                "upload_date": video.upload_date.isoformat() if video.upload_date else None,
                "video_path": video.video_path,
                "file_path": video.video_path,
                "file_size": video.file_size,
                "downloaded": video.downloaded,
                "subscription_id": video.subscription_id,
                "created_at": video.created_at.isoformat() if video.created_at else None,
                "updated_at": video.updated_at.isoformat() if video.updated_at else None
            }
            for video in videos
        ]
    }

# åª’ä½“ç»Ÿè®¡ä¸è®¢é˜…ç»´åº¦ç»Ÿè®¡
@app.get("/api/media/overview")
async def get_media_overview(scan: bool = False, db: Session = Depends(get_db)):
    """åª’ä½“ç›®å½•æ€»è§ˆç»Ÿè®¡
    - é»˜è®¤åŸºäºæ•°æ®åº“å¿«é€Ÿæ±‡æ€»ï¼ˆæ›´è½»é‡ï¼‰
    - å½“ scan=true æ—¶ï¼Œé¢å¤–æ‰«æ DOWNLOAD_PATH è®¡ç®—å®é™…å ç”¨ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰
    """
    total_videos = db.query(func.count(Video.id)).scalar() or 0
    downloaded_videos = db.query(func.count(Video.id)).filter(Video.video_path.isnot(None)).scalar() or 0
    total_size_db = db.query(func.coalesce(func.sum(Video.file_size), 0)).scalar() or 0

    overview: Dict[str, Any] = {
        "total_videos": total_videos,
        "downloaded_videos": downloaded_videos,
        "total_size": int(total_size_db),
        "scan": False,
    }

    if scan:
        download_root = Path(os.getenv('DOWNLOAD_PATH', '/app/downloads'))
        size_on_disk = 0
        files_on_disk = 0
        if download_root.exists():
            for p in download_root.rglob("*"):
                try:
                    if p.is_file():
                        files_on_disk += 1
                        size_on_disk += p.stat().st_size
                except Exception:
                    # å¿½ç•¥ä¸ªåˆ«æ–‡ä»¶è¯»å–å¼‚å¸¸
                    continue
        overview.update({
            "files_on_disk": files_on_disk,
            "size_on_disk": int(size_on_disk),
            "scan": True,
            "download_path": str(download_root),
        })

    return overview


@app.get("/api/media/subscription-stats")
async def get_subscription_stats(db: Session = Depends(get_db)):
    """æŒ‰è®¢é˜…èšåˆç»Ÿè®¡ï¼ˆæ•°é‡ã€å·²ä¸‹è½½ã€å®¹é‡ã€æœ€è¿‘ä¸Šä¼ ï¼‰- ä½¿ç”¨è¿œç«¯æ€»æ•°å£å¾„"""
    # æœ¬åœ°ç»Ÿè®¡ï¼šå·²ä¸‹è½½æ•°é‡ã€æœ€è¿‘ä¸Šä¼ ï¼ˆå®¹é‡éœ€è¦å•ç‹¬è®¡ç®—ï¼‰
    counts = (
        db.query(
            Video.subscription_id.label('sid'),
            func.count(Video.id).label('local_total'),
            func.sum(case((Video.video_path.isnot(None), 1), else_=0)).label('downloaded'),
            func.max(Video.upload_date).label('latest_upload')
        )
        .group_by(Video.subscription_id)
        .all()
    )

    # è®¢é˜…ä¿¡æ¯
    subs = {s.id: s for s in db.query(Subscription).all()}
    
    # æ„å»ºæœ¬åœ°ç»Ÿè®¡å­—å…¸ï¼ˆä¸åŒ…å«å®¹é‡ï¼Œéœ€è¦å•ç‹¬è®¡ç®—ï¼‰
    local_stats = {}
    for row in counts:
        local_stats[row.sid] = {
            'local_total': int(row.local_total or 0),
            'downloaded': int(row.downloaded or 0),
            'latest_upload': row.latest_upload.isoformat() if row.latest_upload else None,
        }
    
    # å•ç‹¬è®¡ç®—æ¯ä¸ªè®¢é˜…çš„å®¹é‡ï¼ˆä¸ç›®å½•ç»Ÿè®¡é€»è¾‘ä¸€è‡´ï¼‰
    for sid in subs.keys():
        videos = db.query(Video).filter(Video.subscription_id == sid, Video.video_path.isnot(None)).all()
        total_size = 0
        for v in videos:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not v.video_path:
                continue
            try:
                from pathlib import Path
                vp = Path(v.video_path).resolve()
                if not vp.exists():
                    continue
            except Exception:
                continue
            
            # è®¡ç®—å®¹é‡ï¼ˆä¸ç›®å½•ç»Ÿè®¡ä¸€è‡´çš„ä¸‰çº§å›é€€ï¼‰
            size_sum = (v.total_size if getattr(v, 'total_size', None) is not None else None)
            if size_sum is None:
                size_sum = int(v.file_size or 0) + int((getattr(v, 'audio_size', 0) or 0))
            # å¦‚æœæ•°æ®åº“å­—æ®µéƒ½ä¸ºç©ºï¼Œç›´æ¥è¯»å–ç£ç›˜æ–‡ä»¶å¤§å°
            if size_sum == 0 and v.video_path:
                try:
                    actual_size = vp.stat().st_size
                    size_sum = actual_size
                except Exception:
                    size_sum = 0
            total_size += int(size_sum or 0)
        
        if sid in local_stats:
            local_stats[sid]['size'] = total_size
        else:
            local_stats[sid] = {
                'local_total': 0,
                'downloaded': 0,
                'size': total_size,
                'latest_upload': None,
            }

    from .models import Settings
    import json as json_lib

    result = []
    for sid, sub in subs.items():
        local = local_stats.get(sid, {
            'local_total': 0,
            'downloaded': 0,
            'size': 0,
            'latest_upload': None,
        })
        
        # è¯»å–è¿œç«¯æ€»æ•°
        remote_total = None
        try:
            key = f"sync:{sid}:status"
            s = db.query(Settings).filter(Settings.key == key).first()
            if s and s.value:
                data = json_lib.loads(s.value)
                rt = data.get("remote_total")
                if isinstance(rt, int) and rt >= 0:
                    remote_total = rt
        except Exception:
            remote_total = None
        
        # è®¢é˜…ç»Ÿè®¡å£å¾„æ”¹ä¸ºâ€œæœ¬åœ°æœ‰æ–‡ä»¶çš„æ•°é‡â€ï¼Œä¸ç›®å½•ç»Ÿè®¡ä¸€è‡´
        on_disk_total = local['downloaded']
        
        result.append({
            "subscription_id": sid,
            "subscription_name": sub.name,
            "type": sub.type,
            "total_videos": on_disk_total,  # ä¸ç›®å½•ç»Ÿè®¡ä¸€è‡´ï¼šä»…ç»Ÿè®¡æœ‰æ–‡ä»¶çš„æ•°é‡
            "remote_total": remote_total,
            "downloaded_videos": local['downloaded'],
            # å¾…ä¸‹è½½å£å¾„ï¼šè¿œç«¯æœŸæœ› - æœ¬åœ°æœ‰æ–‡ä»¶æ•°
            "pending_videos": max(0, (remote_total or on_disk_total) - on_disk_total),
            "total_size": local['size'],
            "latest_upload": local['latest_upload'],
        })

    # æ’åºï¼šæŒ‰æ€»å¤§å°/æœ‰æ–‡ä»¶æ•°é‡å€’åº
    result.sort(key=lambda x: (x["total_size"], x["total_videos"]), reverse=True)
    return result


@app.get("/api/media/subscriptions/{subscription_id}/videos")
async def get_subscription_videos_detail(
    subscription_id: int,
    page: int = 1,
    size: int = 20,
    include_disk: bool = True,
    db: Session = Depends(get_db)
):
    """è®¢é˜…ä¸‹è§†é¢‘æ˜ç»†ï¼ˆåˆ†é¡µï¼‰
    - include_disk: æ˜¯å¦æ ‡è®° on_diskï¼ˆåŸºäº video_path å­˜åœ¨æ€§ï¼‰
    """
    query = db.query(Video).filter(Video.subscription_id == subscription_id)
    total = query.count()
    items = (
        query.order_by(Video.created_at.desc())
        .offset((page - 1) * size)
        .limit(size)
        .all()
    )

    def to_item(v: Video) -> Dict[str, Any]:
        on_disk = None
        if include_disk:
            try:
                on_disk = bool(v.video_path and os.path.exists(v.video_path))
            except Exception:
                on_disk = False
        return {
            "id": v.id,
            "bilibili_id": v.bilibili_id,
            "title": v.title,
            "uploader": v.uploader,
            "duration": v.duration,
            "upload_date": v.upload_date.isoformat() if v.upload_date else None,
            "video_path": v.video_path,
            "file_size": v.file_size,
            "downloaded": v.downloaded,
            "subscription_id": v.subscription_id,
            "created_at": v.created_at.isoformat() if v.created_at else None,
            "updated_at": v.updated_at.isoformat() if v.updated_at else None,
            "on_disk": on_disk,
        }

    return {
        "total": total,
        "page": page,
        "size": size,
        "videos": [to_item(v) for v in items],
    }


# â€”â€” æŒ‰ä¸‹è½½ç›®å½•èšåˆç»Ÿè®¡ä¸ç›®å½•è§†é¢‘åˆ†é¡µ â€”â€”
@app.get("/api/media/directories")
async def get_directory_stats(db: Session = Depends(get_db)):
    """æŒ‰ä¸‹è½½æ ¹ç›®å½•ä¸‹çš„ä¸€çº§ç›®å½•èšåˆç»Ÿè®¡ï¼ˆæœ¬åœ°ä¼˜å…ˆã€ä»…ç»Ÿè®¡çœŸå®å­˜åœ¨æ–‡ä»¶ï¼‰
    - ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ DOWNLOAD_PATHï¼›è‹¥ä¸å®é™…ä¸ç¬¦ï¼Œåˆ™æ ¹æ® DB ä¸­ video_path è‡ªåŠ¨æ¢æµ‹å…¬å…±æ ¹ç›®å½•
    - åˆ†ç»„å£å¾„ï¼šæ ¹ç›®å½•ä¸‹çš„ä¸€çº§ç›®å½•åï¼›æ ¹ä¸‹ç›´å­˜æ–‡ä»¶è®¡å…¥ "_root"ï¼›ä¸åœ¨æ ¹å†…çš„è®¡å…¥ "_others"
    - ç»Ÿè®¡å­—æ®µï¼štotal_videosï¼ˆå­˜åœ¨æ–‡ä»¶æ•°ï¼‰ã€downloaded_videosï¼ˆåŒ total_videosï¼‰ã€total_size
    """

    def _detect_download_root(paths: List[str]) -> Path:
        # ä¼˜å…ˆç¯å¢ƒå˜é‡
        env_root = Path(os.getenv('DOWNLOAD_PATH', '/app/downloads')).resolve()
        try:
            if env_root.exists():
                return env_root
        except Exception:
            pass
        # è‡ªåŠ¨æ¢æµ‹ï¼šå–å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„ï¼Œæ±‚å…¬å…±è·¯å¾„
        existing = []
        for p in paths:
            try:
                if p and os.path.exists(p):
                    existing.append(str(Path(p).resolve()))
            except Exception:
                continue
        if not existing:
            # å›é€€åˆ° env_root å³ä¾¿ä¸å­˜åœ¨
            return env_root
        try:
            common = os.path.commonpath(existing)
            return Path(common)
        except Exception:
            return env_root

    # ä»…è€ƒè™‘æœ‰è·¯å¾„çš„è®°å½•
    videos = db.query(Video).filter(Video.video_path.isnot(None)).all()
    all_paths = [v.video_path for v in videos if v.video_path]
    download_root = _detect_download_root(all_paths)

    # ä¸ downloader._create_subscription_directory å£å¾„ä¸€è‡´çš„ç›®å½•åæ¨å¯¼
    def _sanitize_filename(filename: str) -> str:
        import re
        illegal_chars = r'[<>:"/\\|?*]'
        filename = re.sub(illegal_chars, '_', filename or '')
        filename = filename.strip(' .')
        if len(filename) > 100:
            filename = filename[:100]
        return filename or 'untitled'

    def _expected_dir_for_sub(s: Subscription) -> str:
        if s.type == 'collection':
            base = s.name or ''
            d = _sanitize_filename(base)
            if not d:
                d = _sanitize_filename(f"åˆé›†è®¢é˜…_{s.id}")
            return d
        elif s.type == 'keyword':
            base = s.keyword or s.name or 'å…³é”®è¯è®¢é˜…'
            return _sanitize_filename(f"å…³é”®è¯_{base}")
        elif s.type == 'uploader':
            base = s.name or getattr(s, 'uploader_id', None) or 'UPä¸»è®¢é˜…'
            return _sanitize_filename(f"UPä¸»_{base}")
        else:
            d = _sanitize_filename(s.name or '')
            if not d:
                d = _sanitize_filename(f"è®¢é˜…_{s.id}")
            return d

    subs = db.query(Subscription).all()
    subs_by_dir: Dict[str, Subscription] = { _expected_dir_for_sub(s): s for s in subs }

    stats: Dict[str, Dict[str, Any]] = {}
    for v in videos:
        v_path = (v.video_path or '').strip()
        if not v_path:
            continue
        # ä»…ç»Ÿè®¡ç£ç›˜ä¸ŠçœŸå®å­˜åœ¨çš„æ–‡ä»¶
        try:
            vp = Path(v_path).resolve()
            if not vp.exists():
                continue
        except Exception:
            continue
        # åˆ†ç»„é”®ï¼šä¸¥æ ¼æŒ‰ä¸‹è½½æ ¹ä¸‹çš„ä¸€çº§ç›®å½•ï¼ˆæ¢å¤ä¸å†å²ä¸€è‡´çš„å£å¾„ï¼‰
        try:
            rel = vp.relative_to(download_root)
            first = rel.parts[0] if len(rel.parts) > 0 else ""
        except Exception:
            first = "_others"
        if not first:
            first = "_root"
        s = stats.setdefault(first, {
            "dir": first,
            "subscription_id": (subs_by_dir.get(first).id if subs_by_dir.get(first) else None),
            "display_name": None,  # è‹¥èƒ½è§£æåˆ°è®¢é˜…ï¼Œåˆ™ç”¨å½“å‰è®¢é˜…åä½œä¸ºå±•ç¤ºå
            "_sub_counts": {},     # å†…éƒ¨ï¼šç»Ÿè®¡è¯¥ç›®å½•ä¸‹å„è®¢é˜…å‡ºç°æ¬¡æ•°
            "total_videos": 0,
            "downloaded_videos": 0,
            "total_size": 0,
        })
        s["total_videos"] += 1
        s["downloaded_videos"] += 1  # ä»…ç»Ÿè®¡å­˜åœ¨æ–‡ä»¶ï¼Œç­‰åŒå·²ä¸‹è½½
        try:
            sidv = int(v.subscription_id) if v.subscription_id is not None else None
        except Exception:
            sidv = None
        if sidv is not None:
            s["_sub_counts"][sidv] = int(s["_sub_counts"].get(sidv, 0)) + 1
        # ä¼˜å…ˆä½¿ç”¨ total_sizeï¼›å¦åˆ™å›é€€åˆ° file_size + audio_sizeï¼›æœ€åå›é€€åˆ°å®é™…æ–‡ä»¶å¤§å°
        size_sum = (v.total_size if getattr(v, 'total_size', None) is not None else None)
        if size_sum is None:
            size_sum = int(v.file_size or 0) + int((getattr(v, 'audio_size', 0) or 0))
        # å¦‚æœæ•°æ®åº“å­—æ®µéƒ½ä¸ºç©ºï¼Œç›´æ¥è¯»å–ç£ç›˜æ–‡ä»¶å¤§å°
        if size_sum == 0 and v.video_path:
            try:
                actual_size = vp.stat().st_size
                size_sum = actual_size
            except Exception:
                size_sum = 0
        s["total_size"] += int(size_sum or 0)

    # äº‹åç¡®å®šå±•ç¤ºè®¢é˜…ï¼šè‹¥ç›®å½•åæ— æ³•æ˜ å°„è®¢é˜…ï¼Œåˆ™å–è¯¥ç›®å½•å†…æœ€å¤šçš„è®¢é˜…ID
    subs_by_id: Dict[int, Subscription] = {s.id: s for s in subs}
    for d, s in stats.items():
        if not s.get("subscription_id"):
            sc = s.get("_sub_counts") or {}
            if sc:
                # é€‰æ‹©å‡ºç°æœ€å¤šçš„è®¢é˜…ID
                sid_major = max(sc.items(), key=lambda kv: kv[1])[0]
                s["subscription_id"] = sid_major
        # è®¾ç½®å±•ç¤ºå
        sid_cur = s.get("subscription_id")
        if sid_cur is not None and sid_cur in subs_by_id:
            s["display_name"] = subs_by_id[sid_cur].name or s.get("dir")
        # æ¸…ç†å†…éƒ¨å­—æ®µ
        if "_sub_counts" in s:
            del s["_sub_counts"]

    # è½¬åˆ—è¡¨ï¼ŒæŒ‰å¤§å°/æ•°é‡å€’åº
    result = list(stats.values())
    result.sort(key=lambda x: (x["total_size"], x["downloaded_videos"], x["total_videos"]), reverse=True)
    return {
        "download_path": str(download_root),
        "items": result,
        "total_dirs": len(result),
    }


# â€”â€” å›å¡«ä¸æ ¡å‡†ï¼šåˆ·æ–°è§†é¢‘/éŸ³é¢‘å¤§å°ï¼Œå†™å› DB â€”â€”
@app.post("/api/media/refresh-sizes")
async def refresh_media_sizes(background: BackgroundTasks):
    """åå°å›å¡«ç£ç›˜å¤§å°ï¼ˆè§†é¢‘+éŸ³é¢‘ï¼‰ï¼Œå†™å›åˆ° videos.file_size / audio_size / total_sizeã€‚
    - éé˜»å¡ï¼šå¯åŠ¨åå°ä»»åŠ¡åç«‹å³è¿”å›ã€‚
    - æ‰«æç­–ç•¥ï¼šä»…å¤„ç†æœ‰ video_path çš„è®°å½•ï¼›éŸ³é¢‘å°è¯•åŒ¹é…åŒå .m4aã€‚
    """

    def _find_audio_path(video_path: str) -> Optional[str]:
        try:
            p = Path(video_path)
            stem = p.with_suffix("").name
            candidate = p.with_name(stem + ".m4a")
            if candidate.exists():
                return str(candidate)
        except Exception:
            return None
        return None

    def _worker():
        from .models import db as _db, Video as _Video
        session = _db.get_session()
        updated = 0
        try:
            q = session.query(_Video).filter(_Video.video_path.isnot(None))
            for v in q.yield_per(200):
                v_path = (v.video_path or '').strip()
                if not v_path:
                    continue
                video_size = None
                audio_size = None
                try:
                    if os.path.exists(v_path):
                        video_size = os.path.getsize(v_path)
                except Exception:
                    video_size = None
                # ä»…å½“å®¹å™¨ç¼ºå°‘éŸ³è½¨æ—¶ï¼Œæ‰ç»Ÿè®¡æ—è·¯ .m4a çš„ä½“ç§¯ï¼Œé¿å…åŒé‡ç»Ÿè®¡
                def _has_audio_track(pth: str) -> bool:
                    try:
                        import subprocess
                        proc = subprocess.run(['ffprobe', '-v', 'error', '-select_streams', 'a', '-show_entries', 'stream=index', '-of', 'csv=p=0', pth],
                                              stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                        return any(line.strip() for line in (proc.stdout or '').splitlines())
                    except Exception:
                        # æ¢æµ‹å¤±è´¥æ—¶ï¼Œä¿å®ˆè®¤ä¸ºâ€œæœ‰éŸ³è½¨â€ï¼Œä»¥é¿å…æŠŠæ®‹ç•™ m4a è®¡å…¥å¯¼è‡´å®¹é‡è™šé«˜
                        return True

                try:
                    # åªæœ‰å½“è§†é¢‘å­˜åœ¨è€Œä¸”â€œæ— éŸ³è½¨â€æ—¶ï¼Œå°è¯•ç»Ÿè®¡åŒå m4a
                    if v_path and os.path.exists(v_path) and (not _has_audio_track(v_path)):
                        a_path = _find_audio_path(v_path)
                        if a_path and os.path.exists(a_path):
                            audio_size = os.path.getsize(a_path)
                        else:
                            audio_size = 0
                    else:
                        # æœ‰éŸ³è½¨æˆ–è§†é¢‘ä¸å­˜åœ¨ï¼šä¸è®¡æ—è·¯éŸ³é¢‘
                        audio_size = 0 if video_size is not None else None
                except Exception:
                    audio_size = 0 if video_size is not None else None

                # è‹¥ä¸¤è€…çš†ä¸ºç©ºï¼Œè·³è¿‡
                if video_size is None and audio_size is None:
                    continue

                new_file_size = int(video_size) if video_size is not None else (v.file_size or 0)
                new_audio_size = int(audio_size) if audio_size is not None else (getattr(v, 'audio_size', 0) or 0)
                new_total = int(new_file_size) + int(new_audio_size)

                changed = False
                if v.file_size != new_file_size:
                    v.file_size = new_file_size
                    changed = True
                # å…¼å®¹æ—§åº“æ— å­—æ®µçš„æƒ…å†µï¼šgetattr/setattr
                if getattr(v, 'audio_size', None) != new_audio_size:
                    try:
                        setattr(v, 'audio_size', new_audio_size)
                        changed = True
                    except Exception:
                        pass
                if getattr(v, 'total_size', None) != new_total:
                    try:
                        setattr(v, 'total_size', new_total)
                        changed = True
                    except Exception:
                        pass
                if changed:
                    updated += 1
                    # åˆ†æ‰¹æäº¤ï¼Œé™ä½äº‹åŠ¡ä½“ç§¯
                    if updated % 200 == 0:
                        try:
                            session.commit()
                        except Exception:
                            session.rollback()
            try:
                session.commit()
            except Exception:
                session.rollback()
        finally:
            session.close()

    # åå°æ‰§è¡Œ
    background.add_task(_worker)
    return {"message": "åˆ·æ–°ä»»åŠ¡å·²å¯åŠ¨", "status": "started"}


@app.get("/api/media/directory-videos")
async def get_directory_videos(dir: str = None, sid: int = None, page: int = 1, size: int = 20, db: Session = Depends(get_db)):
    """è·å–æŸä¸€çº§ç›®å½•ä¸‹çš„å…¨éƒ¨è§†é¢‘ï¼ˆå«å­ç›®å½•ï¼‰ï¼Œåˆ†é¡µè¿”å›ï¼ˆåŸºäºç»Ÿä¸€æ ¹ç›®å½•æ¢æµ‹ï¼‰
    - å‚æ•° dir: ç”± get_directory_stats è¿”å›çš„ä¸€çº§ç›®å½•å
    - ä»…è¿”å›ç£ç›˜çœŸå®å­˜åœ¨çš„æ–‡ä»¶å¯¹åº”çš„è®°å½•
    """
    # å¦‚æœæä¾›äº†è®¢é˜…IDï¼Œåˆ™ç”¨å…¶è®¢é˜…åä½œä¸ºä¸€çº§ç›®å½•å
    if sid is not None:
        sub = db.query(Subscription).filter(Subscription.id == sid).first()
        if not sub or not sub.name:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„è®¢é˜…IDæˆ–è®¢é˜…åä¸ºç©º")
        dir = sub.name

    if not dir:
        raise HTTPException(status_code=400, detail="dir ä¸èƒ½ä¸ºç©º")

    # é‡ç”¨æ ¹ç›®å½•æ¢æµ‹é€»è¾‘ï¼Œç¡®ä¿ä¸ç›®å½•èšåˆä¸€è‡´
    videos_all = db.query(Video).filter(Video.video_path.isnot(None)).all()
    paths_all = [v.video_path for v in videos_all if v.video_path]
    def _detect_download_root(paths: List[str]) -> Path:
        env_root = Path(os.getenv('DOWNLOAD_PATH', '/app/downloads')).resolve()
        try:
            if env_root.exists():
                return env_root
        except Exception:
            pass
        existing = []
        for p in paths:
            try:
                if p and os.path.exists(p):
                    existing.append(str(Path(p).resolve()))
            except Exception:
                continue
        if not existing:
            return env_root
        try:
            common = os.path.commonpath(existing)
            return Path(common)
        except Exception:
            return env_root

    download_root = _detect_download_root(paths_all)
    base = (download_root / dir).resolve()
    try:
        base.relative_to(download_root)
    except Exception:
        raise HTTPException(status_code=400, detail="éæ³•ç›®å½•")

    # SQLite å‰ç¼€åŒ¹é…
    prefix = str(base) + os.sep
    query = db.query(Video).filter(Video.video_path.isnot(None), Video.video_path.like(f"{prefix}%"))
    total = query.count()
    items = (
        query.order_by(Video.created_at.desc())
        .offset((page - 1) * size)
        .limit(size)
        .all()
    )

    def to_item(v: Video) -> Dict[str, Any]:
        return {
            "id": v.id,
            "bilibili_id": v.bilibili_id,
            "title": v.title,
            "uploader": v.uploader,
            "duration": v.duration,
            "upload_date": v.upload_date.isoformat() if v.upload_date else None,
            "video_path": v.video_path,
            "file_size": v.file_size,
            "downloaded": v.downloaded,
            "subscription_id": v.subscription_id,
            "created_at": v.created_at.isoformat() if v.created_at else None,
            "updated_at": v.updated_at.isoformat() if v.updated_at else None,
        }

    return {
        "dir": dir,
        "sid": sid,
        "download_path": str(download_root),
        "total": total,
        "page": page,
        "size": size,
        "videos": [to_item(v) for v in items],
    }

@app.delete("/api/videos/{video_id}")
async def delete_video(video_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤è§†é¢‘è®°å½•"""
    video = db.query(Video).filter(Video.id == video_id).first()
    if not video:
        raise HTTPException(status_code=404, detail="è§†é¢‘ä¸å­˜åœ¨")
    
    db.delete(video)
    db.commit()
    
    return {"message": "è§†é¢‘è®°å½•åˆ é™¤æˆåŠŸ"}

# è‡ªåŠ¨å¯¼å…¥ä¸å…³è”API
@app.post("/api/auto-import/scan")
async def auto_import_scan():
    """æ‰«æä¸‹è½½ç›®å½•å¹¶å°†æœªå…¥åº“çš„è§†é¢‘å¯¼å…¥æ•°æ®åº“"""
    try:
        from .auto_import import auto_import_service
        result = auto_import_service.scan_and_import()
        return {"message": "æ‰«æå®Œæˆ", **result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/auto-import/associate")
async def auto_import_associate():
    """å°†å·²å…¥åº“è§†é¢‘æŒ‰è®¢é˜…è§„åˆ™è‡ªåŠ¨å…³è”ï¼Œåˆ·æ–°ç»Ÿè®¡"""
    try:
        from .auto_import import auto_import_service
        result = auto_import_service.auto_associate_subscriptions()
        return {"message": "å…³è”å®Œæˆ", **result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/subscriptions/{subscription_id}/associate")
async def associate_single_subscription(subscription_id: int, db: Session = Depends(get_db)):
    """ä»…å¯¹æŒ‡å®šè®¢é˜…æ‰§è¡Œè‡ªåŠ¨å…³è”å¹¶è¿”å›è¯¥è®¢é˜…çš„æœ€æ–°ç»Ÿè®¡"""
    try:
        sub = db.query(Subscription).filter(Subscription.id == subscription_id).first()
        if not sub:
            raise HTTPException(status_code=404, detail="è®¢é˜…ä¸å­˜åœ¨")
        from .auto_import import auto_import_service
        matches = auto_import_service._find_matching_videos(sub, db)
        for v in matches:
            if not v.subscription_id:
                v.subscription_id = sub.id
        # åˆ·æ–°ç»Ÿè®¡
        total_videos = db.query(Video).filter(Video.subscription_id == sub.id).count()
        downloaded_videos = db.query(Video).filter(
            Video.subscription_id == sub.id,
            Video.video_path.isnot(None)
        ).count()
        sub.total_videos = total_videos
        sub.downloaded_videos = downloaded_videos
        sub.updated_at = datetime.now()
        db.commit()
        pending_videos = max(0, total_videos - downloaded_videos)
        return {
            "message": "å…³è”å®Œæˆ",
            "id": sub.id,
            "total_videos": total_videos,
            "downloaded_videos": downloaded_videos,
            "pending_videos": pending_videos,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ä»»åŠ¡ç®¡ç†API
@app.get("/api/tasks")
async def get_tasks(db: Session = Depends(get_db)):
    """è·å–ä¸‹è½½ä»»åŠ¡åˆ—è¡¨"""
    tasks = db.query(DownloadTask).order_by(DownloadTask.created_at.desc()).limit(50).all()
    
    return [
        {
            "id": task.id,
            "bilibili_id": task.bilibili_id,
            "subscription_id": task.subscription_id,
            "status": task.status,
            "progress": task.progress,
            "error_message": task.error_message,
            "started_at": task.started_at.isoformat() if task.started_at else None,
            "completed_at": task.completed_at.isoformat() if task.completed_at else None,
            "created_at": task.created_at.isoformat() if task.created_at else None
        }
        for task in tasks
    ]

@app.get("/api/tasks/{task_id}/status")
async def get_task_status(task_id: str):
    """è·å–æ‰‹åŠ¨ä»»åŠ¡çŠ¶æ€"""
    status = task_manager.get_task_status(task_id)
    if status['status'] == 'not_found':
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return status

@app.post("/api/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆæ‰‹åŠ¨ä»»åŠ¡"""
    success = task_manager.cancel_task(task_id)
    if not success:
        raise HTTPException(status_code=400, detail="æ— æ³•å–æ¶ˆä»»åŠ¡")
    
    return {"message": "ä»»åŠ¡å·²å–æ¶ˆ"}

@app.post("/api/tasks/clear-completed")
async def clear_completed_tasks(db: Session = Depends(get_db)):
    """æ¸…ç†å·²å®Œæˆ/å¤±è´¥/å–æ¶ˆçš„ä»»åŠ¡è®°å½•ï¼Œå¹¶æ¸…ç†å†…å­˜ä¸­è¿‡æœŸä»»åŠ¡"""
    try:
        # ç»Ÿè®¡æ•°é‡
        from sqlalchemy import or_
        q = db.query(DownloadTask).filter(
            or_(
                DownloadTask.status == 'completed',
                DownloadTask.status == 'failed',
                DownloadTask.status == 'cancelled'
            )
        )
        count = q.count()
        q.delete(synchronize_session=False)
        db.commit()

        # æ¸…ç†å†…å­˜ä»»åŠ¡ï¼ˆç«‹å³æ¸…ç†ï¼‰
        try:
            from .task_manager import enhanced_task_manager
            enhanced_task_manager.cleanup_completed_tasks(hours=0)
        except Exception:
            pass

        return {"message": "æ¸…ç†å®Œæˆ", "cleared": count}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Cookieç®¡ç†API
@app.get("/api/cookies")
async def get_cookies(db: Session = Depends(get_db)):
    """è·å–Cookieåˆ—è¡¨"""
    cookies = db.query(Cookie).order_by(Cookie.created_at.desc()).all()
    
    return [
        {
            "id": cookie.id,
            "name": cookie.name,
            "sessdata": cookie.sessdata,
            "bili_jct": cookie.bili_jct,
            "dedeuserid": cookie.dedeuserid,
            "is_active": cookie.is_active,
            "usage_count": cookie.usage_count,
            "last_used": cookie.last_used.isoformat() if cookie.last_used else None,
            "created_at": cookie.created_at.isoformat() if cookie.created_at else None,
            "updated_at": cookie.updated_at.isoformat() if cookie.updated_at else None
        }
        for cookie in cookies
    ]

@app.get("/api/cookies/{cookie_id}")
async def get_cookie(cookie_id: int, db: Session = Depends(get_db)):
    """è·å–å•ä¸ªCookieè¯¦æƒ…"""
    cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    return {
        "id": cookie.id,
        "name": cookie.name,
        "sessdata": cookie.sessdata,
        "bili_jct": cookie.bili_jct,
        "dedeuserid": cookie.dedeuserid,
        "is_active": cookie.is_active,
        "usage_count": cookie.usage_count,
        "last_used": cookie.last_used.isoformat() if cookie.last_used else None,
        "created_at": cookie.created_at.isoformat() if cookie.created_at else None,
        "updated_at": cookie.updated_at.isoformat() if cookie.updated_at else None
    }

@app.post("/api/cookies")
async def create_cookie(cookie: CookieCreate, db: Session = Depends(get_db)):
    """æ·»åŠ æ–°Cookie"""
    db_cookie = Cookie(
        name=cookie.name,
        sessdata=cookie.sessdata,
        bili_jct=cookie.bili_jct,
        dedeuserid=cookie.dedeuserid,
        created_at=datetime.now()
    )
    
    db.add(db_cookie)
    db.commit()
    db.refresh(db_cookie)
    
    return {"id": db_cookie.id, "message": "Cookieæ·»åŠ æˆåŠŸ"}

@app.put("/api/cookies/{cookie_id}")
async def update_cookie(
    cookie_id: int,
    cookie: CookieUpdate,
    db: Session = Depends(get_db)
):
    """æ›´æ–°Cookie"""
    db_cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not db_cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    # æ›´æ–°å­—æ®µï¼ˆå…¼å®¹ active -> is_activeï¼‰
    data = cookie.dict(exclude_unset=True)
    # æå–å¹¶è§„èŒƒ is_active
    is_active_value = data.pop("is_active", None)
    if is_active_value is None and "active" in data:
        is_active_value = data.pop("active")

    # è®¾ç½®å…¶ä½™å­—æ®µ
    for field, value in data.items():
        setattr(db_cookie, field, value)

    # å•ç‹¬å¤„ç† is_active
    if is_active_value is not None:
        db_cookie.is_active = is_active_value
    
    db_cookie.updated_at = datetime.now()
    db.commit()
    
    return {"message": "Cookieæ›´æ–°æˆåŠŸ"}

@app.delete("/api/cookies/{cookie_id}")
async def delete_cookie(cookie_id: int, db: Session = Depends(get_db)):
    """åˆ é™¤Cookie"""
    db_cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not db_cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    db.delete(db_cookie)
    db.commit()
    
    return {"message": "Cookieåˆ é™¤æˆåŠŸ"}

@app.post("/api/cookies/{cookie_id}/validate")
async def validate_cookie(cookie_id: int, db: Session = Depends(get_db)):
    """éªŒè¯Cookie"""
    db_cookie = db.query(Cookie).filter(Cookie.id == cookie_id).first()
    if not db_cookie:
        raise HTTPException(status_code=404, detail="Cookieä¸å­˜åœ¨")
    
    is_valid = await cookie_manager.validate_cookie(db_cookie)
    
    # é‡æ–°æŸ¥è¯¢Cookieä»¥è·å–æœ€æ–°çŠ¶æ€
    db.refresh(db_cookie)
    
    # æ ¹æ®ç»“æœæ›´æ–°å¤±è´¥è®¡æ•°æˆ–é‡ç½®
    if is_valid:
        try:
            cookie_manager.reset_failures(db, db_cookie.id)
            db.refresh(db_cookie)  # åˆ·æ–°çŠ¶æ€
        except Exception:
            pass
    else:
        try:
            cookie_manager.record_failure(db, db_cookie.id, "éªŒè¯å¤±è´¥")
            db.refresh(db_cookie)  # åˆ·æ–°çŠ¶æ€
        except Exception:
            # è€åº“ä¸æ”¯æŒå¤±è´¥å­—æ®µåˆ™å¯èƒ½å·²è¢«ç›´æ¥ç¦ç”¨
            pass

    # è¯»å–å½“å‰å¤±è´¥è®¡æ•°ï¼ˆè‹¥å­˜åœ¨ï¼‰
    failure_info = {}
    if hasattr(db_cookie, 'failure_count'):
        failure_info = {
            "failure_count": db_cookie.failure_count or 0,
            "last_failure_at": db_cookie.last_failure_at.isoformat() if db_cookie.last_failure_at else None,
            "is_active": db_cookie.is_active,
        }
    
    return {"valid": is_valid and db_cookie.is_active, "message": "éªŒè¯å®Œæˆ", **failure_info}

# ç³»ç»Ÿè®¾ç½®API
@app.get("/api/settings")
async def get_settings(db: Session = Depends(get_db)):
    """è·å–ç³»ç»Ÿè®¾ç½®"""
    settings = db.query(Settings).all()
    
    return {
        setting.key: {
            "value": setting.value,
            "description": setting.description
        }
        for setting in settings
    }

@app.put("/api/settings/{key}")
async def update_setting(key: str, setting: SettingUpdate, db: Session = Depends(get_db)):
    """æ›´æ–°ç³»ç»Ÿè®¾ç½®"""
    db_setting = db.query(Settings).filter(Settings.key == key).first()
    if not db_setting:
        raise HTTPException(status_code=404, detail="è®¾ç½®é¡¹ä¸å­˜åœ¨")
    
    db_setting.value = setting.value
    db_setting.updated_at = datetime.now()
    db.commit()
    
    # å¦‚æœæ˜¯è®¢é˜…æ£€æŸ¥é—´éš”ï¼Œæ›´æ–°è°ƒåº¦å™¨ï¼ˆå…¼å®¹ key: auto_check_interval ä¸ check_intervalï¼‰
    if key in ("auto_check_interval", "check_interval"):
        try:
            interval = int(setting.value)
            scheduler.update_subscription_check_interval(interval)
        except ValueError:
            pass
    
    return {"message": "è®¾ç½®æ›´æ–°æˆåŠŸ"}

# è°ƒåº¦å™¨ç®¡ç†API
@app.get("/api/scheduler/jobs")
async def get_scheduler_jobs():
    """è·å–è°ƒåº¦å™¨ä»»åŠ¡åˆ—è¡¨"""
    return scheduler.get_jobs()

@app.post("/api/scheduler/check-subscriptions")
async def trigger_subscription_check(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘è®¢é˜…æ£€æŸ¥"""
    background_tasks.add_task(scheduler.check_subscriptions)
    return {"message": "è®¢é˜…æ£€æŸ¥å·²å¯åŠ¨"}

@app.post("/api/scheduler/validate-cookies")
async def trigger_cookie_validation(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘CookieéªŒè¯"""
    background_tasks.add_task(scheduler.validate_cookies)
    return {"message": "CookieéªŒè¯å·²å¯åŠ¨"}

# è§†é¢‘æ£€æµ‹æœåŠ¡API
@app.get("/api/video-detection/status")
async def get_video_detection_status():
    """è·å–è§†é¢‘æ£€æµ‹æœåŠ¡çŠ¶æ€"""
    try:
        status = video_detection_service.get_status()
        return {
            "success": True,
            "data": status
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/start")
async def start_video_detection_service():
    """å¯åŠ¨è§†é¢‘æ£€æµ‹æœåŠ¡"""
    try:
        if video_detection_service.is_running:
            return {
                "success": True,
                "message": "è§†é¢‘æ£€æµ‹æœåŠ¡å·²åœ¨è¿è¡Œä¸­"
            }
        
        await video_detection_service.start_service()
        return {
            "success": True,
            "message": "è§†é¢‘æ£€æµ‹æœåŠ¡å¯åŠ¨æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/stop")
async def stop_video_detection_service():
    """åœæ­¢è§†é¢‘æ£€æµ‹æœåŠ¡"""
    try:
        await video_detection_service.stop_service()
        return {
            "success": True,
            "message": "è§†é¢‘æ£€æµ‹æœåŠ¡åœæ­¢æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/scan/full")
async def trigger_full_video_scan(background_tasks: BackgroundTasks):
    """è§¦å‘å®Œæ•´è§†é¢‘æ‰«æ"""
    try:
        # åœ¨åå°æ‰§è¡Œæ‰«æä»»åŠ¡
        background_tasks.add_task(video_detection_service.full_scan)
        
        return {
            "success": True,
            "message": "å®Œæ•´æ‰«æä»»åŠ¡å·²å¯åŠ¨ï¼Œå°†åœ¨åå°æ‰§è¡Œ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/video-detection/scan/incremental")
async def trigger_incremental_video_scan(background_tasks: BackgroundTasks):
    """è§¦å‘å¢é‡è§†é¢‘æ‰«æ"""
    try:
        # åœ¨åå°æ‰§è¡Œå¢é‡æ‰«æ
        background_tasks.add_task(video_detection_service.incremental_scan)
        
        return {
            "success": True,
            "message": "å¢é‡æ‰«æä»»åŠ¡å·²å¯åŠ¨ï¼Œå°†åœ¨åå°æ‰§è¡Œ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/video-detection/config")
async def update_video_detection_config(scan_interval: int = 300):
    """æ›´æ–°è§†é¢‘æ£€æµ‹æœåŠ¡é…ç½®"""
    try:
        if scan_interval < 60:
            raise HTTPException(status_code=400, detail="æ‰«æé—´éš”ä¸èƒ½å°‘äº60ç§’")
        
        video_detection_service.scan_interval = scan_interval
        
        return {
            "success": True,
            "message": f"æ‰«æé—´éš”å·²æ›´æ–°ä¸º{scan_interval}ç§’"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# è®¢é˜…åŒæ­¥çŠ¶æ€ç›¸å…³API
@app.get("/api/subscriptions/{subscription_id}/sync_status")
async def get_sync_status(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–è®¢é˜…åŒæ­¥çŠ¶æ€"""
    try:
        key = f"sync:{subscription_id}:status"
        setting = db.query(Settings).filter(Settings.key == key).first()
        
        if not setting or not setting.value:
            return {
                "subscription_id": subscription_id,
                "status": "idle",
                "started_at": None,
                "updated_at": None,
                "remote_total": 0,
                "existing": 0,
                "pending": 0
            }
        
        try:
            data = json.loads(setting.value)
            return {
                "subscription_id": subscription_id,
                "status": data.get("status", "idle"),
                "started_at": data.get("started_at"),
                "updated_at": data.get("updated_at"),
                "completed_at": data.get("completed_at"),
                "remote_total": data.get("remote_total", 0),
                "existing": data.get("existing", 0),
                "pending": data.get("pending", 0),
                "error": data.get("error")
            }
        except json.JSONDecodeError:
            logger.warning(f"Corrupted sync status JSON for subscription {subscription_id}")
            return {
                "subscription_id": subscription_id,
                "status": "idle",
                "started_at": None,
                "updated_at": None,
                "remote_total": 0,
                "existing": 0,
                "pending": 0
            }
    except Exception as e:
        logger.error(f"Failed to get sync status for subscription {subscription_id}: {e}")
        raise HTTPException(status_code=500, detail="è·å–åŒæ­¥çŠ¶æ€å¤±è´¥")

@app.get("/api/subscriptions/{subscription_id}/sync_trace")
async def get_sync_trace(subscription_id: int, db: Session = Depends(get_db)):
    """è·å–è®¢é˜…åŒæ­¥é“¾è·¯äº‹ä»¶trace"""
    try:
        key = f"sync:{subscription_id}:trace"
        setting = db.query(Settings).filter(Settings.key == key).first()
        
        if not setting or not setting.value:
            return {
                "subscription_id": subscription_id,
                "events": []
            }
        
        try:
            events = json.loads(setting.value)
            if not isinstance(events, list):
                events = []
            return {
                "subscription_id": subscription_id,
                "events": events
            }
        except json.JSONDecodeError:
            logger.warning(f"Corrupted sync trace JSON for subscription {subscription_id}")
            return {
                "subscription_id": subscription_id,
                "events": []
            }
    except Exception as e:
        logger.error(f"Failed to get sync trace for subscription {subscription_id}: {e}")
        raise HTTPException(status_code=500, detail="è·å–åŒæ­¥traceå¤±è´¥")

@app.post("/api/subscriptions/sync_overview")
async def get_sync_overview(request: dict, db: Session = Depends(get_db)):
    """æ‰¹é‡è·å–è®¢é˜…åŒæ­¥çŠ¶æ€æ¦‚è§ˆ"""
    try:
        subscription_ids = request.get('subscription_ids', [])
        
        # é™åˆ¶æ‰¹é‡å¤§å°ï¼Œé˜²æ­¢æ€§èƒ½é—®é¢˜
        if len(subscription_ids) > 100:
            raise HTTPException(status_code=400, detail="æœ€å¤šæ”¯æŒ100ä¸ªè®¢é˜…ID")
        
        if not subscription_ids:
            return {"items": []}
        
        # ä½¿ç”¨ IN æŸ¥è¯¢å‡å°‘æ•°æ®åº“è¯·æ±‚
        keys = [f"sync:{sid}:status" for sid in subscription_ids]
        settings = db.query(Settings).filter(Settings.key.in_(keys)).all()
        
        # æ„å»ºç»“æœæ˜ å°„
        result_map = {}
        for setting in settings:
            try:
                sid = int(setting.key.split(':')[1])
                data = json.loads(setting.value)
                result_map[sid] = {
                    "id": sid,
                    "status": data.get('status', 'idle'),
                    "pending": data.get('pending', 0),
                    "remote_total": data.get('remote_total', 0),
                    "updated_at": data.get('updated_at')
                }
            except (ValueError, json.JSONDecodeError):
                continue
        
        # å¡«å……ç¼ºå¤±çš„è®¢é˜…ï¼ˆè¿”å›é»˜è®¤çŠ¶æ€ï¼‰
        items = []
        for sid in subscription_ids:
            if sid in result_map:
                items.append(result_map[sid])
            else:
                items.append({
                    "id": sid,
                    "status": "idle",
                    "pending": 0,
                    "remote_total": 0,
                    "updated_at": None
                })
        
        return {"items": items}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get sync overview: {e}")
        raise HTTPException(status_code=500, detail="è·å–åŒæ­¥æ¦‚è§ˆå¤±è´¥")

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "6.0.0"
    }
