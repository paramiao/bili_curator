"""
è‡ªåŠ¨å¯¼å…¥æœåŠ¡ - Dockerå¯åŠ¨åè‡ªåŠ¨æ‰«æå¹¶å¯¼å…¥æ–°è§†é¢‘
"""
import os
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Optional
from .models import Database, Video, Subscription
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)

class AutoImportService:
    """è‡ªåŠ¨å¯¼å…¥æœåŠ¡"""
    
    def __init__(self, download_dir: str = "/app/downloads"):
        self.download_dir = Path(download_dir)
        self.db = Database()
    
    def scan_and_import(self) -> dict:
        """æ‰«æç›®å½•å¹¶å¯¼å…¥æ–°è§†é¢‘"""
        logger.info("ğŸ”„ å¼€å§‹è‡ªåŠ¨æ‰«æå¹¶å¯¼å…¥æ–°è§†é¢‘...")
        
        if not self.download_dir.exists():
            logger.warning(f"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {self.download_dir}")
            return {"imported": 0, "skipped": 0, "errors": 0}
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(self.download_dir.rglob("*.json"))
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        session = self.db.get_session()
        try:
            imported_count = 0
            skipped_count = 0
            error_count = 0
            
            for json_file in json_files:
                try:
                    result = self._import_video_from_json(json_file, session)
                    if result == "imported":
                        imported_count += 1
                    elif result == "skipped":
                        skipped_count += 1
                    
                    # æ¯100ä¸ªæäº¤ä¸€æ¬¡ï¼Œé¿å…é•¿æ—¶é—´é”å®š
                    if (imported_count + skipped_count) % 100 == 0:
                        session.commit()
                        logger.info(f"âœ… å·²å¤„ç† {imported_count + skipped_count} ä¸ªæ–‡ä»¶...")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {json_file} å¤±è´¥: {e}")
                    error_count += 1
            
            session.commit()
            
            result = {
                "imported": imported_count,
                "skipped": skipped_count,
                "errors": error_count
            }
            
            logger.info(f"ğŸ‰ è‡ªåŠ¨å¯¼å…¥å®Œæˆ: æˆåŠŸ {imported_count}, è·³è¿‡ {skipped_count}, é”™è¯¯ {error_count}")
            return result
            
        except Exception as e:
            session.rollback()
            logger.error(f"è‡ªåŠ¨å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
            raise
        finally:
            session.close()
    
    def _import_video_from_json(self, json_file: Path, session: Session) -> str:
        """ä»JSONæ–‡ä»¶å¯¼å…¥å•ä¸ªè§†é¢‘"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # è·³è¿‡éæ ‡å‡†JSONæ–‡ä»¶ï¼ˆå¦‚æŸäº›é…ç½®æ–‡ä»¶ï¼‰
            if not isinstance(metadata, dict):
                return "skipped"
            
            video_id = metadata.get('id')
            if not video_id:
                return "skipped"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_video = session.query(Video).filter(
                Video.bilibili_id == video_id
            ).first()
            
            if existing_video:
                return "skipped"
            
            # æŸ¥æ‰¾å¯¹åº”çš„è§†é¢‘æ–‡ä»¶
            base_name = json_file.stem
            video_file = self._find_video_file(json_file.parent, base_name)
            thumbnail_file = self._find_thumbnail_file(json_file.parent, base_name)
            
            # å¤„ç†ä¸Šä¼ æ—¥æœŸ
            upload_date = self._parse_upload_date(metadata.get('upload_date'))
            
            # åˆ›å»ºè§†é¢‘è®°å½•
            video = Video(
                bilibili_id=video_id,
                title=metadata.get('title', ''),
                uploader=metadata.get('uploader', ''),
                uploader_id=metadata.get('uploader_id', ''),
                duration=metadata.get('duration', 0),
                upload_date=upload_date,
                description=metadata.get('description', ''),
                tags=json.dumps(metadata.get('tags', []), ensure_ascii=False),
                video_path=str(video_file) if video_file else None,
                json_path=str(json_file),
                thumbnail_path=str(thumbnail_file) if thumbnail_file else None,
                file_size=video_file.stat().st_size if video_file and video_file.exists() else 0,
                view_count=metadata.get('view_count', 0),
                downloaded=True,
                downloaded_at=datetime.fromtimestamp(video_file.stat().st_mtime) if video_file and video_file.exists() else datetime.now()
            )
            
            session.add(video)
            return "imported"
            
        except Exception as e:
            logger.error(f"å¯¼å…¥è§†é¢‘ {json_file} å¤±è´¥: {e}")
            raise
    
    def _find_video_file(self, directory: Path, base_name: str) -> Optional[Path]:
        """æŸ¥æ‰¾å¯¹åº”çš„è§†é¢‘æ–‡ä»¶"""
        video_extensions = ['.mp4', '.mkv', '.flv', '.webm', '.avi']
        
        for ext in video_extensions:
            video_file = directory / f"{base_name}{ext}"
            if video_file.exists():
                return video_file
        
        return None
    
    def _find_thumbnail_file(self, directory: Path, base_name: str) -> Optional[Path]:
        """æŸ¥æ‰¾å¯¹åº”çš„ç¼©ç•¥å›¾æ–‡ä»¶"""
        thumbnail_extensions = ['.jpg', '.jpeg', '.png', '.webp']
        
        for ext in thumbnail_extensions:
            thumbnail_file = directory / f"{base_name}{ext}"
            if thumbnail_file.exists():
                return thumbnail_file
        
        return None
    
    def _parse_upload_date(self, date_str: str) -> Optional[datetime]:
        """è§£æä¸Šä¼ æ—¥æœŸ"""
        if not date_str:
            return None
        
        try:
            # yt-dlpé€šå¸¸è¿”å›YYYYMMDDæ ¼å¼
            if len(date_str) == 8 and date_str.isdigit():
                return datetime.strptime(date_str, '%Y%m%d')
            # å°è¯•ISOæ ¼å¼
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except:
            return None
    
    def auto_associate_subscriptions(self) -> dict:
        """è‡ªåŠ¨å…³è”è®¢é˜…ä¸å·²å¯¼å…¥çš„è§†é¢‘"""
        logger.info("ğŸ”— å¼€å§‹è‡ªåŠ¨å…³è”è®¢é˜…ä¸å·²å¯¼å…¥è§†é¢‘...")
        
        session = self.db.get_session()
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒè®¢é˜…
            subscriptions = session.query(Subscription).filter(
                Subscription.is_active == True
            ).all()
            
            associated_count = 0
            
            for subscription in subscriptions:
                # æ ¹æ®è®¢é˜…ç±»å‹æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘
                matching_videos = self._find_matching_videos(subscription, session)
                
                for video in matching_videos:
                    if not video.subscription_id:  # åªå…³è”æœªå…³è”çš„è§†é¢‘
                        video.subscription_id = subscription.id
                        associated_count += 1
                
                # æ›´æ–°è®¢é˜…ç»Ÿè®¡
                subscription.downloaded_videos = len([v for v in matching_videos if v.downloaded])
                
            session.commit()
            
            logger.info(f"ğŸ‰ è‡ªåŠ¨å…³è”å®Œæˆ: {associated_count} ä¸ªè§†é¢‘å·²å…³è”åˆ°è®¢é˜…")
            return {"associated": associated_count}
            
        except Exception as e:
            session.rollback()
            logger.error(f"è‡ªåŠ¨å…³è”è¿‡ç¨‹å‡ºé”™: {e}")
            raise
        finally:
            session.close()
    
    def _find_matching_videos(self, subscription: Subscription, session: Session) -> List[Video]:
        """æ ¹æ®è®¢é˜…ç±»å‹æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘"""
        query = session.query(Video)

        if subscription.type == "uploader" and subscription.uploader_id:
            # UPä¸»è®¢é˜…ï¼šåŒ¹é…uploader_id
            return query.filter(Video.uploader_id == subscription.uploader_id).all()

        elif subscription.type == "keyword" and subscription.keyword:
            # å…³é”®è¯è®¢é˜…ï¼šåŒ¹é…æ ‡é¢˜æˆ–æ ‡ç­¾
            keyword = subscription.keyword.lower()
            return query.filter(
                Video.title.ilike(f"%{keyword}%")
            ).all()

        elif subscription.type == "collection" and (subscription.name or subscription.url):
            # åˆé›†è®¢é˜…ï¼šæŒ‰è®¢é˜…ç›®å½•åŒ¹é…ï¼ˆä¸ä¸‹è½½å™¨ç›®å½•è§„åˆ™ä¸€è‡´ï¼š/app/downloads/<sanitized(subscription.name)>ï¼‰
            sub_dir = self._compute_subscription_dir(subscription)
            if sub_dir:
                prefix = str(sub_dir).rstrip('/') + '/'
                # åŒ¹é…è§†é¢‘/JSONè·¯å¾„è½åœ¨è¯¥ç›®å½•ä¸‹çš„è®°å½•
                return query.filter(
                    (Video.video_path.isnot(None) & Video.video_path.ilike(f"{prefix}%")) |
                    (Video.json_path.isnot(None) & Video.json_path.ilike(f"{prefix}%"))
                ).all()

        return []

    def _compute_subscription_dir(self, subscription: Subscription) -> Optional[Path]:
        """è®¡ç®—è®¢é˜…å¯¹åº”çš„ç›®å½•ï¼ˆä¸ä¸‹è½½å™¨å‘½åä¿æŒä¸€è‡´ï¼‰"""
        base_download = self.download_dir
        name = (getattr(subscription, 'name', None) or '').strip()
        dir_name = self._sanitize_filename(name) if name else None
        if not dir_name:
            # å…œåº•
            dir_name = self._sanitize_filename(f"è®¢é˜…_{subscription.id}")
        return base_download / dir_name

    def _sanitize_filename(self, filename: str) -> str:
        import re
        illegal = r'[<>:"/\\|?*]'
        s = re.sub(illegal, '_', filename or '')
        s = s.strip(' .')
        return s[:100] if len(s) > 100 else s

# å…¨å±€è‡ªåŠ¨å¯¼å…¥æœåŠ¡å®ä¾‹
auto_import_service = AutoImportService()
