"""
è‡ªåŠ¨å¯¼å…¥æœåŠ¡ - Dockerå¯åŠ¨åè‡ªåŠ¨æ‰«æå¹¶å¯¼å…¥æ–°è§†é¢‘
"""
import os
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Optional
from .models import Database, Video, Subscription
from .utils.path_utils import strip_info_suffix, base_name_from_json_path
from .services.subscription_stats import (
    recompute_all_subscriptions,
    recompute_subscription_stats,
)
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)

class AutoImportService:
    """è‡ªåŠ¨å¯¼å…¥æœåŠ¡"""
    
    def __init__(self, download_dir: str = "/app/downloads"):
        self.download_dir = Path(download_dir)
        self.db = Database()
        # çŠ¶æ€æ–‡ä»¶ä½äºä¸‹è½½ç›®å½•ï¼Œè®°å½•ä¸Šæ¬¡æˆåŠŸæ‰«ææ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        self.state_file = self.download_dir / ".auto_import_state.json"
    
    def scan_and_import(self) -> dict:
        """æ‰«æç›®å½•å¹¶å¯¼å…¥æ–°è§†é¢‘"""
        logger.info("ğŸ”„ å¼€å§‹è‡ªåŠ¨æ‰«æå¹¶å¯¼å…¥æ–°è§†é¢‘...")
        
        if not self.download_dir.exists():
            logger.warning(f"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {self.download_dir}")
            return {"imported": 0, "skipped": 0, "errors": 0}
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶ï¼ˆæ”¯æŒå¢é‡æ‰«æï¼‰
        last_scan_ts = self._load_last_scan_ts()
        all_json_files = list(self.download_dir.rglob("*.json"))
        if last_scan_ts:
            json_files = []
            for p in all_json_files:
                try:
                    if p.stat().st_mtime > last_scan_ts:
                        json_files.append(p)
                except Exception:
                    # è¯»å–æ–‡ä»¶çŠ¶æ€å¤±è´¥åˆ™è·³è¿‡è¯¥æ–‡ä»¶
                    continue
            logger.info(f"ğŸ“„ å¢é‡æ¨¡å¼ï¼šæ€» {len(all_json_files)}ï¼Œå¾…å¤„ç† {len(json_files)}ï¼ˆlast_scan_ts={last_scan_ts}ï¼‰")
        else:
            json_files = all_json_files
            logger.info(f"ğŸ“„ é¦–æ¬¡/å…¨é‡æ‰«æï¼šæ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        session = self.db.get_session()
        try:
            imported_count = 0
            skipped_count = 0
            error_count = 0
            
            for json_file in json_files:
                try:
                    result = self._import_video_from_json(json_file, session)
                    if result == "imported":
                        imported_count += 1
                    elif result == "skipped":
                        skipped_count += 1
                    
                    # æ¯100ä¸ªæäº¤ä¸€æ¬¡ï¼Œé¿å…é•¿æ—¶é—´é”å®š
                    if (imported_count + skipped_count) % 100 == 0:
                        session.commit()
                        logger.info(f"âœ… å·²å¤„ç† {imported_count + skipped_count} ä¸ªæ–‡ä»¶...")
                
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {json_file} å¤±è´¥: {e}")
                    error_count += 1
            
            session.commit()
            # å¯¼å…¥å®Œæˆååˆ·æ–°æ‰€æœ‰è®¢é˜…ç»Ÿè®¡ï¼ˆæ— æ³•å‡†ç¡®å®šä½è®¢é˜…å½’å±æ—¶é‡‡ç”¨å…¨é‡åˆ·æ–°ï¼‰
            try:
                recompute_all_subscriptions(session, touch_last_check=False)
                session.commit()
            except Exception as e:
                session.rollback()
                logger.warning(f"åˆ·æ–°è®¢é˜…ç»Ÿè®¡å¤±è´¥(è‡ªåŠ¨å¯¼å…¥å)ï¼š{e}")

            # æˆåŠŸå®Œæˆåæ›´æ–°æ‰«ææ—¶é—´
            try:
                self._save_last_scan_ts(datetime.now())
            except Exception as e:
                logger.warning(f"ä¿å­˜å¢é‡æ‰«ææ—¶é—´å¤±è´¥ï¼š{e}")
            
            result = {
                "imported": imported_count,
                "skipped": skipped_count,
                "errors": error_count
            }
            
            logger.info(f"ğŸ‰ è‡ªåŠ¨å¯¼å…¥å®Œæˆ: æˆåŠŸ {imported_count}, è·³è¿‡ {skipped_count}, é”™è¯¯ {error_count}")
            return result
            
        except Exception as e:
            session.rollback()
            logger.error(f"è‡ªåŠ¨å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
            raise
        finally:
            session.close()

    def scan_and_import_for_subscription(self, subscription_id: int) -> dict:
        """ä»…æ‰«ææŒ‡å®šè®¢é˜…å¯¹åº”ç›®å½•å¹¶å¯¼å…¥æ–°è§†é¢‘ï¼Œæœ€ååªé‡ç®—è¯¥è®¢é˜…ç»Ÿè®¡ã€‚
        æ³¨æ„ï¼šä¸ºé¿å…ä¸å…¨å±€å¢é‡çŠ¶æ€è€¦åˆï¼Œæ­¤å¤„ä¸ä½¿ç”¨ last_scan_tsï¼Œå›ºå®šæŒ‰è®¢é˜…ç›®å½•å…¨é‡æ‰«æã€‚
        """
        logger.info(f"ğŸ”„ [sub={subscription_id}] å¼€å§‹æŒ‰è®¢é˜…ç›®å½•æ‰«æå¹¶å¯¼å…¥æ–°è§†é¢‘...")

        session = self.db.get_session()
        try:
            # å®šä½è®¢é˜…ä¸å…¶ç›®å½•
            sub: Optional[Subscription] = session.query(Subscription).filter(Subscription.id == subscription_id).first()
            if not sub:
                logger.warning(f"è®¢é˜…ä¸å­˜åœ¨: {subscription_id}")
                return {"imported": 0, "skipped": 0, "errors": 0}

            sub_dir = self._compute_subscription_dir(sub)
            if not sub_dir.exists():
                logger.info(f"è®¢é˜…ç›®å½•ä¸å­˜åœ¨æˆ–æ— å†…å®¹: {sub_dir}")
                return {"imported": 0, "skipped": 0, "errors": 0}

            json_files = list(sub_dir.rglob("*.json"))
            logger.info(f"ğŸ“„ [sub={subscription_id}] è®¢é˜…ç›®å½•æ‰«æï¼šæ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

            imported_count = 0
            skipped_count = 0
            error_count = 0

            for json_file in json_files:
                try:
                    result = self._import_video_from_json(json_file, session)
                    if result == "imported":
                        imported_count += 1
                    elif result == "skipped":
                        skipped_count += 1
                    if (imported_count + skipped_count) % 100 == 0:
                        session.commit()
                except Exception as e:
                    logger.error(f"[sub={subscription_id}] å¤„ç†æ–‡ä»¶ {json_file} å¤±è´¥: {e}")
                    error_count += 1

            # æäº¤å¯¼å…¥
            session.commit()

            # ä»…é‡ç®—è¯¥è®¢é˜…ç»Ÿè®¡
            try:
                recompute_subscription_stats(session, subscription_id, touch_last_check=False)
                session.commit()
            except Exception as e:
                session.rollback()
                logger.warning(f"[sub={subscription_id}] åˆ·æ–°è®¢é˜…ç»Ÿè®¡å¤±è´¥(è‡ªåŠ¨å¯¼å…¥å)ï¼š{e}")

            result = {
                "imported": imported_count,
                "skipped": skipped_count,
                "errors": error_count,
            }
            logger.info(f"ğŸ‰ [sub={subscription_id}] è‡ªåŠ¨å¯¼å…¥å®Œæˆ: æˆåŠŸ {imported_count}, è·³è¿‡ {skipped_count}, é”™è¯¯ {error_count}")
            return result
        except Exception as e:
            session.rollback()
            logger.error(f"[sub={subscription_id}] è‡ªåŠ¨å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
            raise
        finally:
            session.close()

    def _load_last_scan_ts(self) -> float:
        """è¯»å–ä¸Šæ¬¡æ‰«æçš„æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ã€‚ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥æ—¶è¿”å› 0ã€‚"""
        try:
            if not self.state_file.exists():
                return 0
            import json as _json
            with open(self.state_file, 'r', encoding='utf-8') as f:
                data = _json.load(f)
            ts = float(data.get('last_scan_ts', 0))
            return ts if ts > 0 else 0
        except Exception:
            return 0

    def _save_last_scan_ts(self, dt: datetime) -> None:
        """ä¿å­˜æœ¬æ¬¡æ‰«æå®Œæˆæ—¶é—´æˆ³ï¼ˆç§’ï¼‰ã€‚"""
        try:
            self.state_file.parent.mkdir(parents=True, exist_ok=True)
            import json as _json
            with open(self.state_file, 'w', encoding='utf-8') as f:
                _json.dump({'last_scan_ts': dt.timestamp()}, f)
        except Exception:
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
            pass
    
    def _import_video_from_json(self, json_file: Path, session: Session) -> str:
        """ä»JSONæ–‡ä»¶å¯¼å…¥å•ä¸ªè§†é¢‘"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # è·³è¿‡éæ ‡å‡†JSONæ–‡ä»¶ï¼ˆå¦‚æŸäº›é…ç½®æ–‡ä»¶ï¼‰
            if not isinstance(metadata, dict):
                return "skipped"
            
            video_id = metadata.get('id')
            if not video_id:
                return "skipped"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_video = session.query(Video).filter(
                Video.bilibili_id == video_id
            ).first()
            
            if existing_video:
                return "skipped"
            
            # æŸ¥æ‰¾å¯¹åº”çš„è§†é¢‘/ç¼©ç•¥å›¾æ–‡ä»¶ï¼ˆç»Ÿä¸€ä½¿ç”¨å·¥å…·å‡½æ•°å¤„ç† *.info.jsonï¼‰
            base_name = base_name_from_json_path(json_file)
            video_file = self._find_video_file(json_file.parent, base_name)
            thumbnail_file = self._find_thumbnail_file(json_file.parent, base_name)
            
            # å¤„ç†ä¸Šä¼ æ—¥æœŸ
            upload_date = self._parse_upload_date(metadata.get('upload_date'))
            
            # å®‰å…¨è·å–æ–‡ä»¶å¤§å°ä¸ä¿®æ”¹æ—¶é—´ï¼ˆé¿å…åœ¨ exists ä¸ stat ä¹‹é—´çš„ç«æ€ï¼Œå¹¶ä¸”åª stat ä¸€æ¬¡ï¼‰
            file_size = 0
            downloaded_at = datetime.now()
            if video_file and video_file.exists():
                try:
                    stat_res = video_file.stat()
                    file_size = stat_res.st_size
                    downloaded_at = datetime.fromtimestamp(stat_res.st_mtime)
                except Exception:
                    # è¯»å–æ–‡ä»¶çŠ¶æ€å¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
                    pass

            # åˆ›å»ºè§†é¢‘è®°å½•
            video = Video(
                bilibili_id=video_id,
                title=metadata.get('title', ''),
                uploader=metadata.get('uploader', ''),
                uploader_id=metadata.get('uploader_id', ''),
                duration=metadata.get('duration', 0),
                upload_date=upload_date,
                description=metadata.get('description', ''),
                tags=json.dumps(metadata.get('tags', []), ensure_ascii=False),
                video_path=str(video_file) if video_file else None,
                json_path=str(json_file),
                thumbnail_path=str(thumbnail_file) if thumbnail_file else None,
                file_size=file_size,
                view_count=metadata.get('view_count', 0),
                downloaded=True,
                downloaded_at=downloaded_at
            )
            
            session.add(video)
            return "imported"
            
        except Exception as e:
            logger.error(f"å¯¼å…¥è§†é¢‘ {json_file} å¤±è´¥: {e}")
            raise
    
    def _find_video_file(self, directory: Path, base_name: str) -> Optional[Path]:
        """æŸ¥æ‰¾å¯¹åº”çš„è§†é¢‘æ–‡ä»¶"""
        # å†æ¬¡å…œåº•å‰¥ç¦» .infoï¼Œç¡®ä¿è°ƒç”¨æ–¹ä¼ å‚å¼‚å¸¸æ—¶ä¹Ÿèƒ½åŒ¹é…
        base_name = strip_info_suffix(base_name)
        video_extensions = ['.mp4', '.mkv', '.flv', '.webm', '.avi']
        
        for ext in video_extensions:
            video_file = directory / f"{base_name}{ext}"
            if video_file.exists():
                return video_file
        
        return None
    
    def _find_thumbnail_file(self, directory: Path, base_name: str) -> Optional[Path]:
        """æŸ¥æ‰¾å¯¹åº”çš„ç¼©ç•¥å›¾æ–‡ä»¶"""
        base_name = strip_info_suffix(base_name)
        thumbnail_extensions = ['.jpg', '.jpeg', '.png', '.webp']
        
        for ext in thumbnail_extensions:
            thumbnail_file = directory / f"{base_name}{ext}"
            if thumbnail_file.exists():
                return thumbnail_file
        
        return None
    
    def _parse_upload_date(self, date_str: str) -> Optional[datetime]:
        """è§£æä¸Šä¼ æ—¥æœŸ"""
        if not date_str:
            return None
        
        try:
            # yt-dlpé€šå¸¸è¿”å›YYYYMMDDæ ¼å¼
            if len(date_str) == 8 and date_str.isdigit():
                return datetime.strptime(date_str, '%Y%m%d')
            # å°è¯•ISOæ ¼å¼
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except:
            return None
    
    def auto_associate_subscriptions(self) -> dict:
        """è‡ªåŠ¨å…³è”è®¢é˜…ä¸å·²å¯¼å…¥çš„è§†é¢‘"""
        logger.info("ğŸ”— å¼€å§‹è‡ªåŠ¨å…³è”è®¢é˜…ä¸å·²å¯¼å…¥è§†é¢‘...")
        
        session = self.db.get_session()
        try:
            # è·å–æ‰€æœ‰æ´»è·ƒè®¢é˜…
            subscriptions = session.query(Subscription).filter(
                Subscription.is_active == True
            ).all()
            
            associated_count = 0
            
            for subscription in subscriptions:
                # æ ¹æ®è®¢é˜…ç±»å‹æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘
                matching_videos = self._find_matching_videos(subscription, session)
                
                for video in matching_videos:
                    if not video.subscription_id:  # åªå…³è”æœªå…³è”çš„è§†é¢‘
                        video.subscription_id = subscription.id
                        associated_count += 1
                
                # ç»Ÿä¸€é€šè¿‡ç»Ÿè®¡æœåŠ¡åˆ·æ–°è¯¥è®¢é˜…çš„ç»Ÿè®¡å­—æ®µ
                try:
                    recompute_subscription_stats(session, subscription.id, touch_last_check=False)
                except Exception as e:
                    logger.warning(f"åˆ·æ–°è®¢é˜…ç»Ÿè®¡å¤±è´¥(è‡ªåŠ¨å…³è”é˜¶æ®µ sub={subscription.id})ï¼š{e}")
                
            session.commit()
            
            logger.info(f"ğŸ‰ è‡ªåŠ¨å…³è”å®Œæˆ: {associated_count} ä¸ªè§†é¢‘å·²å…³è”åˆ°è®¢é˜…")
            return {"associated": associated_count}
            
        except Exception as e:
            session.rollback()
            logger.error(f"è‡ªåŠ¨å…³è”è¿‡ç¨‹å‡ºé”™: {e}")
            raise
        finally:
            session.close()
    
    def _find_matching_videos(self, subscription: Subscription, session: Session) -> List[Video]:
        """æ ¹æ®è®¢é˜…ç±»å‹æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘"""
        query = session.query(Video)

        if subscription.type == "uploader" and subscription.uploader_id:
            # UPä¸»è®¢é˜…ï¼šåŒ¹é…uploader_id
            return query.filter(Video.uploader_id == subscription.uploader_id).all()

        elif subscription.type == "keyword" and subscription.keyword:
            # å…³é”®è¯è®¢é˜…ï¼šåŒ¹é…æ ‡é¢˜æˆ–æ ‡ç­¾
            keyword = subscription.keyword.lower()
            return query.filter(
                Video.title.ilike(f"%{keyword}%")
            ).all()

        elif subscription.type == "collection" and (subscription.name or subscription.url):
            # åˆé›†è®¢é˜…ï¼šæŒ‰è®¢é˜…ç›®å½•åŒ¹é…ï¼ˆä¸ä¸‹è½½å™¨ç›®å½•è§„åˆ™ä¸€è‡´ï¼š/app/downloads/<sanitized(subscription.name)>ï¼‰
            sub_dir = self._compute_subscription_dir(subscription)
            if sub_dir:
                prefix = str(sub_dir).rstrip('/') + '/'
                # åŒ¹é…è§†é¢‘/JSONè·¯å¾„è½åœ¨è¯¥ç›®å½•ä¸‹çš„è®°å½•
                return query.filter(
                    (Video.video_path.isnot(None) & Video.video_path.ilike(f"{prefix}%")) |
                    (Video.json_path.isnot(None) & Video.json_path.ilike(f"{prefix}%"))
                ).all()

        return []

    def _compute_subscription_dir(self, subscription: Subscription) -> Optional[Path]:
        """è®¡ç®—è®¢é˜…å¯¹åº”çš„ç›®å½•ï¼ˆä¸ä¸‹è½½å™¨å‘½åä¿æŒä¸€è‡´ï¼‰"""
        base_download = self.download_dir
        name = (getattr(subscription, 'name', None) or '').strip()
        dir_name = self._sanitize_filename(name) if name else None
        if not dir_name:
            # å…œåº•
            dir_name = self._sanitize_filename(f"è®¢é˜…_{subscription.id}")
        return base_download / dir_name

    def _sanitize_filename(self, filename: str) -> str:
        import re
        illegal = r'[<>:"/\\|?*]'
        s = re.sub(illegal, '_', filename or '')
        s = s.strip(' .')
        return s[:100] if len(s) > 100 else s

# å…¨å±€è‡ªåŠ¨å¯¼å…¥æœåŠ¡å®ä¾‹
auto_import_service = AutoImportService()
